<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>elk圣经：Elasticsearch、Logstash、Kibana从入门到精通 | 繁华流年间</title><meta name="author" content="周五打工人"><meta name="copyright" content="周五打工人"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="elk（Elasticsearch、Logstash、Kibana）从入门到精通的 学习路线ELK 其实并不是一款软件，而是一整套解决方案，是三个软件产品的首字母缩写  Elasticsearch：负责日志检索和储存  Logstash：负责日志的收集和分析、处理  Kibana：负责日志的可视化 这三款软件都是开源软件，通常是配合使用，而且又先后归于 Elastic.co 公司名下，故被简称为">
<meta property="og:type" content="article">
<meta property="og:title" content="elk圣经：Elasticsearch、Logstash、Kibana从入门到精通">
<meta property="og:url" content="http://flowtime.asia/2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/index.html">
<meta property="og:site_name" content="繁华流年间">
<meta property="og:description" content="elk（Elasticsearch、Logstash、Kibana）从入门到精通的 学习路线ELK 其实并不是一款软件，而是一整套解决方案，是三个软件产品的首字母缩写  Elasticsearch：负责日志检索和储存  Logstash：负责日志的收集和分析、处理  Kibana：负责日志的可视化 这三款软件都是开源软件，通常是配合使用，而且又先后归于 Elastic.co 公司名下，故被简称为">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://flowtime.asia/img/liushui.png">
<meta property="article:published_time" content="2025-03-08T07:49:00.000Z">
<meta property="article:modified_time" content="2025-03-09T13:03:33.028Z">
<meta property="article:author" content="周五打工人">
<meta property="article:tag" content="ELK">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://flowtime.asia/img/liushui.png"><link rel="shortcut icon" href="/img/index.jpg"><link rel="canonical" href="http://flowtime.asia/2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script async="async" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle = window.adsbygoogle || []).push({
  google_ad_client: 'ca-pub-3578075519463317',
  enable_page_level_ads: 'true'
});</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'elk圣经：Elasticsearch、Logstash、Kibana从入门到精通',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-03-09 21:03:33'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/transparent.css"><link rel="stylesheet" href="/css/footer.css"><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/nav.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">51</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">42</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/liushui.png')"><nav id="nav"><span id="blog-info"><a href="/" title="繁华流年间"></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">elk圣经：Elasticsearch、Logstash、Kibana从入门到精通</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-03-08T07:49:00.000Z" title="发表于 2025-03-08 15:49:00">2025-03-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9E%B6%E6%9E%84/">架构</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="elk圣经：Elasticsearch、Logstash、Kibana从入门到精通"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="elk（Elasticsearch、Logstash、Kibana）从入门到精通的-学习路线"><a href="#elk（Elasticsearch、Logstash、Kibana）从入门到精通的-学习路线" class="headerlink" title="elk（Elasticsearch、Logstash、Kibana）从入门到精通的 学习路线"></a>elk（Elasticsearch、Logstash、Kibana）从入门到精通的 学习路线</h1><p>ELK 其实并不是一款软件，而是一整套解决方案，是三个软件产品的首字母缩写</p>
<ul>
<li><p>Elasticsearch：负责日志检索和储存</p>
</li>
<li><p>Logstash：负责日志的收集和分析、处理</p>
</li>
<li><p>Kibana：负责日志的可视化</p>
<p>这三款软件都是开源软件，通常是配合使用，而且又先后归于 Elastic.co 公司名下，故被<strong>简称为 ELK</strong></p>
</li>
</ul>
<h1 id="1-ELK日志平台介绍"><a href="#1-ELK日志平台介绍" class="headerlink" title="1 ELK日志平台介绍"></a>1 ELK日志平台介绍</h1><p>ELK指的是Elastic公司下面Elasticsearch、Logstash、Kibana三大开源框架首字母大写简称。 Elasticsearch、Logstash、Kibana三大开源框架首字母大写简称。</p>
<h2 id="1-1-基础日志平台的背景"><a href="#1-1-基础日志平台的背景" class="headerlink" title="1.1 基础日志平台的背景"></a>1.1 基础日志平台的背景</h2><p>日志主要包括系统日志、应用程序日志和安全日志。系统运维和开发人员可以通过日志了解服务器软硬件信息、检查配置过程中的错误及错误发生的原因。经常分析日志可以了解服务器的负荷，性能安全性，从而及时采取措施纠正错误。</p>
<p>通常，日志被分散的储存不同的设备上。</p>
<p>如果你管理数十上百台服务器，你还在使用依次登录每台机器的传统方法查阅日志。</p>
<p>这样是不是感觉很繁琐和效率低下。当务之急我们使用集中化的日志管理，例如：开源的syslog，将所有服务器上的日志收集汇总。</p>
<p>集中化管理日志后，日志的统计和检索又成为一件比较麻烦的事情，一般我们使用grep、awk和wc等Linux命令能实现检索和统计，但是对于要求更高的查询、排序和统计等要求和庞大的机器数量依然使用这样的方法难免有点力不从心。</p>
<p>随着服务容器化，跑在一台CentOS服务器上，服务器搭建了docker环境，安装了docker-compose或者k8s，但在日志处理方面，暂时没有一个好的方法能够收集完全的日志，</p>
<p>只能依赖进入至服务器后，以docker logs containerID的方法来进入查看，非常不方便，所以，基础日志平台就迫在眉睫。</p>
<h2 id="1-2-ELK的关系"><a href="#1-2-ELK的关系" class="headerlink" title="1.2 ELK的关系"></a>1.2 ELK的关系</h2><p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_1.png"></p>
<p>在ELK架构中，Elasticsearch、Logstash和Kibana三款软件作用如下：</p>
<p><strong>1、Elasticsearch</strong> Elasticsearch是一个高度可扩展的全文搜索和分析引擎，基于Apache Lucence（事实上，Lucence也是百度所采用的搜索引擎）构建，能够对大容量的数据进行接近实时的存储、搜索和分析操作。 <strong>2、Logstash</strong><br>Logstash是一个数据收集引擎，它可以动态的从各种数据源搜集数据，并对数据进行过滤、分析和统一格式等操作，并将输出结果存储到指定位置上。Logstash支持普通的日志文件和自定义Json格式的日志解析。</p>
<p><strong>负责日志清洗： 日志 过滤，格式处理，等等。 有大量的 自定义完成。</strong></p>
<p><strong>3、Kibana</strong> Kibana是一个数据分析和可视化平台，通常与Elasticsearch配合使用，用于对其中的数据进行搜索、分析，并且以统计图标的形式展示。</p>
<h2 id="1-3-ELK的架构"><a href="#1-3-ELK的架构" class="headerlink" title="1.3 ELK的架构"></a>1.3 ELK的架构</h2><p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_2.png"></p>
<p>如上图所示，filebeats 安装在各个设备上，用于收集日志信息，收集到的日志信息统一汇总到Elasticsearch上，然后由Kibana负责web端的展示。</p>
<p>其中，如果终端设备过多，会导致logstash 过载的现象，此时，我们可以采用一台 mq 设备作为消息队列，以暂时缓存数据，避免 logstash 压力突发。</p>
<p><strong>ELK架构优点如下：</strong> <strong>1、处理方式灵活。</strong> Elasticsearch是全文索引，具有强大的搜索能力。 <strong>2、配置相对简单。</strong> Kibana的配置非常简单，Elasticsearch则全部使用Json接口，配置也不复杂，Logstash的配置使用模块的方式，配置也相对简单。 <strong>3、检索性能高。</strong>ELK架构通常可以达到百亿级数据的查询秒级响应。 <strong>4、集群线性扩展。</strong> Elasticsearch本身没有单点的概念，自动默认集群模式，Elasticsearch和Logstash都可以灵活扩展。<strong>5、页面美观。</strong> Kibana的前端设计美观，且操作简单。</p>
<p><strong>Logstash</strong>:从各种数据源搜集数据，并对数据进行过滤、分析、丰富、统一格式等操作，然后存储到 ES。</p>
<p><strong>Elasticsearch</strong>:对大容量的数据进行接近实时的存储、搜索和分析操作。</p>
<p><strong>Kibana</strong>：数据分析和可视化平台。与 Elasticsearch 配合使用，对数据进行搜索、分析和以统计图表的方式展示。</p>
<h3 id="1-3-1-简单的ELK日志平台"><a href="#1-3-1-简单的ELK日志平台" class="headerlink" title="1.3.1 简单的ELK日志平台"></a>1.3.1 简单的ELK日志平台</h3><p>刚来公司的时候，我们公司的日志收集系统ELK经常会出现查询不了最新的日志的情况，后面去查发现 ES的节点经常也是yellow或者red的情况。有时候会收到开发的投诉。</p>
<p>最简单的，elk 架构图解如下:</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_3.png"></p>
<p>其中ElasticSearch 是三台服务器构成的集群，其中：</p>
<ul>
<li><p>ElasticSearch做倒排索引，</p>
</li>
<li><p>Logstash跑在每个服务器上，各种日志通过Logstash搜集，Grok，Geoip等插件进行处理然后统一送到ElasticSearch的集群。</p>
</li>
<li><p>Kibana做图形化的展示。</p>
</li>
</ul>
<p>这种elk架构比较简单，也存在一些问题：</p>
<p>1、Logstash依赖Java虚拟机占用系统的内存和CPU都比较大，</p>
<p>2、Logstash在数据量较大的时候容易导致其他业务应用程序崩溃，影响业务正常使用</p>
<p>3、随着时间的积累，es空间不能满足现状</p>
<p>4、Kibana没有安全管控机制，没有权限审核，安全性较差。</p>
<p>5、ElasticSearch 主节点也是数据节点，导致有时候查询较慢</p>
<h3 id="1-3-2-ELK改进之引入Filebeat"><a href="#1-3-2-ELK改进之引入Filebeat" class="headerlink" title="1.3.2 ELK改进之引入Filebeat"></a>1.3.2 ELK改进之引入Filebeat</h3><p>ElasticSearch的版本，我们还是选择原来的 6.2.x的版本，然后重新搭建了一套ELK的日志系统。</p>
<p>ElasticSearch 6.x 的版本如果要做用于鉴权的话，必须依赖X-Pack，但是X-pack是付费的产品，所以，引入x-pack，虽然能实现 Index 级别的权限管控，确保数据安全，但是涉及到费用的问题。</p>
<p>于是，ElasticSearch的版本采用ElasticSearch 7.x的版本，用户鉴权采用其免费的 basic 认证实现（因为7.x的新版本在性能上优化，查询和写入速度会更快）</p>
<p>架构图解如下:</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_4.png"></p>
<p>整个架构的具体的改进方法如下:</p>
<p>1、客户端选用更轻量化的Filebeat，Filebeat 采用 Golang 语言进行编写的，优点是暂用系统资源小，收集效率高。</p>
<p>2、Filebeat 数据收集之后统一送到多个 Logstatsh进行统一的过滤，然后将过滤后的数据写入ElasticSearch集群。</p>
<p>3、将原有的3个es节点增加至6个节点，其中3个ES节点是master节点，其余的节点是数据节点，如果磁盘不够用可以横向扩展数据节点。</p>
<p>6、ElasticSearch集群的硬盘采用 SSD的硬盘</p>
<p>7、ElasticSearch 做冷热数据分离</p>
<p>8、60天之前的索引数据进行关闭，有需要用的时候手工打开</p>
<p>9、ElasticSearch的版本采用ElasticSearch 7.x的版本，用户鉴权采用其免费的 basic 认证实现（因为7.x的新版本在性能上优化，查询和写入速度会更快）</p>
<p>到此，我们的日志系统算暂时是正常并且能满足日志查日志的需求了，也很少出现卡顿的现象了，并且服务器的资源使用率直接下降了一半。</p>
<h3 id="1-3-3-ELK的应用场景"><a href="#1-3-3-ELK的应用场景" class="headerlink" title="1.3.3 ELK的应用场景"></a>1.3.3 ELK的应用场景</h3><ul>
<li>异常分析</li>
</ul>
<p>通过将应用的日志内容通过Logstash输入到Elasticsearch中来实现对程序异常的分析排查</p>
<ul>
<li>业务分析</li>
</ul>
<p>将消息的通讯结果通过Logstash输入到Elasticsearch中来实现对业务效果的整理</p>
<ul>
<li>系统分析</li>
</ul>
<p>将处理内容的延迟作为数据输入到Elasticsearch 中来实现对应用性能的调优</p>
<h2 id="1-4-ELK的不足"><a href="#1-4-ELK的不足" class="headerlink" title="1.4 ELK的不足"></a>1.4 ELK的不足</h2><h3 id="es的资源占用"><a href="#es的资源占用" class="headerlink" title="es的资源占用"></a>es的资源占用</h3><p>一般使用 ES 时，必须要事先评估好节点配置和集群规模，可以从以下几个方面进行评估：</p>
<h3 id="存储容量："><a href="#存储容量：" class="headerlink" title="存储容量："></a>存储容量：</h3><p>要考虑索引副本数量、数据膨胀、ES 内部任务额外占用的磁盘空间（比如 segment merge )以及操作系统占用的磁盘空间等因素，</p>
<p>如果再需要预留 50% 的空闲磁盘空间，那么集群总的存储容量大约为源数据量的 4 倍；</p>
<h3 id="计算资源："><a href="#计算资源：" class="headerlink" title="计算资源："></a>计算资源：</h3><p>主要考虑写入，2 核 8GB 的节点可以支持 5000 qps 的写入，随着节点数量和节点规格的提升，写入能力基本呈线性增长；</p>
<h3 id="索引和分片数量评估："><a href="#索引和分片数量评估：" class="headerlink" title="索引和分片数量评估："></a>索引和分片数量评估：</h3><ul>
<li><p>一般一个 shard 的数据量在 30-50 GB为宜，可以以此确定索引的分片数量以及确定按天还是按月建索引。</p>
</li>
<li><p>需要控制单节点总的分片数量，1GB 堆内存支持 20-30 个分片为宜。</p>
</li>
<li><p>另外需要控制集群整体的分片数量，集群总体的分片数量一般不要超过 3w 。</p>
<p>算下来 3W * 50G &#x3D; 1500 T &#x3D; 1.5P</p>
<p>那么，elk 如何支持 一天1000PB，一个月上万PB规模的日志量呢？</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_5.png"></p>
</li>
</ul>
<p>从吞吐量上来说，虽然mq进行扩展，能支撑100w 级别qps的吞吐量</p>
<p>但是， 后端的logstash 吞吐峰值15000 qps ，es的单节点写入 是 5000 qps 左右，</p>
<p>30K * 100Wqps 的日志吞吐量，如果不希望发生太大的日志延迟， 消息积压，</p>
<p><strong>需要 100+个 logstash 节点， 300+个ES节点</strong></p>
<p><strong>这个需要庞大的资源成本，庞大的运维成本</strong></p>
<h3 id="如何满足10w级、100Wqps吞吐量qps、EB级日志存储呢"><a href="#如何满足10w级、100Wqps吞吐量qps、EB级日志存储呢" class="headerlink" title="如何满足10w级、100Wqps吞吐量qps、EB级日志存储呢"></a>如何满足10w级、100Wqps吞吐量qps、EB级日志存储呢</h3><p><strong>参考23章视频：《100Wqps 超高并发日志平台》实操</strong></p>
<p>如果又要兼顾吞吐量，又要 降低硬件成本和运维成本，必须要</p>
<ul>
<li><p>缩短 日志传输和处理链路，</p>
</li>
<li><p>并采用更高性能，更大压缩比例的存储组件，如clickhouse，</p>
</li>
</ul>
<p>架构如下：</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_6.png"></p>
<p>clickhouse 的数据压缩比例，请参考另外一篇博客：</p>
<p>clickhouse 超底层原理 + 高可用实操 （史上最全）</p>
<p>最终，压缩后的数据，只剩下 原始数据的 20%-30% ， 单数据库这块，减少了50% 的硬盘容量，</p>
<p>使用elk方案，数据有多个副本，包括MQ（主副本2 份），数据库（1 份），现在减少到 数据库（1 份），这里至少减少50% ，</p>
<h1 id="2-一键安装-es-logstash-kibana"><a href="#2-一键安装-es-logstash-kibana" class="headerlink" title="2 一键安装 es+logstash+ kibana"></a>2 一键安装 es+logstash+ kibana</h1><blockquote>
<p>实操过程，请参见23章视频：《100Wqps 超高并发日志平台》实操</p>
</blockquote>
<h3 id="对应的镜像版本"><a href="#对应的镜像版本" class="headerlink" title="对应的镜像版本"></a>对应的镜像版本</h3><blockquote>
<p>elasticsearch:7.14.0 kibana:7.14.0 logstash:7.14.0 filebeat:7.14.0</p>
</blockquote>
<h3 id="docker编码文件"><a href="#docker编码文件" class="headerlink" title="docker编码文件"></a>docker编码文件</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">version: &quot;3.5&quot;  </span><br><span class="line">services:  </span><br><span class="line">  elasticsearch:  </span><br><span class="line">     image: andylsr/elasticsearch-with-ik-icu:7.14.0  </span><br><span class="line">     container\_name: elasticsearch  </span><br><span class="line">     hostname: elasticsearch  </span><br><span class="line">     restart: always  </span><br><span class="line">     ports:  </span><br><span class="line">       - 9200:9200  </span><br><span class="line">     volumes:  </span><br><span class="line">       - ./elasticsearch7/logs:/usr/share/elasticsearch/logs  </span><br><span class="line">       - ./elasticsearch7/data:/usr/share/elasticsearch/data  </span><br><span class="line">       - ./elasticsearch7/config/single-node.yml:/usr/share/elasticsearch/config/elasticsearch.yml  </span><br><span class="line">       - ./elasticsearch7/config/jvm.options:/usr/share/elasticsearch/config/jvm.options  </span><br><span class="line">       - ./elasticsearch7/config/log4j2.properties:/usr/share/elasticsearch/config/log4j2.properties  </span><br><span class="line">     environment:  </span><br><span class="line">       - &quot;ES\_JAVA\_OPTS=-Xms512m -Xmx512m&quot;  </span><br><span class="line">       - &quot;TZ=Asia/Shanghai&quot;  </span><br><span class="line">       - &quot;TAKE\_FILE\_OWNERSHIP=true&quot;   #volumes 挂载权限 如果不想要挂载es文件改配置可以删除  </span><br><span class="line">     ulimits:  </span><br><span class="line">       memlock:  </span><br><span class="line">         soft: -1  </span><br><span class="line">         hard: -1  </span><br><span class="line">     networks:  </span><br><span class="line">       base-env-network:  </span><br><span class="line">         aliases:  </span><br><span class="line">          - elasticsearch  </span><br><span class="line">  kibana:  </span><br><span class="line">    image: docker.elastic.co/kibana/kibana:7.14.0  </span><br><span class="line">    container\_name: kibana  </span><br><span class="line">    volumes:  </span><br><span class="line">      - ./elasticsearch7/config/kibana.yml:/usr/share/kibana/config/kibana.yml  </span><br><span class="line">    ports:  </span><br><span class="line">      - 15601:5601  </span><br><span class="line">    ulimits:  </span><br><span class="line">      nproc: 65535  </span><br><span class="line">      memlock: -1  </span><br><span class="line">    depends\_on:  </span><br><span class="line">       - elasticsearch  </span><br><span class="line">    networks:                      </span><br><span class="line">       base-env-network:  </span><br><span class="line">         aliases:  </span><br><span class="line">          - kibana  </span><br><span class="line">  logstash:  </span><br><span class="line">    image:  logstash:7.14.0  </span><br><span class="line">    container\_name: logstash  </span><br><span class="line">    hostname: logstash  </span><br><span class="line">    restart: always  </span><br><span class="line">    ports:  </span><br><span class="line">      - 19600:9600  </span><br><span class="line">      - 15044:5044  </span><br><span class="line">    volumes:  </span><br><span class="line">      - ./logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:rw  </span><br><span class="line">      - ./logstash/logstash.yml:/usr/share/logstash/config/logstash.yml  </span><br><span class="line">      - ./logstash/data:/home/logstash/data  </span><br><span class="line">    networks:  </span><br><span class="line">       base-env-network:  </span><br><span class="line">         aliases:  </span><br><span class="line">          - logstash  </span><br><span class="line">\# docker network create base-env-network            </span><br><span class="line">networks:  </span><br><span class="line">  base-env-network:  </span><br><span class="line">    external:  </span><br><span class="line">      name: &quot;base-env-network&quot;    </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="访问kibana"><a href="#访问kibana" class="headerlink" title="访问kibana"></a>访问kibana</h3><p><a target="_blank" rel="noopener" href="http://cdh1:15601/">http://cdh1:15601</a></p>
<p>SkyWalking</p>
<p><a target="_blank" rel="noopener" href="http://cdh2:13800/">http://cdh2:13800/</a></p>
<p>kibana</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_7.png"></p>
<blockquote>
<p>以上的实操过程，请参见23章视频：《100Wqps 超高并发日志平台》实操</p>
</blockquote>
<h3 id="在kibana显示的效果"><a href="#在kibana显示的效果" class="headerlink" title="在kibana显示的效果"></a>在kibana显示的效果</h3><p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_8.png"></p>
<p>在kibana组件上查看，可以看到创建了一个filebeat开头的数据索引，如下图:</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_9.png"></p>
<p>在日志搜索界面，可以看到service-hi应用输出的日志，如图所示：</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_10.png"></p>
<h1 id="3-Elasticsearch基础和实操"><a href="#3-Elasticsearch基础和实操" class="headerlink" title="3 Elasticsearch基础和实操"></a>3 Elasticsearch基础和实操</h1><p><strong>ES实操参考第38章：ElasticSearch 学习圣经：从0到1, 精通 ElasticSearch 工业级实操，这里不再重复</strong></p>
<p>Elasticsearch 是一个分布式的开源搜索和分析引擎，在 <em>Apache Lucene</em> 的基础上开发而成。</p>
<p>Lucene 是开源的搜索引擎工具包，Elasticsearch 充分利用Lucene，并对其进行了扩展，使存储、索引、搜索都变得更快、更容易， 而最重要的是， 正如名字中的“ elastic ”所示， 一切都是灵活、有弹性的。而且，应用代码也不是必须用Java 书写才可以和Elasticsearc兼容，完全可以通过JSON 格式的HTTP 请求来进行索引、搜索和管理Elasticsearch 集群。</p>
<p>如果你已经听说过Lucene ，那么可能你也听说了Solr，</p>
<p>Solr也是开源的基于Lucene 的分布式搜索引擎，跟Elasticsearch有很多相似之处。</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_11.png"></p>
<p>但是Solr 诞生于2004 年，而Elasticsearch诞生于2010，Elasticsearch凭借后发优势和更活跃的社区、更完备的生态系统，迅速反超Solr，成为搜索市场的第二代霸主。</p>
<p>Elasticsearch具有以下优势：</p>
<ul>
<li><p><strong>Elasticsearch 很快。</strong> 由于 Elasticsearch 是在 Lucene 基础上构建而成的，所以在全文本搜索方面表现十分出色。Elasticsearch 同时还是一个近实时的搜索平台，这意味着从文档索引操作到文档变为可搜索状态之间的延时很短，一般只有一秒。因此，Elasticsearch 非常适用于对时间有严苛要求的用例，例如安全分析和基础设施监测。</p>
</li>
<li><p><strong>Elasticsearch 具有分布式的本质特征。</strong> Elasticsearch 中存储的文档分布在不同的容器中，这些容器称为_分片_，可以进行复制以提供数据冗余副本，以防发生硬件故障。Elasticsearch 的分布式特性使得它可以扩展至数百台（甚至数千台）服务器，并处理 PB 量级的数据。</p>
</li>
<li><p><strong>Elasticsearch 包含一系列广泛的功能。</strong> 除了速度、可扩展性和弹性等优势以外，Elasticsearch 还有大量强大的内置功能（例如数据汇总和索引生命周期管理），可以方便用户更加高效地存储和搜索数据。</p>
</li>
<li><p><strong>Elastic Stack 简化了数据采集、可视化和报告过程。</strong> 人们通常将 Elastic Stack 称为 <em>ELK Stack_（代指_Elasticsearch_、_Logstash</em> 和 _Kibana_），目前 Elastic Stack 包括一系列丰富的轻量型数据采集代理，这些代理统称为 _Beats_，可用来向 Elasticsearch 发送数据。通过与 Beats 和 Logstash 进行集成，用户能够在向 Elasticsearch 中索引数据之前轻松地处理数据。同时，Kibana 不仅可针对 Elasticsearch 数据提供实时可视化，同时还提供 UI 以便用户快速访问应用程序性能监测 (APM)、日志和基础设施指标等数据。</p>
</li>
</ul>
<h1 id="4-filebeat基础和实操"><a href="#4-filebeat基础和实操" class="headerlink" title="4 filebeat基础和实操"></a>4 filebeat基础和实操</h1><p>当你要面对成百上千、甚至成千上万的服务器、虚拟机和容器生成的日志时，Filebeat 将为你提供一种轻量型方法，用于转发和汇总日志与文件，让简单的事情不再繁杂。</p>
<p>关于Filebeat，记住两点：</p>
<ul>
<li><p><strong>轻量级日志采集器</strong></p>
</li>
<li><p><strong>输送至 Elasticsearch 或 Logstash，在 Kibana 中实现可视化</strong></p>
</li>
</ul>
<p>官网文档<a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/beats/filebeat/7.14/filebeat-overview.html">https://www.elastic.co/guide/en/beats/filebeat/7.14/filebeat-overview.html</a></p>
<h2 id="4-1-filebeat和beats的关系"><a href="#4-1-filebeat和beats的关系" class="headerlink" title="4.1 filebeat和beats的关系"></a>4.1 filebeat和beats的关系</h2><p>filebeat是Beats中的一员。</p>
<p>Beats在是一个轻量级日志采集器，其实Beats家族有6个成员，目前Beats包含六种工具：</p>
<ul>
<li><p>Packetbeat：网络数据（收集网络流量数据）</p>
</li>
<li><p>Metricbeat：指标（收集系统、进程和文件系统级别的CPU和内存使用情况等数据）</p>
</li>
<li><p>Filebeat：日志文件（收集文件数据）</p>
</li>
<li><p>Winlogbeat：windows事件日志（收集Windows事件日志数据）</p>
</li>
<li><p>Auditbeat：审计数据（收集审计日志）</p>
</li>
<li><p>Heartbeat：运行时间监控（收集系统运行时的数据）</p>
</li>
</ul>
<h2 id="4-2-Filebeat工作原理"><a href="#4-2-Filebeat工作原理" class="headerlink" title="4.2 Filebeat工作原理"></a>4.2 Filebeat工作原理</h2><hr>
<p>Filebeat由两个主要组件组成：<strong>inputs</strong> 和  <strong>harvesters</strong> （直译：收割机，采集器）。</p>
<p>这些组件一起工作以跟踪文件，并将事件数据发送到你指定的输出。</p>
<p>Filebeat的工作方式如下：</p>
<blockquote>
<p>启动Filebeat时，它将启动一个或多个输入，这些输入将在为日志数据指定的位置中查找。</p>
</blockquote>
<p>对于Filebeat所找到的每个日志，Filebeat都会启动收割机。</p>
<p>每个收割机都读取一个日志以获取新内容，并将新日志数据发送到libbeat，libbeat会汇总事件并将汇总的数据发送到您为Filebeat配置的输出。</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_12.png"></p>
<p>Filebeat是一个轻量级日志传输Agent，可以将指定日志转发到Logstash、Elasticsearch、Kafka、Redis等中。</p>
<p>Filebeat占用资源少，而且安装配置也比较简单，支持目前各类主流OS及Docker平台。</p>
<p>Filebeat是用于转发和集中日志数据的轻量级传送程序。</p>
<p>作为服务器上的代理安装，Filebeat监视您指定的日志文件或位置，收集日志事件，并将它们转发到Elasticsearch或Logstash进行索引。</p>
<p><strong>harvester是什么</strong></p>
<p>一个harvester负责读取一个单个文件的内容。</p>
<p>harvester逐行读取每个文件（一行一行地读取每个文件），并把这些内容发送到输出。</p>
<p>每个文件启动一个harvester。</p>
<p>harvester负责打开和关闭这个文件，这就意味着在harvester运行时文件描述符保持打开状态。</p>
<p>在harvester正在读取文件内容的时候，文件被删除或者重命名了，那么Filebeat会续读这个文件。</p>
<p>这就有一个问题了，就是只要负责这个文件的harvester没用关闭，那么磁盘空间就不会释放。</p>
<p>默认情况下，Filebeat保存文件打开直到close_inactive到达。</p>
<p><strong>input是什么</strong></p>
<p>一个input负责管理harvesters，并找到所有要读取的源。</p>
<p>如果input类型是log，则input查找驱动器上与已定义的glob路径匹配的所有文件，并为每个文件启动一个harvester。</p>
<p>每个input都在自己的Go例程中运行。</p>
<p>下面的例子配置Filebeat从所有匹配指定的glob模式的文件中读取行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filebeat.inputs:  </span><br><span class="line">\- type: log  </span><br><span class="line">  paths:  </span><br><span class="line">    - /var/log/\*.log  </span><br><span class="line">    - /var/path2/\*.log  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>Filebeat如何保持文件状态</strong></p>
<p>Filebeat 保存每个文件的状态，并经常刷新状态，并且将状态到磁盘上的注册文件（<strong>registry</strong>）。</p>
<p><strong>状态用于记住harvester读取的最后一个偏移量</strong>，并确保所有日志行被发送（到输出）。</p>
<p>如果输出，比如Elasticsearch 或者 Logstash等，无法访问，那么Filebeat会跟踪已经发送的最后一行，并只要输出再次变得可用时继续读取文件。</p>
<p>当Filebeat运行时，会将每个文件的状态新保存在内存中。</p>
<p>当Filebeat重新启动时，将使用注册文件中的数据重新构建状态，Filebeat将在最后一个已知位置继续每个harvester。</p>
<p>对于每个输入，Filebeat保存它找到的每个文件的状态。</p>
<p>因为文件可以重命名或移动，所以文件名和路径不足以标识文件。对于每个文件，Filebeat存储惟一标识符，以检测文件是否以前读取过。</p>
<p>如果你的情况涉及每天创建大量的新文件，你可能会发现注册表文件变得太大了。</p>
<p>（画外音：Filebeat 保存每个文件的状态，并将状态保存到registry_file中的磁盘。当重新启动Filebeat时，文件状态用于在以前的位置继续读取文件。如果每天生成大量新文件，注册表文件可能会变得太大。为了减小注册表文件的大小，有两个配置选项可用：clean_remove 和 clean_inactive。对于你不再访问且被忽略的旧文件，建议您使用clean_inactive。如果想从磁盘上删除旧文件，那么使用clean_remove选项。）</p>
<p><strong>Filebeat如何确保至少投递一次（at-least-once）？</strong></p>
<p>Filebeat保证事件将被投递到配置的输出中至少一次，并且不会丢失数据。</p>
<p>Filebeat能够实现这种行为，因为它将每个事件的投递状态存储在注册文件中。</p>
<p>在定义的输出被阻塞且没有确认所有事件的情况下，Filebeat将继续尝试发送事件，直到输出确认收到事件为止。</p>
<p>如果Filebeat在发送事件的过程中关闭了，则在关闭之前它不会等待输出确认所有事件。当Filebeat重新启动时，发送到输出（但在Filebeat关闭前未确认）的任何事件将再次发送。</p>
<p>这确保每个事件至少被发送一次，但是你最终可能会将重复的事件发送到输出。你可以通过设置shutdown_timeout选项，将Filebeat配置为在关闭之前等待特定的时间。</p>
<h2 id="4-3-Filebeat启动命令"><a href="#4-3-Filebeat启动命令" class="headerlink" title="4.3 Filebeat启动命令"></a>4.3 Filebeat启动命令</h2><p>下载地址：<a target="_blank" rel="noopener" href="https://www.elastic.co/cn/downloads/past-releases#filebeat">https://www.elastic.co/cn/downloads/past-releases#filebeat</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">./filebeat -e -c filebeat 配置文件  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filebeat -e -c /path/to/your/filebeat.yml  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li><p><code>-e</code>：会让 Filebeat 将日志输出到控制台。</p>
</li>
<li><p><code>-c /path/to/your/filebeat.yml</code>：会让 Filebeat 使用指定路径的配置文件，而不是默认的配置文件。</p>
</li>
</ul>
<h2 id="4-4-Filebeat文件夹结构"><a href="#4-4-Filebeat文件夹结构" class="headerlink" title="4.4 Filebeat文件夹结构"></a>4.4 Filebeat文件夹结构</h2><p>|   </p>
<p> | <strong>描述</strong> |<br>| — | — |<br>| filebeat| 用于启动filebeat的二进制文件|<br>| data| 持久化数据文件的位置|<br>| logs| Filebeat创建的日志的位置|<br>| modules.d| 简化filebeat配置的模板文件夹，如nginx&#x2F;kafka等日志收集模板|<br>| filebeat.yml| filebeat配置文件|</p>
<h2 id="4-5-Filebeat配置参考"><a href="#4-5-Filebeat配置参考" class="headerlink" title="4.5 Filebeat配置参考"></a>4.5 Filebeat配置参考</h2><h3 id="输入配置"><a href="#输入配置" class="headerlink" title="输入配置"></a>输入配置</h3><p>为了手动配置Filebeat（代替用模块），你可以在filebeat.yml中的filebeat.inputs区域下指定一个inputs列表。</p>
<p>列表时一个YMAL数组，并且你可以指定多个inputs，相同input类型也可以指定多个。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filebeat.inputs:  </span><br><span class="line">\- type: log  </span><br><span class="line">  paths:  </span><br><span class="line">    - /var/log/system.log  </span><br><span class="line">    - /var/log/wifi.log  </span><br><span class="line">\- type: log  </span><br><span class="line">  paths:  </span><br><span class="line">    - &quot;/var/log/apache2/\*&quot;  </span><br><span class="line">  fields:  </span><br><span class="line">    apache: true  </span><br><span class="line">  fields\_under\_root: true  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Filebeat常见的配置类型参考官网如下：</p>
<ul>
<li><p>AWS CloudWatch</p>
</li>
<li><p>AWS S3</p>
</li>
<li><p>Azure Event Hub</p>
</li>
<li><p>Cloud Foundry</p>
</li>
<li><p>Container</p>
</li>
<li><p>Docker</p>
</li>
<li><p>filestream</p>
</li>
<li><p>GCP Pub&#x2F;Sub</p>
</li>
<li><p>HTTP Endpoint</p>
</li>
<li><p>HTTP JSON</p>
</li>
<li><p>Kafka</p>
</li>
<li><p>Log</p>
</li>
<li><p>MQTT</p>
</li>
<li><p>NetFlow</p>
</li>
<li><p>Office 365 Management Activity API</p>
</li>
<li><p>Redis</p>
</li>
<li><p>Stdin</p>
</li>
<li><p>Syslog</p>
</li>
<li><p>TCP</p>
</li>
<li><p>UDP</p>
</li>
</ul>
<h4 id="log-input"><a href="#log-input" class="headerlink" title="log input"></a>log input</h4><p>从日志文件读取行</p>
<p>为了配置这种input，需要指定一个paths列表，列表中的每一项必须能够定位并抓取到日志行。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filebeat.inputs:  </span><br><span class="line">\- type: log  </span><br><span class="line">  paths:  </span><br><span class="line">    - /var/log/messages  </span><br><span class="line">    - /var/log/\*.log  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>你还可以应用设置其它额外的配置项（比如，fields, include_lines, exclude_lines, multiline等等）来从这些文件中读取行</p>
<p>你设置的这些配置对所有这种类型的input在获取日志行的时候都生效。</p>
<p>为了对不同的文件应用不同的配置，你需要定义多个input区域：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filebeat.inputs:  </span><br><span class="line">\- type: log 　　# 从system.log和wifi.log中读取日志行  </span><br><span class="line">  paths:  </span><br><span class="line">    - /var/log/system.log  </span><br><span class="line">    - /var/log/wifi.log  </span><br><span class="line">\- type: log 　　# 从apache2目录下的每一个文件中读取日志行，并且在输出的时候会加上额外的字段apache  </span><br><span class="line">  paths:  </span><br><span class="line">    - &quot;/var/log/apache2/\*&quot;  </span><br><span class="line">  fields:  </span><br><span class="line">    apache: true  </span><br><span class="line">  fields\_under\_root: true  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="配置项"><a href="#配置项" class="headerlink" title="配置项"></a>配置项</h5><p><strong>paths</strong></p>
<p>例如：&#x2F;var&#x2F;log&#x2F;<em>&#x2F;</em>.log 将会抓取&#x2F;var&#x2F;log子目录目录下所有.log文件。</p>
<p>它不会从&#x2F;var&#x2F;log本身目录下的日志文件。如果你应用recursive_glob设置的话，它将递归地抓取所有子目录下的所有.log文件。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">recursive\_glob.enabled  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>允许将扩展为递归glob模式。</p>
<p>启用这个特性后，每个路径中最右边的<strong>被扩展为固定数量的glob模式。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">例如：/foo/\*\*扩展到/foo， /foo/\*， /foo/\*\*，等等。  </span><br><span class="line">  </span><br><span class="line">如果启用，它将单个\*\*扩展为8级深度\*模式。  </span><br><span class="line">这个特性默认是启用的，设置recursive\_glob.enabled为false可以禁用它。  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>encoding</strong></p>
<p>读取的文件的编码</p>
<p>下面是一些W3C推荐的简单的编码：</p>
<ul>
<li><p>plain, latin1, utf-8, utf-16be-bom, utf-16be, utf-16le, big5, gb18030, gbk, hz-gb-2312</p>
</li>
<li><p>euc-kr, euc-jp, iso-2022-jp, shift-jis, 等等</p>
</li>
</ul>
<p>plain编码是特殊的，因为它不校验或者转换任何输入。</p>
<p><strong>exclude_lines</strong></p>
<p>一组正则表达式，用于匹配你想要排除的行。</p>
<p>Filebeat会删除这组正则表达式匹配的行。默认情况下，没有行被删除。空行被忽略。</p>
<p>如果指定了multiline，那么在用exclude_lines过滤之前会将每个多行消息合并成一个单行。</p>
<p>下面的例子配置Filebeat删除以DBG开头的行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filebeat.inputs:  </span><br><span class="line">\- type: log  </span><br><span class="line">  ...  </span><br><span class="line">  exclude\_lines: \[&#x27;^DBG&#x27;\]  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>include_lines</strong></p>
<p>一组正则表达式，用于匹配你想要包含的行。Filebeat只会导出那些匹配这组正则表达式的行。默认情况下，所有的行都会被导出。空行被忽略。</p>
<p>如果指定了multipline设置，每个多行消息先被合并成单行以后再执行include_lines过滤。</p>
<p>下面是一个例子，配置Filebeat导出以ERR或者WARN开头的行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filebeat.inputs:  </span><br><span class="line">\- type: log  </span><br><span class="line">  ...  </span><br><span class="line">  include\_lines: \[&#x27;^ERR&#x27;, &#x27;^WARN&#x27;\]  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>（画外音：如果 include_lines 和 exclude_lines 都被定义了，那么Filebeat先执行 include_lines 后执行 exclude_lines，而与这两个选项被定义的顺序没有关系。include_lines 总是在 exclude_lines选项前面执行，即使在配置文件中 exclude_lines 出现在 include_lines的前面。）</p>
<p>下面的例子导出那些除了以DGB开头的所有包含sometext的行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filebeat.inputs:  </span><br><span class="line">\- type: log  </span><br><span class="line">  ...  </span><br><span class="line">  include\_lines: \[&#x27;sometext&#x27;\]  </span><br><span class="line">  exclude\_lines: \[&#x27;^DBG&#x27;\]  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>harvester_buffer_size</strong></p>
<p>当抓取一个文件时每个harvester使用的buffer的字节数。默认是16384。</p>
<p><strong>max_bytes</strong></p>
<p><strong>单个日志消息允许的最大字节数。超过max_bytes的字节将被丢弃且不会被发送。对于多行日志消息来说这个设置是很有用的，因为它们往往很大。默认是10MB（10485760）。</strong></p>
<p><strong>json</strong></p>
<p>这些选项使得Filebeat将日志作为JSON消息来解析。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">json.keys\_under\_root: true  </span><br><span class="line">json.add\_error\_key: true  </span><br><span class="line">json.message\_key: log  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>为了启用JSON解析模式，你必须至少指定下列设置项中的一个：</p>
<p>keys_under_root</p>
<p>默认情况下，解码后的JSON被放置在一个以”json”为key的输出文档中。如果你启用这个设置，那么这个key在文档中被复制为顶级。默认是false。</p>
<p>overwrite_keys</p>
<p>如果keys_under_root被启用，那么在key冲突的情况下，解码后的JSON对象将覆盖Filebeat正常的字段</p>
<p>add_error_key</p>
<p>如果启用，则当JSON反编排出现错误的时候Filebeat添加 “error.message” 和 “error.type: json”两个key，或者当没有使用message_key的时候。</p>
<p>message_key</p>
<p>一个可选的配置，用于在应用行过滤和多行设置的时候指定一个JSON key。指定的这个key必须在JSON对象中是顶级的，而且其关联的值必须是一个字符串，否则没有过滤或者多行聚集发送。</p>
<p>ignore_decoding_error</p>
<p>一个可选的配置，用于指定是否JSON解码错误应该被记录到日志中。如果设为true，错误将被记录。默认是false。</p>
<p><strong>multiline</strong></p>
<p>用于控制Filebeat如何扩多行处理日志消息</p>
<p><strong>exclude_files</strong></p>
<p>一组正则表达式，用于匹配你想要忽略的文件。默认没有文件被排除。</p>
<p>下面是一个例子，忽略.gz的文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filebeat.inputs:  </span><br><span class="line">\- type: log  </span><br><span class="line">  ...  </span><br><span class="line">  exclude\_files: \[&#x27;\\.gz$&#x27;\]  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>ignore_older</strong></p>
<p>如果启用，那么Filebeat会忽略在指定的时间跨度之前被修改的文件。如果你想要保留日志文件一个较长的时间，那么配置ignore_older是很有用的。例如，如果你想要开始Filebeat，但是你只想发送最近一周最新的文件，这个情况下你可以配置这个选项。</p>
<p>你可以用时间字符串，比如2h（2小时），5m（5分钟）。默认是0，意思是禁用这个设置。</p>
<p>你必须设置<strong>ignore_older</strong>比<strong>close_inactive</strong>更大。</p>
<p><strong>close_*</strong></p>
<p>close_*配置项用于在一个确定的条件或者时间点之后关闭harvester。关闭harvester意味着关闭文件处理器。如果在harvester关闭以后文件被更新，那么在scan_frequency结束后改文件将再次被拾起。然而，当harvester关闭的时候如果文件被删除或者被移动，那么Filebeat将不会被再次拾起，并且这个harvester还没有读取的数据将会丢失。</p>
<p><strong>close_inactive</strong></p>
<p>当启用此选项时，如果文件在指定的持续时间内未被获取，则Filebeat将关闭文件句柄。当harvester读取最后一行日志时，指定周期的计数器就开始工作了。它不基于文件的修改时间。如果关闭的文件再次更改，则会启动一个新的harvester，并且在scan_frequency结束后，将获得最新的更改。</p>
<p>推荐给close_inactive设置一个比你的日志文件更新的频率更大一点儿的值。例如，如果你的日志文件每隔几秒就会更新，你可以设置close_inactive为1m。如果日志文件的更新速率不固定，那么可以用多个配置。</p>
<p>将close_inactive设置为更低的值意味着文件句柄可以更早关闭。然而，这样做的副作用是，如果harvester关闭了，新的日志行不会实时发送。</p>
<p>关闭文件的时间戳不依赖于文件的修改时间。代替的，Filebeat用一个内部时间戳来反映最后一次读取文件的时间。例如，如果close_inactive被设置为5分钟，那么在harvester读取文件的最后一行以后，这个5分钟的倒计时就开始了。</p>
<p>你可以用时间字符串，比如2h（2小时），5m（5分钟）。默认是5m。</p>
<p><strong>close_renamed</strong></p>
<p>当启用此选项时，Filebeat会在重命名文件时关闭文件处理器。默认情况下，harvester保持打开状态并继续读取文件，因为文件处理器不依赖于文件名。如果启用了close_rename选项，并且重命名或者移动的文件不再匹配文件模式的话，那么文件将不会再次被选中。Filebeat将无法完成文件的读取。</p>
<p><strong>close_removed</strong></p>
<p>当启用此选项时，Filebeat会在删除文件时关闭harvester。通常，一个文件只有在它在由close_inactive指定的期间内不活跃的情况下才会被删除。但是，如果一个文件被提前删除，并且你不启用close_removed，则Filebeat将保持文件打开，以确保harvester已经完成。如果由于文件过早地从磁盘中删除而导致文件不能完全读取，请禁用此选项。</p>
<p><strong>close_timeout</strong></p>
<p>当启用此选项是，Filebeat会给每个harvester一个预定义的生命时间。无论读到文件的什么位置，只要close_timeout周期到了以后就会停止读取。当你想要在文件上只花费预定义的时间时，这个选项对旧的日志文件很有用。尽管在close_timeout时间以后文件就关闭了，但如果文件仍然在更新，则Filebeat将根据已定义的scan_frequency再次启动一个新的harvester。这个harvester的close_timeout将再次启动，为超时倒计时。</p>
<p><strong>scan_frequency</strong></p>
<p>Filebeat多久检查一次指定路径下的新文件（PS：检查的频率）。例如，如果你指定的路径是 &#x2F;var&#x2F;log&#x2F;* ，那么会以指定的scan_frequency频率去扫描目录下的文件（PS：周期性扫描）。指定1秒钟扫描一次目录，这还不是很频繁。不建议设置为小于1秒。</p>
<p>如果你需要近实时的发送日志行的话，不要设置scan_frequency为一个很低的值，而应该调整close_inactive以至于文件处理器保持打开状态，并不断地轮询你的文件。</p>
<p>默认是10秒。</p>
<p><strong>scan.sort</strong></p>
<p>如果你指定了一个非空的值，那么你可以决定用scan.order的升序或者降序。可能的值是 modtime 和 filename。为了按文件修改时间排序，用modtime，否则用 filename。默认此选项是禁用的。</p>
<p><strong>scan.order</strong></p>
<p>可能的值是 asc 或者 desc。默认是asc。</p>
<p>更多配置请查看</p>
<p><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-options.html">https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-options.html</a></p>
<p>这里再重点说一下 ignore_older , close_inactive , scan_frequency 这三个配置项</p>
<ul>
<li><p>ignore_older： 它是设置一个时间范围（跨度），不在这个跨度范围之内的文件更新都不管</p>
</li>
<li><p>scan_frequency： 它设置的是扫描文件的频率，看看文件是否更新</p>
</li>
<li><p>close_inactive：它设置的是文件如果多久没更新的话就关闭文件句柄，它是有一个倒计时，如果在倒计时期间，文件没有任何变化，则当倒计时结束的时候关闭文件句柄。不建议设置为小于1秒。</p>
</li>
</ul>
<p>如果文件句柄关了以后，文件又被更新，那么在下一个扫描周期结束的时候变化发现这个改变，于是会再次打开这个文件读取日志行，前面我们也提到过，每个文件上一次读到什么位置（偏移量）都记录在registry文件中。</p>
<h5 id="特别说明：multiline管理多行消息"><a href="#特别说明：multiline管理多行消息" class="headerlink" title="特别说明：multiline管理多行消息"></a>特别说明：multiline管理多行消息</h5><p>Filebeat获取的文件可能包含跨多行文本的消息。</p>
<p>例如，多行消息在包含Java堆栈跟踪的文件中很常见。为了正确处理这些多行事件，你需要在filebeat.yml中配置multiline以指定哪一行是单个事件的一部分。</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_13.png"></p>
<p>你可以在filebeat.yml的filebeat.inputs区域指定怎样处理跨多行的消息。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">multiline.pattern: &#x27;^\\\[&#x27;  </span><br><span class="line">multiline.negate: true  </span><br><span class="line">multiline.match: after  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>上面的例子中，Filebeat将所有不以 [ 开始的行与之前的行进行合并。</p>
<p><strong>multiline.pattern</strong></p>
<p>指定用于匹配多行的正则表达式</p>
<p><strong>multiline.negate</strong></p>
<p>定义模式是否被否定。默认false。</p>
<p><strong>multiline.match</strong></p>
<p>指定Filebeat如何把多行合并成一个事件。可选的值是 <strong>after</strong> 或者 <strong>before</strong>。</p>
<p>这种行为还收到negate的影响：</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_14.png"></p>
<p><strong>multiline.flush_pattern</strong></p>
<p>指定一个正则表达式，多行将从内存刷新到磁盘。</p>
<p><strong>multiline.max_lines</strong></p>
<p>可以合并成一个事件的最大行数。如果一个多行消息包含的行数超过max_lines，则超过的行被丢弃。默认是500。</p>
<h3 id="输出配置"><a href="#输出配置" class="headerlink" title="输出配置"></a>输出配置</h3><p>常见的输出配置类型，参考官网如下：</p>
<ul>
<li><p>Elasticsearch Service</p>
</li>
<li><p>Elasticsearch</p>
</li>
<li><p>Logstash</p>
</li>
<li><p>Kafka</p>
</li>
<li><p>Redis</p>
</li>
<li><p>File</p>
</li>
<li><p>Console</p>
</li>
</ul>
<h4 id="Logstash-output"><a href="#Logstash-output" class="headerlink" title="Logstash output"></a>Logstash output</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">output.logstash:  </span><br><span class="line">  hosts: \[&quot;127.0.0.1:5044&quot;\]  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>上面是配置Filebeat输出到Logstash，那么Logstash本身也有配置，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">input &#123;  </span><br><span class="line">  beats &#123;  </span><br><span class="line">    port =&gt; 5044  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">output &#123;  </span><br><span class="line">  elasticsearch &#123;  </span><br><span class="line">    hosts =&gt; \[&quot;http://localhost:9200&quot;\]  </span><br><span class="line">    index =&gt; &quot;%&#123;\[@metadata\]\[beat\]&#125;-%&#123;\[@metadata\]\[version\]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;   </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>负载均衡</strong></p>
<p>为了启用负载均衡，当你配置输出的时候你需要指定 <strong>loadbalance: true</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">output.logstash:  </span><br><span class="line">  hosts: \[&quot;localhost:5044&quot;, &quot;localhost:5045&quot;\]  </span><br><span class="line">  loadbalance: true  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="4-6-实操案例"><a href="#4-6-实操案例" class="headerlink" title="4.6 实操案例"></a>4.6 实操案例</h2><h3 id="4-6-1-标准输入，控制台输出"><a href="#4-6-1-标准输入，控制台输出" class="headerlink" title="4.6.1 标准输入，控制台输出"></a>4.6.1 标准输入，控制台输出</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">#安装  </span><br><span class="line">yum \-y localinstall filebeat-7.17.3-x86\_64.rpm  </span><br><span class="line">  </span><br><span class="line">mkdir ~/filebeat/config  </span><br><span class="line">cat \&gt; ~/filebeat/config/01-stdin-to-console.yml &lt;&lt;&#x27;EOF&#x27;  </span><br><span class="line">\# 指定输⼊的类型  </span><br><span class="line">filebeat.inputs:  </span><br><span class="line">\# 指定输⼊的类型为&quot;stdin&quot;,表示标准输⼊  </span><br><span class="line">\- type: stdin  </span><br><span class="line">\# 指定输出的类型  </span><br><span class="line">output.console:  </span><br><span class="line">  \# 打印漂亮的格式  </span><br><span class="line">  pretty: true  </span><br><span class="line">EOF  </span><br><span class="line">  </span><br><span class="line">#运⾏filebeat实例  </span><br><span class="line">~/filebeat/filebeat \-e \-c ~/filebeat/config/01-stdin-to-console.yml  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_15.png"></p>
<h3 id="4-6-2-日志输入，控制台输出"><a href="#4-6-2-日志输入，控制台输出" class="headerlink" title="4.6.2 日志输入，控制台输出"></a>4.6.2 日志输入，控制台输出</h3><p>filebeat配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filebeat.inputs:  </span><br><span class="line">  - type: log  </span><br><span class="line">    # 是否启动当前的输入类型，默认值为true  </span><br><span class="line">    enabled: true  </span><br><span class="line">    # 指定数据路径  </span><br><span class="line">    paths:  </span><br><span class="line">      - /tmp/nginx.log  </span><br><span class="line">      - /tmp/\*.txt  </span><br><span class="line">    # 给当前的输入类型打上标签  </span><br><span class="line">    tags: \[&quot;elk\_nginx&quot;,&quot;容器运维&quot;,&quot;DBA运维&quot;,&quot;SRE运维工程师&quot;\]  </span><br><span class="line">    # 自定义字段  </span><br><span class="line">    fields:  </span><br><span class="line">      school: &quot;北京昌平区沙河镇&quot;  </span><br><span class="line">      class: &quot;linux80&quot;  </span><br><span class="line">  - type: log  </span><br><span class="line">    enabled: true  </span><br><span class="line">    paths:  </span><br><span class="line">      - /tmp/tomcat.log  </span><br><span class="line">    tags: \[&quot;elk\_tomcat&quot;,&quot;云原生开发&quot;\]  </span><br><span class="line">    fields:  </span><br><span class="line">      name: &quot;mickael&quot;  </span><br><span class="line">      hobby: &quot;linux,抖音&quot;  </span><br><span class="line">    # 将自定义字段的key-value放到顶级字段.  </span><br><span class="line">    # 默认值为false，会将数据放在一个叫&quot;fields&quot;字段的下面.  </span><br><span class="line">    fields\_under\_root: true  </span><br><span class="line">output.console:  </span><br><span class="line">  pretty: true  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>测试命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">echo aaa &gt; /tmp/test.log  </span><br><span class="line">  echo bbbb &gt; /tmp/test.log  </span><br><span class="line">  echo bbbb &gt;&gt; /tmp/test.log  </span><br><span class="line">  echo 123 &gt; /tmp/test.log  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>结果</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_16.png"></p>
<h3 id="4-6-3-日志输入，ES输出"><a href="#4-6-3-日志输入，ES输出" class="headerlink" title="4.6.3 日志输入，ES输出"></a>4.6.3 日志输入，ES输出</h3><p>filebeat配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filebeat.inputs:  </span><br><span class="line">\- type: log  </span><br><span class="line">  \# 是否启动当前的输入类型，默认值为true  </span><br><span class="line">  enabled: true  </span><br><span class="line">  \# 指定数据路径  </span><br><span class="line">  paths:  </span><br><span class="line">    \- /tmp/nginx.log  </span><br><span class="line">    \- /tmp/\*.txt  </span><br><span class="line">  \# 给当前的输入类型打上标签  </span><br><span class="line">  tags: \[&quot;elk\_nginx&quot;,&quot;容器运维&quot;,&quot;DBA运维&quot;,&quot;SRE运维工程师&quot;\]  </span><br><span class="line">  \# 自定义字段  </span><br><span class="line">  fields:  </span><br><span class="line">    school: &quot;北京昌平区沙河镇&quot;  </span><br><span class="line">    class: &quot;linux80&quot;  </span><br><span class="line">\- type: log  </span><br><span class="line">  enabled: true  </span><br><span class="line">  paths:  </span><br><span class="line">    \- /tmp/tomcat.log  </span><br><span class="line">  tags: \[&quot;elk\_tomcat&quot;,&quot;云原生开发&quot;\]  </span><br><span class="line">  fields:  </span><br><span class="line">    name: &quot;mickael&quot;  </span><br><span class="line">    hobby: &quot;linux,抖音&quot;  </span><br><span class="line">  \# 将自定义字段的key-value放到顶级字段.  </span><br><span class="line">  \# 默认值为false，会将数据放在一个叫&quot;fields&quot;字段的下面.  </span><br><span class="line">  fields\_under\_root: true  </span><br><span class="line">  </span><br><span class="line">output.elasticsearch:  </span><br><span class="line">  enabled: true  </span><br><span class="line">  hosts:  </span><br><span class="line">    \[&quot;http://localhost:9200&quot;\]  </span><br><span class="line">  \# index: &quot;elk\_nginx-%&#123;+yyyy.MM.dd&#125;&quot;  </span><br><span class="line">  indices:  </span><br><span class="line">    \- index: &quot;elk\_nginx-%&#123;+yyyy.MM.dd&#125;&quot;  </span><br><span class="line">      \# 匹配指定字段包含的内容  </span><br><span class="line">      when.contains:  </span><br><span class="line">        tags: &quot;elk\_nginx&quot;  </span><br><span class="line">    \- index: &quot;elk\_tomcat-%&#123;+yyyy.MM.dd&#125;&quot;  </span><br><span class="line">      when.contains:  </span><br><span class="line">        tags: &quot;elk\_tomcat&quot;  </span><br><span class="line">  \# 禁⽤索引⽣命周期管理  </span><br><span class="line">  setup.ilm.enabled: false  </span><br><span class="line">  \# 设置索引模板的名称  </span><br><span class="line">  setup.template.name: &quot;elk\_&quot;  </span><br><span class="line">  \# 设置索引模板的匹配模式  </span><br><span class="line">  setup.template.pattern: &quot;elk\_\*&quot;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>测试命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">echo nginx\_log &gt; nginx.log  </span><br><span class="line">echo tomcat\_log &gt; tomcat.log  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>结果</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_17.png"></p>
<h3 id="4-6-4-企业级实战案例：Tomcat日志"><a href="#4-6-4-企业级实战案例：Tomcat日志" class="headerlink" title="4.6.4 企业级实战案例：Tomcat日志"></a>4.6.4 企业级实战案例：Tomcat日志</h3><h4 id="4-6-4-1-原生tomcat日志"><a href="#4-6-4-1-原生tomcat日志" class="headerlink" title="4.6.4.1 原生tomcat日志"></a>4.6.4.1 原生tomcat日志</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filebeat.inputs:  </span><br><span class="line">\- type: log  </span><br><span class="line">  enabled: true  </span><br><span class="line">  paths:  </span><br><span class="line">    \- /root/apache-tomcat-9.0.97/logs/\*.txt  </span><br><span class="line">  tags: \[&quot;access&quot;\]  </span><br><span class="line">  </span><br><span class="line">output.elasticsearch:  </span><br><span class="line">  enabled: true  </span><br><span class="line">  hosts:  </span><br><span class="line">    \[&quot;http://localhost:9200&quot;\]  </span><br><span class="line">  index: &quot;elk\_tomcat-%&#123;+yyyy.MM.dd&#125;&quot;  </span><br><span class="line">  \# 禁⽤索引⽣命周期管理  </span><br><span class="line">  setup.ilm.enabled: false  </span><br><span class="line">  \# 设置索引模板的名称  </span><br><span class="line">  setup.template.name: &quot;elk\_&quot;  </span><br><span class="line">  \# 设置索引模板的匹配模式  </span><br><span class="line">  setup.template.pattern: &quot;elk\_\*&quot;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_18.png"></p>
<p>可以看到访问日志，是原生的字符串，存储在一起的，比如想要获取响应码&#x3D;200的日志，就不方便了</p>
<p>对于原生的日志，进行解析分析，基本2种方案</p>
<ol>
<li><p>应用设置日志格式为json，然后配置<code>json.keys_under_root: true</code>解析json格式日志，比如spirng boot应用的</p>
</li>
<li><p>使用logstash</p>
</li>
</ol>
<h4 id="4-6-4-2-json格式tomcat日志"><a href="#4-6-4-2-json格式tomcat日志" class="headerlink" title="4.6.4.2 json格式tomcat日志"></a>4.6.4.2 json格式tomcat日志</h4><p>先看方案1，方案2参考logstash章节</p>
<p>方案1，设置tomcat日志格式配置如下，conf&#x2F;server.xml</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;Valve className\=&quot;org.apache.catalina.valves.AccessLogValve&quot; directory\=&quot;logs&quot;  </span><br><span class="line">                prefix\=&quot;localhost\_access\_log&quot; suffix\=&quot;.txt&quot;  </span><br><span class="line">                pattern\=&quot;&#123;&amp;quot;clientip&amp;quot;:&amp;quot;%h&amp;quot;,&amp;quot;ClientUser&amp;quot;:&amp;quot;%l&amp;quot;,&amp;quot;authenticated&amp;quot;:&amp;quot;%u&amp;quot;,&amp;quot;AccessTime&amp;quot;:&amp;quot;%t&amp;quot;,&amp;quot;request&amp;quot;:&amp;quot;%r&amp;quot;,&amp;quot;status&amp;quot;:&amp;quot;%s&amp;quot;,&amp;quot;SendBytes&amp;quot;:&amp;quot;%b&amp;quot;,&amp;quot;Query?string&amp;quot;:&amp;quot;%q&amp;quot;,&amp;quot;partner&amp;quot;:&amp;quot;%&#123;Referer&#125;i&amp;quot;,&amp;quot;http\_user\_agent&amp;quot;:&amp;quot;%&#123;UserAgent&#125;i&amp;quot;&#125;&quot; /&gt;  </span><br><span class="line">  </span><br><span class="line">        &lt;!--pattern=&quot;%h %l %u %t &amp;quot;%r&amp;quot; %s %b&quot; --&gt;  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>解析tomcat的json日志的filebeat配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filebeat.inputs:  </span><br><span class="line">\- type: log  </span><br><span class="line">  enabled: true  </span><br><span class="line">  paths:  </span><br><span class="line">    \- /root/apache-tomcat-9.0.97/logs/\*.txt  </span><br><span class="line">  \# 解析message字段的json格式，并放在顶级字段中  </span><br><span class="line">  json.keys\_under\_root: true  </span><br><span class="line">  tags: \[&quot;access&quot;\]  </span><br><span class="line">  </span><br><span class="line">output.elasticsearch:  </span><br><span class="line">  enabled: true  </span><br><span class="line">  hosts:  </span><br><span class="line">    \[&quot;http://localhost:9200&quot;\]  </span><br><span class="line">  index: &quot;elk\_tomcat-%&#123;+yyyy.MM.dd&#125;&quot;  </span><br><span class="line">\# 禁⽤索引⽣命周期管理  </span><br><span class="line">setup.ilm.enabled: false  </span><br><span class="line">\# 设置索引模板的名称  </span><br><span class="line">setup.template.name: &quot;elk\_&quot;  </span><br><span class="line">\# 设置索引模板的匹配模式  </span><br><span class="line">setup.template.pattern: &quot;elk\_\*&quot;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>结果：</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_19.png"></p>
<h4 id="4-6-4-3-多行匹配，收集tomcat错误日志"><a href="#4-6-4-3-多行匹配，收集tomcat错误日志" class="headerlink" title="4.6.4.3 多行匹配，收集tomcat错误日志"></a>4.6.4.3 多行匹配，收集tomcat错误日志</h4><p>Filebeat获取的文件可能包含跨多行文本的消息。</p>
<p>例如，多行消息在包含Java堆栈跟踪的文件中很常见。为了正确处理这些多行事件，你需要在filebeat.yml中配置multiline以指定哪一行是单个事件的一部分。</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_20.png"></p>
<p>你可以在filebeat.yml的filebeat.inputs区域指定怎样处理跨多行的消息。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">multiline.pattern: &#x27;^\\\[&#x27;  </span><br><span class="line">multiline.negate: true  </span><br><span class="line">multiline.match: after  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>上面的例子中，Filebeat将所有不以 [ 开始的行与之前的行进行合并。</p>
<p><strong>multiline.pattern</strong></p>
<p>指定用于匹配多行的正则表达式</p>
<p><strong>multiline.negate</strong></p>
<p>定义模式是否被否定。默认false。</p>
<p><strong>multiline.match</strong></p>
<p>指定Filebeat如何把多行合并成一个事件。可选的值是 <strong>after</strong> 或者 <strong>before</strong>。</p>
<p>这种行为还收到negate的影响：</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_21.png"></p>
<p><strong>multiline.flush_pattern</strong></p>
<p>指定一个正则表达式，多行将从内存刷新到磁盘。</p>
<p><strong>multiline.max_lines</strong></p>
<p>可以合并成一个事件的最大行数。如果一个多行消息包含的行数超过max_lines，则超过的行被丢弃。默认是500。</p>
<p>filebeat配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filebeat.inputs:  </span><br><span class="line">\- type: log  </span><br><span class="line">  enabled: true  </span><br><span class="line">  paths:  </span><br><span class="line">    \- /root/apache-tomcat-9.0.97/logs/\*.out  </span><br><span class="line">  \# 指定多⾏匹配的类型，可选值为&quot;pattern&quot;,&quot;count&quot;  </span><br><span class="line">  multiline.type: pattern  </span><br><span class="line">  \# 指定匹配模式  </span><br><span class="line">  multiline.pattern: &#x27;^\\d&#123;2&#125;&#x27;  </span><br><span class="line">  \# 下⾯2个参数参考官⽅架构图即可，如上图所示。  </span><br><span class="line">  multiline.negate: true  </span><br><span class="line">  multiline.match: after  </span><br><span class="line">  tags: \[&quot;access&quot;\]  </span><br><span class="line">  </span><br><span class="line">output.elasticsearch:  </span><br><span class="line">  enabled: true  </span><br><span class="line">  hosts:  </span><br><span class="line">    \[&quot;http://localhost:9200&quot;\]  </span><br><span class="line">  index: &quot;elk\_tomcat-%&#123;+yyyy.MM.dd&#125;&quot;  </span><br><span class="line">\# 禁⽤索引⽣命周期管理  </span><br><span class="line">setup.ilm.enabled: false  </span><br><span class="line">\# 设置索引模板的名称  </span><br><span class="line">setup.template.name: &quot;elk\_&quot;  </span><br><span class="line">\# 设置索引模板的匹配模式  </span><br><span class="line">setup.template.pattern: &quot;elk\_\*&quot;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_21_1.png"></p>
<h1 id="5-logstash基础和实操"><a href="#5-logstash基础和实操" class="headerlink" title="5 logstash基础和实操"></a>5 logstash基础和实操</h1><p>简单来说logstash就是一根具备实时数据传输能力的管道，负责将数据信息从管道的输入端传输到管道的输出端；与此同时这根管道还可以让你根据自己的需求在中间加上滤网，Logstash提供里很多功能强大的滤网以满足你的各种应用场景。</p>
<p>logstash常用于日志系统中做日志采集设备，最常用于ELK中作为日志收集器使用，比如对于非结构化日志数据的解析和处理，就可以使用logstash</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_22.png"></p>
<h2 id="5-1-logstash作用："><a href="#5-1-logstash作用：" class="headerlink" title="5.1 logstash作用："></a>5.1 logstash作用：</h2><p>集中、转换和存储你的数据，是一个开源的服务器端数据处理管道，可以同时从多个数据源获取数据，并对其进行转换，然后将其发送到你最喜欢的“存储</p>
<h2 id="5-2-logstash的架构："><a href="#5-2-logstash的架构：" class="headerlink" title="5.2 logstash的架构："></a>5.2 logstash的架构：</h2><p>logstash的基本流程架构：input | filter | output 如需对数据进行额外处理，filter可省略。logstash架构和filebeat很想，也可以说filebeat不分功能模仿的logstash</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_23.png"></p>
<h3 id="Input-输入）："><a href="#Input-输入）：" class="headerlink" title="Input(输入）："></a>Input(输入）：</h3><p><strong>采集各种样式，大小和相关来源数据，从各个服务器中收集数据。</strong></p>
<p>数据往往以各种各样的形式，或分散或集中地存在于很多系统中。 Logstash 支持各种输入选择 ，可以在同一时间从众多常用来源捕捉事件。 能够以连续的流式传输方式，轻松地从您的日志、指标、Web 应用、数据存储以及各种 AWS 服务采集数据。<br><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_24.png"></p>
<p>input：必须，负责产生事件（Inputs generate events），</p>
<p>常用：File、syslog、redis、beats（如：Filebeats）</p>
<h3 id="Filter-过滤器）"><a href="#Filter-过滤器）" class="headerlink" title="Filter(过滤器）"></a>Filter(过滤器）</h3><p>用于在将event通过output发出之前，对其实现某些处理功能。</p>
<p>filters：可选，负责数据处理与转换（filters modify them），</p>
<p>常用：grok、mutate、drop、clone、geoip</p>
<p>grok：用于分析结构化文本数据。</p>
<h3 id="Output-输出）："><a href="#Output-输出）：" class="headerlink" title="Output(输出）："></a>Output(输出）：</h3><p>将我们过滤出的数据保存到那些数据库和相关存储中。<br><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_25.png"></p>
<p>outputs：必须，负责数据输出（outputs ship them elsewhere），</p>
<p>常用：elasticsearch、file、graphite、statsd</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_26.png"><br><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_26_1.png"></p>
<h2 id="5-3-Logstash的不足"><a href="#5-3-Logstash的不足" class="headerlink" title="5.3 Logstash的不足"></a>5.3 Logstash的不足</h2><p>早期的ELK架构中使用Logstash收集、解析日志，</p>
<p><strong>但是：Logstash对内存、cpu、io等资源消耗比较高。</strong></p>
<p>相比Logstash，Beats所占系统的CPU和内存几乎可以忽略不计。</p>
<p>所以，在收集这块，一般使用filebeat 代替 Logstash</p>
<h2 id="5-4-logstash读取filebeat-输出到es集群"><a href="#5-4-logstash读取filebeat-输出到es集群" class="headerlink" title="5.4 logstash读取filebeat-输出到es集群"></a>5.4 logstash读取filebeat-输出到es集群</h2><p>在分布式系统中，一台主机可能有多个应用，应用将日志输出到主机的指定目录，这时由logstash来搬运日志并解析日志，然后输出到elasticsearch上。</p>
<p>由于于logstash是java应用，解析日志是非的消耗cpu和内存，logstash安装在应用部署的机器上显得非常的笨重。</p>
<p>最常见的做法是用filebeat部署在应用的机器上，logstash单独部署，然后由filebeat将日志输出给logstash解析，解析完由logstash再传给elasticsearch。</p>
<p>在上面的配置中，输入数据源为filebeat，输出源为elasticsearch。</p>
<p>修改logstash的安装目录的config目录下的logstash.conf文件，配置如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">input &#123;  </span><br><span class="line">  beats &#123;  </span><br><span class="line">    port =&gt; &quot;5044&quot;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">filter &#123;  </span><br><span class="line">      </span><br><span class="line">    if &quot;message-dispatcher&quot; in \[tags\]&#123;  </span><br><span class="line">        grok &#123;  </span><br><span class="line">            match =&gt; \[&quot;message&quot;, &quot;%&#123;TIMESTAMP\_ISO8601:time&#125;\\s\* \\s\*%&#123;NOTSPACE:thread-id&#125;\\s\* \\s\*%&#123;LOGLEVEL:level&#125;\\s\* \\s\*%&#123;JAVACLASS:class&#125;\\s\* \\- \\s\*%&#123;JAVALOGMESSAGE:logmessage&#125;\\s\*&quot;\]  </span><br><span class="line">        &#125;  </span><br><span class="line">          </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    if &quot;ExampleApplication&quot; in \[tags\]&#123;  </span><br><span class="line">        grok &#123;  </span><br><span class="line">            match =&gt; \[&quot;message&quot;, &quot;%&#123;TIMESTAMP\_ISO8601:time&#125;\\s\* \\s\*%&#123;NOTSPACE:thread-id&#125;\\s\* \\s\*%&#123;LOGLEVEL:level&#125;\\s\* \\s\*%&#123;JAVACLASS:class&#125;\\s\* \\- \\s\*%&#123;JAVALOGMESSAGE:logmessage&#125;\\s\*&quot;\]  </span><br><span class="line">        &#125;  </span><br><span class="line">          </span><br><span class="line">    &#125;  </span><br><span class="line">    mutate &#123;  </span><br><span class="line">        remove\_field =&gt; &quot;log&quot;  </span><br><span class="line">        remove\_field =&gt; &quot;beat&quot;  </span><br><span class="line">        remove\_field =&gt; &quot;meta&quot;  </span><br><span class="line">        remove\_field =&gt; &quot;prospector&quot;  </span><br><span class="line">        remove\_field =&gt; &quot;\[host\]\[os\]&quot;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">output &#123;  </span><br><span class="line">    stdout &#123; codec =&gt; rubydebug &#125;  </span><br><span class="line">    if &quot;message-dispatcher&quot; in \[tags\]&#123;  </span><br><span class="line">        elasticsearch &#123;  </span><br><span class="line">           hosts =&gt; \[ &quot;elasticsearch:9200&quot; \]  </span><br><span class="line">           index =&gt; &quot;message-dispatcher-%&#123;+yyyy.MM.dd&#125;&quot;        </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    if &quot;ExampleApplication&quot; in \[tags\]&#123;  </span><br><span class="line">        elasticsearch &#123;  </span><br><span class="line">           hosts =&gt; \[ &quot;elasticsearch:9200&quot; \]  </span><br><span class="line">           index =&gt; &quot;ExampleApplication-%&#123;+yyyy.MM.dd&#125;&quot;        </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;      </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>更多的输入和输出源的配置见官网</p>
<p><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html">https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html</a></p>
<h2 id="5-5-logstash实操案例"><a href="#5-5-logstash实操案例" class="headerlink" title="5.5 logstash实操案例"></a>5.5 logstash实操案例</h2><h3 id="5-5-1-标准输入，标准输出"><a href="#5-5-1-标准输入，标准输出" class="headerlink" title="5.5.1 标准输入，标准输出"></a>5.5.1 标准输入，标准输出</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">#安装  </span><br><span class="line">yum -y localinstall logstash-7.17.3-x86\_64.rpm  </span><br><span class="line">ln -sv /usr/share/logstash/bin/logstash /usr/local/bin/  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">cat &gt; 01-stdin-to-stdout.conf &lt;&lt;&#x27;EOF&#x27;  </span><br><span class="line">input &#123;  </span><br><span class="line">  stdin &#123;&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">output &#123;  </span><br><span class="line">  stdout &#123;&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">EOF  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">#检查配置⽂件语法  </span><br><span class="line">logstash -tf 01-stdin-to-stdout.conf  </span><br><span class="line">#启动logstash实例  </span><br><span class="line">logstash -f 01-stdin-to-stdout.conf  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_27.png"></p>
<h3 id="5-5-2-tcp输入，标准输出"><a href="#5-5-2-tcp输入，标准输出" class="headerlink" title="5.5.2 tcp输入，标准输出"></a>5.5.2 tcp输入，标准输出</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">input &#123;  </span><br><span class="line">  tcp &#123;  </span><br><span class="line">    port \=&gt; 8888  </span><br><span class="line">  &#125;  </span><br><span class="line">  tcp &#123;  </span><br><span class="line">    port \=&gt; 9999  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">output &#123;  </span><br><span class="line">  stdout &#123;&#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>测试输入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">telnet localhost 8888  </span><br><span class="line">\&gt; aaaa  </span><br><span class="line">\&gt; bbbb  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_28.png"></p>
<h3 id="5-5-3-http输入，标准输出"><a href="#5-5-3-http输入，标准输出" class="headerlink" title="5.5.3 http输入，标准输出"></a>5.5.3 http输入，标准输出</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">input &#123;  </span><br><span class="line">  http &#123;  </span><br><span class="line">    port \=&gt; 8888  </span><br><span class="line">  &#125;  </span><br><span class="line">  http &#123;  </span><br><span class="line">    port \=&gt; 9999  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">output &#123;  </span><br><span class="line">  stdout &#123;&#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>浏览器访问<a target="_blank" rel="noopener" href="http://cdh1:8888/">http://cdh1:8888</a></p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_29.png"></p>
<h3 id="5-5-4-gork插件"><a href="#5-5-4-gork插件" class="headerlink" title="5.5.4 gork插件"></a>5.5.4 gork插件</h3><p>Grok 是一个十分强大的 Logstash Filter 插件，它可以通过正则解析任意文本，将非结构化日志数据格式转换为结构化的、方便查询的结构。</p>
<p>它是目前 Logstash 中解析非结构化日志数据最好的方式。</p>
<p>Grok 的语法规则是： 这里的 “语法” 指的是匹配模式，例如，使用 NUMBER 模式可以匹配出数字，IP 模式则会匹配出 127.0.0.1 这样的 IP 地址。比如按以下格式输入内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">172.16.213.132 \[16/Jun/2020:16:24:19 +0800\] &quot;GET / HTTP/1.1&quot; 403 5039  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>那么， • %{IP:clientip} 匹配模式将获得的结果为：clientip: 172.16.213.132 • %{HTTPDATE:timestamp} 匹配模式将获得的结果为：timestamp: 16&#x2F;Jun&#x2F;2020:16:24:19 +0800 • %{QS:referrer} 匹配模式将获得的结果为：referrer: “GET &#x2F; HTTP&#x2F;1.1” 到这里为止，我们已经获取了上面输入中前三个部分的内容，分别是 clientip、timestamp 和 referrer 三个字段。</p>
<p>如果要获取剩余部分的信息，方法类似。</p>
<p><strong>要在线调试 Grok，可以点击****在线调试，可点击这里进行在线调试，非常方便。</strong></p>
<p>下面是一个组合匹配模式，它可以获取上面输入的所有内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">%&#123;IP:clientip&#125;\\ \\\[%&#123;HTTPDATE:timestamp&#125;\\\]\\ %&#123;QS:referrer&#125;\\ %&#123;NUMBER:response&#125;\\ %&#123;NUMBER:bytes&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>正则匹配是非常严格的匹配，在这个组合匹配模式中，使用了转义字符 \，这是因为输入的内容中有空格和中括号。</p>
<p>通过上面这个组合匹配模式，我们将输入的内容分成了 5 个部分，即 5 个字段。</p>
<p>将输入内容分割为不同的数据字段，这对于日后解析和查询日志数据非常有用，这正是我们使用 grok 的目的。</p>
<p>Logstash 默认提供了近 200 个匹配模式（其实就是定义好的正则表达式）让我们来使用，可以在 Logstash 安装目录下找到。</p>
<p>例如，我这里的路径为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">/usr/local/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-patterns-core\-4.1.2/patterns  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>此目录下有定义好的各种匹配模式，基本匹配定义在 grok-patterns 文件中。</p>
<p>从这些定义好的匹配模式中，可以查到上面使用的四个匹配模式对应的定义规则。</p>
<p>除此之外，还有很多默认定义好的匹配模式文件，比如 httpd、java、linux-syslog、redis、mongodb、nagios 等，这些已经定义好的匹配模式，可以直接在 Grok 过滤器中进行引用。</p>
<p>当然也可以定义自己需要的匹配模式。</p>
<p>在了解完 Grok 的匹配规则之后，下面通过一个配置实例深入介绍下 Logstash 是如何将非结构化日志数据转换成结构化数据的。</p>
<p>首先看下面的一个事件配置文件：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">input&#123;  </span><br><span class="line">  stdin&#123;&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">filter&#123;  </span><br><span class="line">   grok&#123;  </span><br><span class="line">     match =&gt; \[&quot;message&quot;, &quot;%&#123;IP:clientip&#125;\\ \\\[%&#123;HTTPDATE:timestamp&#125;\\\]\\ %&#123;QS:referrer&#125;\\ %         &#123;NUMBER:response&#125;\\ %&#123;NUMBER:bytes&#125;&quot;\]  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">output&#123;  </span><br><span class="line">   stdout&#123;  </span><br><span class="line">     codec =&gt; &quot;rubydebug&quot;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在这个配置文件中，输入配置成了 stdin，在 filter 中添加了 grok 过滤插件，并通过 match 来执行正则表达式解析，</p>
<p>grok 中括号中的正则表达式就是上面提到的组合匹配模式，然后通过 rubydebug 编码格式输出信息。</p>
<p>这样的组合有助于调试和分析输出结果。</p>
<p>通过此配置启动 Logstash 进程后，我们仍然输入之前给出的那段内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">172.16.213.132 \[16/Jun/2020:16:24:19 +0800\] &quot;GET / HTTP/1.1&quot; 403 5039  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>然后，查看 rubydebug 格式的日志输出，内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;  </span><br><span class="line">     &quot;timestamp&quot; =&gt; &quot;16/Jun/2020:16:24:19 +0800&quot;,  </span><br><span class="line">      &quot;response&quot; =&gt; &quot;403&quot;,  </span><br><span class="line">         &quot;bytes&quot; =&gt; &quot;5039&quot;,  </span><br><span class="line">      &quot;@version&quot; =&gt; &quot;1&quot;,  </span><br><span class="line">      &quot;clientip&quot; =&gt; &quot;172.16.213.132&quot;,  </span><br><span class="line">          &quot;host&quot; =&gt; &quot;nnmaster.cloud&quot;,  </span><br><span class="line">      &quot;referrer&quot; =&gt; &quot;\\&quot;GET / HTTP/1.1\\&quot;&quot;,  </span><br><span class="line">       &quot;message&quot; =&gt; &quot;172.16.213.132 \[16/Jun/2020:16:24:19 +0800\] \\&quot;GET / HTTP/1.1\\&quot; 403 5039&quot;,  </span><br><span class="line">    &quot;@timestamp&quot; =&gt; 2020-06-16T07:46:53.120Z  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>从这个输出可知，通过 Grok 定义好的 5 个字段都获取到了内容，并正常输出了。</p>
<p>案例配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">input &#123;  </span><br><span class="line">  stdin &#123;&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">filter &#123;  </span><br><span class="line">  grok &#123;  </span><br><span class="line">    match \=&gt; &#123;  </span><br><span class="line">      &quot;message&quot; \=&gt; &quot;%&#123;IP:clientIP&#125; %&#123;WORD:method&#125; %&#123;URIPATHPARAM:request&#125; %&#123;NUMBER:bytes&#125; %&#123;NUMBER:duration&#125;&quot;  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">output &#123;  </span><br><span class="line">  stdout &#123;&#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输入内容</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">55.3.244.1 GET /index.html 15824 0.043  </span><br><span class="line">10.0.0.103 POST /oldboyedu.html 888888 5.20  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>测试结果</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_30.png"></p>
<h3 id="5-5-5-filebeat输入，输出es-tomcat日志"><a href="#5-5-5-filebeat输入，输出es-tomcat日志" class="headerlink" title="5.5.5 filebeat输入，输出es(tomcat日志)"></a>5.5.5 filebeat输入，输出es(tomcat日志)</h3><p>filebeat配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filebeat.inputs:  </span><br><span class="line">\- type: log  </span><br><span class="line">  enabled: true  </span><br><span class="line">  paths:  </span><br><span class="line">    - /root/apache-tomcat-9.0.97/logs/\*.txt  </span><br><span class="line">  json.keys\_under\_root: true  </span><br><span class="line">  </span><br><span class="line">output.logstash:  </span><br><span class="line">  hosts: \[&quot;127.0.0.1:7777&quot;\]  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>logstash配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">input &#123;  </span><br><span class="line">  beats &#123;  </span><br><span class="line">    port =&gt; 7777  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">filter &#123;  </span><br><span class="line">    mutate &#123;  </span><br><span class="line">      # 移除指定的字段  </span><br><span class="line">      remove\_field =&gt; \[&quot;tags&quot;,&quot;log&quot;,&quot;agent&quot;,&quot;@version&quot;, &quot;input&quot;,&quot;ecs&quot;\]  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">output &#123;  </span><br><span class="line">    stdout &#123;&#125;  </span><br><span class="line">  </span><br><span class="line">    elasticsearch &#123;  </span><br><span class="line">      hosts =&gt; \[&quot;127.0.0.1:9200&quot;\]  </span><br><span class="line">      index =&gt; &quot;elk\_tomcat&quot;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="6-Kibana基础和实操"><a href="#6-Kibana基础和实操" class="headerlink" title="6 Kibana基础和实操"></a>6 Kibana基础和实操</h1><p><strong>ES实操参考第38章：ElasticSearch 学习圣经：从0到1, 精通 ElasticSearch 工业级实操，这里不再重复</strong></p>
<p>这里补充基于tomcat日志，Kibana可视化库和dashborad基本用法</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_31.png"></p>
<ol>
<li><strong>PV指标</strong></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">Page View(简称:&quot;PV&quot;) ：  </span><br><span class="line">	⻚⾯访问或点击量。  </span><br><span class="line">kibana界⾯⿏标依次点击如下:  </span><br><span class="line">	(1)菜单栏;  </span><br><span class="line">	(2)Visualize Library(可视化库);  </span><br><span class="line">	(3)新建可视化  </span><br><span class="line">	(4)基于聚合  </span><br><span class="line">	(5)指标  </span><br><span class="line">	(6)选择索引模式(例如&quot;elk\_\*&quot;)  </span><br><span class="line">	(7)指标栏中选择:  </span><br><span class="line">		聚合: 计数  </span><br><span class="line">		定制标签: PV  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol start="2">
<li><strong>基于clientip的PV指标</strong></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">客户端IP:  </span><br><span class="line">	通常指的是访问Web服务器的客户端IP地址，但要注意，客户端IP数量并不难代表UV。  </span><br><span class="line">kibana界⾯⿏标依次点击如下:  </span><br><span class="line">	(1)菜单栏;  </span><br><span class="line">	(2)Visualize Library(可视化库);  </span><br><span class="line">	(3)创建可视化  </span><br><span class="line">	(4)基于聚合  </span><br><span class="line">	(5)指标  </span><br><span class="line">	(6)选择索引模式(例如&quot;elk\_\*&quot;)  </span><br><span class="line">	(7)指标栏中选择:	聚合: 唯⼀计数  </span><br><span class="line">		字段: clientip.keyword  </span><br><span class="line">		定制标签: IP  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol start="3">
<li><strong>带宽</strong></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">带宽:  </span><br><span class="line">	统计nginx返回给客户端⽂件⼤⼩的字段进⾏累计求和。  </span><br><span class="line">kibana界⾯⿏标依次点击如下:  </span><br><span class="line">	(1)菜单栏;  </span><br><span class="line">	(2)Visualize Library(可视化库);  </span><br><span class="line">	(3)创建可视化  </span><br><span class="line">    (4)基于聚合  </span><br><span class="line">    (5)指标  </span><br><span class="line">    (6)选择索引模式(例如&quot;elk\_\*&quot;)  </span><br><span class="line">    (7)指标栏中选择:  </span><br><span class="line">		聚合: 求和  </span><br><span class="line">		字段: SendBytes  </span><br><span class="line">		定制标签: 带宽  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol start="4">
<li><strong>客户端城市分布</strong></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">分析客户端的城市分布:  </span><br><span class="line">	需要借助logstash的filter插件的geoip实现对客户端的IP地址进⾏地域解析。  </span><br><span class="line">kibana界⾯⿏标依次点击如下:  </span><br><span class="line">    (1)菜单栏;  </span><br><span class="line">    (2)Visualize Library(可视化库);  </span><br><span class="line">    (3)创建可视化  </span><br><span class="line">    (4)基于聚合  </span><br><span class="line">    (5)垂直条形图  </span><br><span class="line">    (6)选择索引模式(例如&quot;elk\_\*&quot;)  </span><br><span class="line">    (7)指标栏中设置(即Y轴)  </span><br><span class="line">        聚合: 计数  </span><br><span class="line">        定制标签: 城市分布  </span><br><span class="line">    (8)添加&quot;存储痛&quot;，选择&quot;X&quot;轴  </span><br><span class="line">        聚合: 词  </span><br><span class="line">        字段: city\_name.keyword  </span><br><span class="line">        ...  </span><br><span class="line">        定制标签: 城市名称  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol start="5">
<li><strong>IP的TopN统计:</strong></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">IP的TopN统计:  </span><br><span class="line">	统计访问量的客户端IP最⼤的是谁。  </span><br><span class="line">kibana界⾯⿏标依次点击如下:  </span><br><span class="line">    (1)菜单栏;  </span><br><span class="line">    (2)Visualize Library(可视化库);  </span><br><span class="line">    (3)创建可视化  </span><br><span class="line">    (4)基于聚合  </span><br><span class="line">    (5)仪表盘  </span><br><span class="line">    (6)选择索引模式(例如&quot;elk\_\*&quot;)  </span><br><span class="line">    (7)指标栏中设置(即Y轴)  </span><br><span class="line">        聚合: 计数  </span><br><span class="line">    (8)添加&quot;存储痛&quot;，选择&quot;X&quot;轴  </span><br><span class="line">        聚合: 词  </span><br><span class="line">        字段: client.keyword  </span><br><span class="line">        顺序: 降序  </span><br><span class="line">        ⼤⼩: 3  </span><br><span class="line">        ...  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="7-ELK综合实操"><a href="#7-ELK综合实操" class="headerlink" title="7 ELK综合实操"></a>7 ELK综合实操</h1><p>使用FELK架构部署监控app日志</p>
<h2 id="使用filebeat发送日志"><a href="#使用filebeat发送日志" class="headerlink" title="使用filebeat发送日志"></a>使用filebeat发送日志</h2><h3 id="制作filebeat镜像"><a href="#制作filebeat镜像" class="headerlink" title="制作filebeat镜像"></a>制作filebeat镜像</h3><p>官方文档</p>
<p><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-getting-started.html">https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-getting-started.html</a></p>
<p>下载filebeat，下载命令如下：</p>
<p><a target="_blank" rel="noopener" href="https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.14.0-linux-x86/_64.tar.gz">https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.14.0-linux-x86\_64.tar.gz</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"> wget  https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.2.0-linux-x86\_64.tar.gz  </span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">tar -zxvf filebeat-7.2.0-linux-x86\_64.tar.gz  </span><br><span class="line">mv filebeat-7.2.0-linux-x86\_64 /usr/share/  </span><br><span class="line">cd /usr/share/filebeat-7.2.0-linux-x86\_64/</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="制作基础的unbantu镜像"><a href="#制作基础的unbantu镜像" class="headerlink" title="制作基础的unbantu镜像"></a>制作基础的unbantu镜像</h3><p>why unbantu？ not alpine? not centos？</p>
<p>Alpine 只有仅仅 5 MB 大小，并且拥有很友好的包管理机制。</p>
<p>Docker 官方推荐使用 Alpine 替代 Ubuntu 做为容器的基础镜像。</p>
<p>曾经尝试使用alpine:3.7作为底层镜像, 按照zookeeper，但是一直启动不来，换成了centos的镜像，排查过程反复实验，耗时很久。</p>
<p>网上小伙伴构建filebeat镜像，基于alpine:3.7, 构建后的镜像运行时报“standard_init_linux.go:190: exec user process caused “no such file or directory””，故最后还是选择ubuntu。</p>
<p>这里选择ubuntu的原因,是其作为底层打包出来的镜像比centos要小很多。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">\# 基础镜像 生成的镜像作为基础镜像  </span><br><span class="line">FROM ubuntu:18.04  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">\# 指定维护者的信息  </span><br><span class="line">MAINTAINER 创客</span><br><span class="line">  </span><br><span class="line">\# RUN apt-get update  &amp;&amp; apt-get -y install openjdk-8-jdk  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">#install wget,sudo,python,vim,ping and ssh command  </span><br><span class="line">  </span><br><span class="line">RUN sed -i s@/archive.ubuntu.com/@/mirrors.aliyun.com/@g /etc/apt/sources.list   &amp;&amp; apt-get  clean &amp;&amp; \\  </span><br><span class="line">    apt-get update &amp;&amp; apt-get -y install wget &amp;&amp; apt-get -y install sudo &amp;&amp; \\  </span><br><span class="line">    apt-get -y install iputils-ping &amp;&amp; \\  </span><br><span class="line">    apt-get -y install net-tools &amp;&amp; \\  </span><br><span class="line">    apt install -y tzdata &amp;&amp; \\  </span><br><span class="line">    rm -rf /etc/localtime  &amp;&amp; ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp;  dpkg-reconfigure -f noninteractive tzdata &amp;&amp; \\  </span><br><span class="line">    apt-get  clean  </span><br><span class="line">  </span><br><span class="line"> #    echo &quot;Asia/Shanghai&quot; &gt; /etc/timezone &amp;&amp;  dpkg-reconfigure -f noninteractive tzdata &amp;&amp; \\  </span><br><span class="line">   </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">\# RUN dpkg-reconfigure -f noninteractive tzdata  </span><br><span class="line">      </span><br><span class="line">\# RUN apt-get clean  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"> #apt-get -y install python &amp;&amp; \\  </span><br><span class="line"> # apt-get -y install vim &amp;&amp; \\  </span><br><span class="line"> #  apt-get -y install openssh-server &amp;&amp; \\  </span><br><span class="line"> # apt-get -y install python-pip  &amp;&amp; \\  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">\# 复制并解压  </span><br><span class="line">ADD jdk-8u121-linux-x64.tar.gz /usr/local/  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">ENV work\_path /usr/local  </span><br><span class="line">WORKDIR $work\_path  </span><br><span class="line">  </span><br><span class="line">\# java  </span><br><span class="line">ENV JAVA\_HOME /usr/local/jdk1.8.0\_121  </span><br><span class="line">ENV JRE\_HOME  /usr/local/jdk1.8.0\_121/jre  </span><br><span class="line">ENV CLASSPATH .:$JAVA\_HOME/lib/dt.jar:$JAVA\_HOME/lib/tools.jar:$JRE\_HOME/lib  </span><br><span class="line">ENV PATH $&#123;PATH&#125;:$&#123;JAVA\_HOME&#125;/bin  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>dockfile add命令：</strong></p>
<p>ADD指令的功能是将主机构建环境（上下文）目录中的文件和目录、以及一个URL标记的文件 拷贝到镜像中。</p>
<p>其格式是： ADD 源路径 目标路径</p>
<p>注意事项：</p>
<p>1、如果源路径是个文件，且目标路径是以 &#x2F; 结尾， 则docker会把目标路径当作一个目录，会把源文件拷贝到该目录下。</p>
<p>如果目标路径不存在，则会自动创建目标路径。</p>
<p>2、如果源路径是个文件，且目标路径是不是以 &#x2F; 结尾，则docker会把目标路径当作一个文件。</p>
<p>如果目标路径不存在，会以目标路径为名创建一个文件，内容同源文件；</p>
<p>如果目标文件是个存在的文件，会用源文件覆盖它，当然只是内容覆盖，文件名还是目标文件名。</p>
<p>如果目标文件实际是个存在的目录，则会源文件拷贝到该目录下。 注意，这种情况下，最好显示的以 &#x2F; 结尾，以避免混淆。</p>
<p>3、如果源路径是个目录，且目标路径不存在，则docker会自动以目标路径创建一个目录，把源路径目录下的文件拷贝进来。</p>
<p>如果目标路径是个已经存在的目录，则docker会把源路径目录下的文件拷贝到该目录下。</p>
<p><strong>4、如果源文件是个归档文件（压缩文件，比如 .tar文件），则docker会自动帮解压。</strong></p>
<h3 id="推送镜像到dockerhub"><a href="#推送镜像到dockerhub" class="headerlink" title="推送镜像到dockerhub"></a>推送镜像到dockerhub</h3><p>这个镜像解决了jdk问题，时区问题</p>
<p>推送到了dockerhub，大家可以直接作为基础镜像使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">docker login  </span><br><span class="line">  </span><br><span class="line">docker tag 8d0abdffe76f nien/ubuntu:18.04  </span><br><span class="line">  </span><br><span class="line">docker push nien/ubuntu:18.04  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="制作filebeat镜像-1"><a href="#制作filebeat镜像-1" class="headerlink" title="制作filebeat镜像"></a>制作filebeat镜像</h3><p>官方文档</p>
<p><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-getting-started.html">https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-getting-started.html</a></p>
<p>下载filebeat，下载命令如下：</p>
<p><a target="_blank" rel="noopener" href="https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.14.0-linux-x86/_64.tar.gz">https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.14.0-linux-x86\_64.tar.gz</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"> wget  https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.2.0-linux-x86\_64.tar.gz  </span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">tar -zxvf filebeat-7.2.0-linux-x86\_64.tar.gz  </span><br><span class="line">mv filebeat-7.2.0-linux-x86\_64 /usr/share/  </span><br><span class="line">cd /usr/share/filebeat-7.2.0-linux-x86\_64/</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="dockerfile"><a href="#dockerfile" class="headerlink" title="dockerfile"></a>dockerfile</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">\# 基础镜像 生成的镜像作为基础镜像  </span><br><span class="line">FROM nien/ubuntu:18.04  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">\# 指定维护者的信息  </span><br><span class="line">MAINTAINER 创客</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">\# 复制并解压  </span><br><span class="line">ADD filebeat-7.14.0-linux-x86\_64.tar.gz /usr/local/  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>构建镜像</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">docker build -t filebeat:7.14.0  .  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>构建之后，进入容器，可以看到 &#x2F;usr&#x2F;local 目录下的filebeat-7.14.0-linux-x86_64</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">\[root@cdh2 filebeat\]# docker run -it filebeat:7.14.0 /bin/bash  </span><br><span class="line">root@7ba04f21f26e:/usr/local# ll  </span><br><span class="line">total 48  </span><br><span class="line">drwxr-xr-x 1 root root 4096 Apr  2 09:26 ./  </span><br><span class="line">drwxr-xr-x 1 root root 4096 Mar 16 03:27 ../  </span><br><span class="line">drwxr-xr-x 2 root root 4096 Mar 16 03:27 bin/  </span><br><span class="line">drwxr-xr-x 2 root root 4096 Mar 16 03:27 etc/  </span><br><span class="line">drwxr-xr-x 5 root root 4096 Apr  2 09:26 filebeat-7.14.0-linux-x86\_64/  </span><br><span class="line">drwxr-xr-x 2 root root 4096 Mar 16 03:27 games/  </span><br><span class="line">drwxr-xr-x 2 root root 4096 Mar 16 03:27 include/  </span><br><span class="line">drwxr-xr-x 8 uucp  143 4096 Dec 13  2016 jdk1.8.0\_121/  </span><br><span class="line">drwxr-xr-x 2 root root 4096 Mar 16 03:27 lib/  </span><br><span class="line">lrwxrwxrwx 1 root root    9 Mar 16 03:27 man -&gt; share/man/  </span><br><span class="line">drwxr-xr-x 2 root root 4096 Mar 16 03:27 sbin/  </span><br><span class="line">drwxr-xr-x 1 root root 4096 Apr  2 00:44 share/  </span><br><span class="line">drwxr-xr-x 2 root root 4096 Mar 16 03:27 src/  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="推送镜像到dockerhub-1"><a href="#推送镜像到dockerhub-1" class="headerlink" title="推送镜像到dockerhub"></a>推送镜像到dockerhub</h4><p>这个镜像解决了jdk问题，时区问题</p>
<p>推送到了dockerhub，大家可以直接作为基础镜像使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">\[root@cdh2 filebeat\]# docker tag fb44037ab5f9 nien/filebeat:7.14.0  </span><br><span class="line">  </span><br><span class="line">\[root@cdh2 filebeat\]# docker push nien/filebeat:7.14.0  </span><br><span class="line">The push refers to repository \[docker.io/nien/filebeat\]  </span><br><span class="line">069c957c7a4e: Pushing \[=======&gt;                                           \]  19.99MB/140MB  </span><br><span class="line">b17e3cbc28a1: Mounted from nien/ubuntu  </span><br><span class="line">5695cc8dd56c: Mounted from nien/ubuntu  </span><br><span class="line">9d6787a516e7: Mounted from nien/ubuntu  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如果要收集日志，就可以用这个基础镜像加点配置就ok啦</p>
<h2 id="example-application微服务的filebeat配置："><a href="#example-application微服务的filebeat配置：" class="headerlink" title="example-application微服务的filebeat配置："></a>example-application微服务的filebeat配置：</h2><blockquote>
<p>实操过程，请参见23章视频：《100Wqps 超高并发日志平台》实操</p>
</blockquote>
<h3 id="filebeat-yml的参考配置："><a href="#filebeat-yml的参考配置：" class="headerlink" title="filebeat.yml的参考配置："></a>filebeat.yml的参考配置：</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">\# ============================== Filebeat inputs ===============================  </span><br><span class="line">filebeat.config.inputs:  </span><br><span class="line">  enable: true  </span><br><span class="line">  path: /work/filebeat/input.yml  </span><br><span class="line">  reload.enabled: true  </span><br><span class="line">  reload.period: 2s  </span><br><span class="line">  </span><br><span class="line">\# ============================== Filebeat modules ==============================  </span><br><span class="line">  </span><br><span class="line">filebeat.config.modules:  </span><br><span class="line">  # Glob pattern for configuration loading  </span><br><span class="line">  path: $&#123;path.config&#125;/modules.d/\*.yml  </span><br><span class="line">  </span><br><span class="line">  # Set to true to enable config reloading  </span><br><span class="line">  reload.enabled: true  </span><br><span class="line">  </span><br><span class="line">  # Period on which files under path should be checked for changes  </span><br><span class="line">  #reload.period: 10s  </span><br><span class="line">  </span><br><span class="line">#----------------------------- Logstash output --------------------------------  </span><br><span class="line">output.logstash:  </span><br><span class="line">  # The Logstash hosts  </span><br><span class="line">  hosts: \[&quot;cdh1:15044&quot;\]  </span><br><span class="line">  </span><br><span class="line">  # Optional SSL. By default is off.  </span><br><span class="line">  # List of root certificates for HTTPS server verifications  </span><br><span class="line">  #ssl.certificate\_authorities: \[&quot;/etc/pki/root/ca.pem&quot;\]  </span><br><span class="line">  </span><br><span class="line">  # Certificate for SSL client authentication  </span><br><span class="line">  #ssl.certificate: &quot;/etc/pki/client/cert.pem&quot;  </span><br><span class="line">  </span><br><span class="line">  # Client Certificate Key  </span><br><span class="line">  #ssl.key: &quot;/etc/pki/client/cert.key&quot;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出到logstsh的地址为logstash，这里用的是容器的名称， logstash和 这个微服务，需要在同一个网络。</p>
<p>如果不是，可以使用虚拟机的名称，然后把 5044，映射到15044</p>
<h3 id="input-yml配置："><a href="#input-yml配置：" class="headerlink" title="input.yml配置："></a>input.yml配置：</h3><p>主要配置的是日志的搜集目录为&#x2F;work&#x2F;logs&#x2F;output.log，这个目录是应用message-dispatcher输出日志的文件。</p>
<p>由于其他的微服务也是固定在这个 文件，</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_32.png"></p>
<p>所以这个路径，基本可以固定。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">#filebeat.input:  </span><br><span class="line">\- type: log  </span><br><span class="line">  </span><br><span class="line">  # Change to true to enable this input configuration.  </span><br><span class="line">  enabled: true  </span><br><span class="line">  </span><br><span class="line">  # Paths that should be crawled and fetched. Glob based paths.  </span><br><span class="line">  paths:  </span><br><span class="line">    - /work/logs/info/\*.log  </span><br><span class="line">    - /work/logs/error/\*.log  </span><br><span class="line">  </span><br><span class="line">  #  </span><br><span class="line">  # - /work/logs/output.log  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  multiline:  </span><br><span class="line">    pattern: &#x27;^\\s\*(\\d&#123;4&#125;|\\d&#123;2&#125;)\\-(\\d&#123;2&#125;|\[a-zA-Z\]&#123;3&#125;)\\-(\\d&#123;2&#125;|\\d&#123;4&#125;)&#x27;   # 指定匹配的表达式（匹配以 2017-11-15 08:04:23:889 时间格式开头的字符串）  </span><br><span class="line">    negate: true                                # 是否匹配到  </span><br><span class="line">    match: after                                # 合并到上一行的末尾, 为了error日志  </span><br><span class="line">    max\_lines: 1000                             # 最大的行数  </span><br><span class="line">    timeout: 30s                                # 如果在规定的时候没有新的日志事件就不等待后面的日志  </span><br><span class="line">  </span><br><span class="line">  tags: \[&quot;example-application&quot;\]      #用于logstash过滤  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  #fields:   </span><br><span class="line">    #source: ExampleApplication  </span><br><span class="line">   #tags: \[&quot;GUID&quot;\]  </span><br><span class="line">    #- /var/log/\*.log  </span><br><span class="line">    #- c:\\programdata\\elasticsearch\\logs\\\*  </span><br><span class="line">  #include\_l ines: \[&#x27;^ERROR&#x27;\]    </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>启动filebeat，执行一下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">nohup  /user/local/filebeat-7.14.0-linux-x86\_64/filebeat  -c /work/filebeat/filebeat.yaml   &gt;&gt; /work/filebeat/out.log 2&gt;&amp;1  &amp;  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="修改dockerfile"><a href="#修改dockerfile" class="headerlink" title="修改dockerfile"></a>修改dockerfile</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">FROM  nien/filebeat:7.14.0  </span><br><span class="line">  </span><br><span class="line">\# 指定维护者的信息  </span><br><span class="line">MAINTAINER 创客</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">ADD dispatcher-provider-1.0-SNAPSHOT.jar  /app/message-dispatcher.jar  </span><br><span class="line">ADD deploy-sit.sh  /app/run.sh  </span><br><span class="line">RUN chmod +x /app/run.sh  </span><br><span class="line">  </span><br><span class="line">\# WORKDIR /app/  </span><br><span class="line">  </span><br><span class="line">ENTRYPOINT /bin/bash -c  &quot;/app/run.sh start&quot;  </span><br><span class="line">\# ENTRYPOINT /bin/bash </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="一键发布"><a href="#一键发布" class="headerlink" title="一键发布"></a>一键发布</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">使用shell脚本一键发布，这里的脚本，请参见视频  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>具体的演示，请参见视频</p>
<h3 id="启动之后"><a href="#启动之后" class="headerlink" title="启动之后"></a>启动之后</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">spatcher    | ----------------------------------------------------------  </span><br><span class="line">message-dispatcher    |         UAA 推送中台 push-provider is running! Access URLs:  </span><br><span class="line">message-dispatcher    |         Local:          http://127.0.0.1:7703/message-dispatcher-provider/  </span><br><span class="line">message-dispatcher    |         swagger-ui:     http://127.0.0.1:7703/message-dispatcher-provider/swagger-ui.html  </span><br><span class="line">message-dispatcher    |         actuator:       http://127.0.0.1:7703/message-dispatcher-provider/actuator/info  </span><br><span class="line">message-dispatcher    |         ----------------------------------------------------------  </span><br><span class="line">message-di  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="http://cdh2:7703/message-dispatcher-provider/swagger-ui.html">http://cdh2:7703/message-dispatcher-provider/swagger-ui.html</a></p>
<h3 id="message-dispatcher微服务的日志"><a href="#message-dispatcher微服务的日志" class="headerlink" title="message-dispatcher微服务的日志"></a>message-dispatcher微服务的日志</h3><p>在SpringBoot应用message-dispatcher微服务的日志，输出日志如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">\[root@cdh2 filebeat\]# cd  /home/docker-compose/sit-ware/message-dispatcher/work/logs/  </span><br><span class="line">\[root@cdh2 logs\]# cat output.log  </span><br><span class="line">2022-04-02 09:03:30.103 \[background-preinit\] DEBUG o.h.v.m.ResourceBundleMessageInterpolator:89 - Loaded expression factory via original TCCL  </span><br><span class="line">2022-04-02 09:03:59.633 \[main\] INFO  o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:330 - Bean &#x27;org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration&#x27; of type \[org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration$$EnhancerBySpringCGLIB$$e81692de\] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)  </span><br><span class="line">2022-04-02 09:04:05.331 \[main\] INFO  c.a.n.client.config.impl.LocalConfigInfoProcessor:195 - LOCAL\_SNAPSHOT\_PATH:/root/nacos/config  </span><br><span class="line">2022-04-02 09:04:06.034 \[main\] INFO  com.alibaba.nacos.client.config.impl.Limiter:53 - limitTime:5.0  </span><br><span class="line">2022-04-02 09:04:06.899 \[main\] INFO  com.alibaba.nacos.client.config.utils.JVMUtil:47 - isMultiInstance:false  </span><br><span class="line">2022-04-02 09:04:07.068 \[main\] WARN  c.a.cloud.nacos.client.NacosPropertySourceBuilder:87 - Ignore the empty nacos configuration and get it based on dataId\[message-dispatcher-provider\] &amp; group\[DEFAULT\_GROUP\]  </span><br><span class="line">2022-04-02 09:04:07.100 \[main\] WARN  c.a.cloud.nacos.client.NacosPropertySourceBuilder:87 - Ignore the empty nacos configuration and get it based on dataId\[message-dispatcher-provider.yml\] &amp; group\[DEFAULT\_GROUP\]  </span><br><span class="line">2022-04-02 09:04:07.191 \[main\] INFO  o.s.c.b.c.PropertySourceBootstrapConfiguration:101 - Located property source: CompositePropertySource &#123;name=&#x27;NACOS&#x27;, propertySources=\[NacosPropertySource &#123;name=&#x27;message-dispatcher-provider-sit.yml,DEFAULT\_GROUP&#x27;&#125;, NacosPropertySource &#123;name=&#x27;message-dispatcher-provider.yml,DEFAULT\_GROUP&#x27;&#125;, NacosPropertySource &#123;name=&#x27;message-dispatcher-provider,DEFAULT\_GROUP&#x27;&#125;, NacosPropertySource &#123;name=&#x27;sharding-db-dev.yml,DEFAULT\_GROUP&#x27;&#125;\]&#125;  </span><br><span class="line">2022-04-02 09:04:07.304 \[main\] INFO  c.c.s.message.start.MessageDispatchApplication:652 - The following profiles are active: sit  </span><br><span class="line">2022-04-02 09:04:28.417 \[main\] INFO  o.s.d.r.config.RepositoryConfigurationDelegate:247 - Multiple Spring Data modules found, entering strict repository configuration mode!  </span><br><span class="line">2022-04-02 09:04:28.418 \[main\] INFO  o.s.d.r.config.RepositoryConfigurationDelegate:127 - Bootstrapping Spring Data JPA repositories in DEFAULT mode.  </span><br><span class="line">2022-04-02 09:04:34.251 \[main\] INFO  o.s.d.r.config.RepositoryConfigurationDelegate:185 - Finished Spring Data repository scanning in 5673ms. Found 3 JPA repository interfaces.  </span><br><span class="line">2022-04-02 09:04:37.630 \[main\] WARN  o.springframework.boot.actuate.endpoint.EndpointId:131 - Endpoint ID &#x27;nacos-config&#x27; contains invalid characters, please migrate to a valid format.  </span><br><span class="line">2022-04-02 09:07:17.969 \[main\] ERROR org.springframework.boot.SpringApplication:823 - Application run failed  </span><br><span class="line">org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#x27;messageController&#x27;: Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#x27;messagePushServiceImpl&#x27;: Injection of resource dependencies failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name &#x27;rocketmqMessageService&#x27; defined in URL \[jar:file:/app/message-dispatcher.jar!/BOOT-INF/classes!/com/crazymaker/springcloud/message/service/impl/RocketmqMessageService.class\]: Initialization of bean failed; nested exception is java.lang.IllegalStateException: org.apache.rocketmq.remoting.exception.RemotingTimeoutException: wait response on the channel &lt;dh2/192.168.56.122:9876&gt; timeout, 3000(ms)  </span><br><span class="line">        at org.springframework.context.annotation.CommonAnnotationBeanPostProcessor.postProcessProperties(CommonAnnotationBeanPostProcessor.java:325)  </span><br><span class="line">        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1404)  </span><br><span class="line">        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:592)  </span><br><span class="line">        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:515)  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>然后在部署了filebeat的机器上部署该应用，应用的输出文件为&#x2F;var&#x2F;log&#x2F;service-hi.log，应用启动命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">1 nohup java -jar  elk-test-0.0.1-SNAPSHOT.jar &gt; /var/log/service-hi.log 2&gt;&amp;1  &amp;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>应用启动成功后日志输出如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">1 2019-07-02 17:13:13.530  INFO 31579 --- \[pool-1-thread-1\] com.example.elktest.ElkTestApplication   : seed:562779  </span><br><span class="line">2 2019-07-02 17:13:13.630  INFO 31579 --- \[pool-1-thread-1\] com.example.elktest.ElkTestApplication   : seed:963836  </span><br><span class="line">3 2019-07-02 17:13:13.730  INFO 31579 --- \[pool-1-thread-1\] com.example.elktest.ElkTestApplication   : seed:825694  </span><br><span class="line">4 2019-07-02 17:13:13.830  INFO 31579 --- \[pool-1-thread-1\] com.example.elktest.ElkTestApplication   : seed:33228  </span><br><span class="line">5 2019-07-02 17:13:13.930  INFO 31579 --- \[pool-1-thread-1\] com.example.elktest.ElkTestApplication   : seed:685589  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这时的日志数据的传输路径如下图：</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_33.png"></p>
<h3 id="查看日志索引"><a href="#查看日志索引" class="headerlink" title="查看日志索引"></a>查看日志索引</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">docker run -\-name filebeat \-d  \\  </span><br><span class="line">\-v /home/qw/elk/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml \\  </span><br><span class="line">\-v /home/qw/elk/testlog/:/home/ \\  </span><br><span class="line"> elastic/filebeat:7.2.0  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>效果</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_34.png"></p>
<p>可以看到 在kibana中多了两个索引 需要配置</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_35.png"><br>创建一个<br><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_36.png"><br>选择<br><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_37.png"><br>最终展示</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_38.png"><br>到这里简单收集日志就完成了,需要更多复杂业务配置,需要大家根据需求自己配置详细信息.</p>
<h2 id="logstash-详解"><a href="#logstash-详解" class="headerlink" title="logstash 详解"></a>logstash 详解</h2><p>Logstash 是一款强大的数据处理工具，它可以实现数据传输，格式处理，格式化输出，</p>
<p>logstash 还有强大的插件功能，常用于日志处理. logstash我们只让它进行日志处理，处理完之后将其输出到elasticsearch。</p>
<p>官方文档</p>
<p><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/logstash/7.17/index.html">https://www.elastic.co/guide/en/logstash/7.17/index.html</a></p>
<h3 id="stash第一个事件"><a href="#stash第一个事件" class="headerlink" title="stash第一个事件"></a>stash第一个事件</h3><p>Logstash管道有两个必需元素，输入和输出，以及一个可选元素filter。</p>
<p>输入插件使用来自源的数据，过滤器插件在您指定时修改数据，输出插件将数据写入目标。 如下图</p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_39.png"></p>
<p>根据官方文档Logstash对数据的处理主要流程是</p>
<ol>
<li><p>首先数据传入logstash，在其内部对数据进行过滤和处理</p>
</li>
<li><p>logstash将处理过的数据传递给Elasticsearch</p>
</li>
<li><p>Elasticsearch对数据进行存储、创建索引等内容</p>
</li>
<li><p>kibana对数据提供可视化的支持</p>
</li>
</ol>
<h4 id="Logstash的核心流程的三个环节"><a href="#Logstash的核心流程的三个环节" class="headerlink" title="Logstash的核心流程的三个环节"></a>Logstash的核心流程的三个环节</h4><blockquote>
<p>Logstash核心分三个环节：</p>
<ul>
<li><p>数据输入</p>
</li>
<li><p>数据处理</p>
</li>
<li><p>数据输出</p>
</li>
</ul>
</blockquote>
<p>其数据输入、处理、输出主要在配置中间中下面部分进行配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">input &#123;&#125;  </span><br><span class="line">filter &#123;&#125;  </span><br><span class="line">output &#123;&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="logstash数值类型"><a href="#logstash数值类型" class="headerlink" title="logstash数值类型"></a>logstash数值类型</h3><ul>
<li><strong>数组</strong></li>
</ul>
<p>match &#x3D;&gt;[“datetime”, “UNIX”, “ISO8601”]</p>
<ul>
<li><strong>布尔</strong></li>
</ul>
<p>必须是一个true或false</p>
<p>ssl_enable &#x3D;&gt; true</p>
<ul>
<li><strong>字节</strong></li>
</ul>
<p>一个字段是字节字符串字段表示有效字节的单元。它是一种方便的方式在特定尺寸的插件选项。</p>
<p>支持SI (k M G T P E Z Y)和Binary (TiKimigipiziyiei)单位。</p>
<p>二进制单元在基座单元和Si-1024在基底1000。</p>
<p>这个字段是大小写敏感的。如果未指定单位,则整数表示的字符串的字节数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">my\_bytes =&gt; &quot;1113&quot; # 1113 bytes   </span><br><span class="line">  </span><br><span class="line">my\_bytes =&gt; &quot;10MiB&quot; # 10485760 bytes  </span><br><span class="line">  </span><br><span class="line"> my\_bytes =&gt; &quot;100kib&quot; # 102400bytes   </span><br><span class="line">  </span><br><span class="line">my\_bytes =&gt; &quot;180 mb&quot;# 180000000 bytes  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>编解码器</strong></li>
</ul>
<p>codec &#x3D;&gt; “json”</p>
<ul>
<li><strong>哈希</strong></li>
</ul>
<p>哈希是一个键值对的集合中指定的格式，多个键值对的条目以空格分隔而不是逗号。</p>
<p>match &#x3D;&gt; { “field1” &#x3D;&gt; “value1” “field2” &#x3D;&gt;”value2” … }</p>
<ul>
<li><strong>数字</strong></li>
</ul>
<p>数字必须有效的数字值(浮点或整数)。</p>
<p>port &#x3D;&gt; 33</p>
<ul>
<li><strong>密码</strong></li>
</ul>
<p>密码是一个字符串的单个值，则不对其进行记录或打印。</p>
<p>my_password &#x3D;&gt; “password”</p>
<ul>
<li>uri</li>
</ul>
<p>my_uri &#x3D;&gt;”<a target="_blank" rel="noopener" href="http://foo:bar@example.net/">http://foo:bar@example.net</a>“</p>
<ul>
<li><strong>路径</strong></li>
</ul>
<p>一个路径是一个字符串，表示系统运行的有效路径。</p>
<p>my_path &#x3D;&gt;”&#x2F;tmp&#x2F;logstash”</p>
<ul>
<li><strong>转义序列</strong></li>
</ul>
<p>默认地，转义字符没有被启用。如果你希望使用转义字符串序列，您需要在你的logstash.yml中设置config.support_escapes: true</p>
<table>
<thead>
<tr>
<th><strong>Text</strong></th>
<th><strong>Result</strong></th>
</tr>
</thead>
<tbody><tr>
<td>\r</td>
<td>carriage return (ASCII 13)</td>
</tr>
<tr>
<td>\n</td>
<td>new line (ASCII 10)</td>
</tr>
<tr>
<td>\t</td>
<td>tab (ASCII 9)</td>
</tr>
<tr>
<td>\</td>
<td>backslash (ASCII 92)</td>
</tr>
<tr>
<td>“</td>
<td>double quote (ASCII 34)</td>
</tr>
<tr>
<td>‘</td>
<td>single quote (ASCII 39)</td>
</tr>
</tbody></table>
<h3 id="logstash-条件判断"><a href="#logstash-条件判断" class="headerlink" title="logstash 条件判断"></a>logstash 条件判断</h3><p>有时您只想在特定条件下过滤或输出事件。为此，您可以使用条件。</p>
<p>Logstash中的条件查看和行为与编程语言中的条件相同。条件语句支持if，else if以及else报表和可以被嵌套。</p>
<p><strong>条件语法</strong></p>
<p>if EXPRESSION{ … } else if EXPRESSION { … } else { … }</p>
<h3 id="logstash-比较运算符"><a href="#logstash-比较运算符" class="headerlink" title="logstash 比较运算符"></a>logstash 比较运算符</h3><p>等于: &#x3D;&#x3D;, !&#x3D;, &lt;, &gt;, &lt;&#x3D;, &gt;&#x3D; 　　正则: &#x3D;, ! (checks a pattern on the right against a string value on the left) 　　包含关系: in, not in</p>
<p>支持的布尔运算符：and, or, nand, xor</p>
<p>支持的一元运算符: !</p>
<table>
<thead>
<tr>
<th><strong>作用</strong></th>
<th><strong>符号</strong></th>
</tr>
</thead>
<tbody><tr>
<td>等于</td>
<td>=&#x3D;</td>
</tr>
<tr>
<td>不等于</td>
<td>!&#x3D;</td>
</tr>
<tr>
<td>小于</td>
<td>&lt;</td>
</tr>
<tr>
<td>大于</td>
<td>&gt;</td>
</tr>
<tr>
<td>小于等于</td>
<td>&lt;&#x3D;</td>
</tr>
<tr>
<td>大于等于</td>
<td>&gt;&#x3D;</td>
</tr>
<tr>
<td>匹配正则</td>
<td>=~</td>
</tr>
<tr>
<td>不匹配正则</td>
<td>!~</td>
</tr>
<tr>
<td>包含</td>
<td>in</td>
</tr>
<tr>
<td>不包含</td>
<td>not in</td>
</tr>
<tr>
<td>与</td>
<td>and</td>
</tr>
<tr>
<td>或</td>
<td>or</td>
</tr>
<tr>
<td>非与</td>
<td>nand</td>
</tr>
<tr>
<td>非或</td>
<td>xor</td>
</tr>
<tr>
<td>复合表达式</td>
<td>()</td>
</tr>
<tr>
<td>取反符合</td>
<td>!()</td>
</tr>
</tbody></table>
<h2 id="数据输入环节"><a href="#数据输入环节" class="headerlink" title="数据输入环节"></a>数据输入环节</h2><blockquote>
<p>input配置定义了数据的来源。其主要支持下面方式</p>
</blockquote>
<p>事件源可以是从stdin屏幕输入读取，可以从file指定的文件，也可以从es，filebeat，kafka，redis等读取</p>
<h3 id="stdin"><a href="#stdin" class="headerlink" title="stdin"></a>stdin</h3><p>监控控制台输入。</p>
<p>要测试Logstash安装成功，运行最基本的Logstash管道。 执行以下的命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">bin/logstash -e &#x27;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;&#x27;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>-e 标志使您可以直接从命令行指定配置。</p>
<p>通过在命令行指定配置，可以快速测试配置，而无需在迭代之间编辑文件。</p>
<p>示例中的管道从标准输入stdin获取输入，并以结构化格式将输入移动到标准输出stdout。</p>
<p>启动Logstash后，等到看到“Pipeline main started”，然后在命令提示符下输入hello world，显示的如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">hello world  </span><br><span class="line">&#123;  </span><br><span class="line">     &quot;host&quot; =&gt; &quot;VM\_0\_13\_centos&quot;,  </span><br><span class="line">     &quot;message&quot; =&gt; &quot;hello world&quot;,  </span><br><span class="line">     &quot;@version&quot; =&gt; &quot;1&quot;,  </span><br><span class="line">    &quot;@timestamp&quot; =&gt; 2019-07-02T06:26:28.684Z  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="file"><a href="#file" class="headerlink" title="file"></a>file</h3><p>监控文件内容</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">file&#123;  </span><br><span class="line">    path =&gt; \[&#x27;/var/log/nginx/access.log&#x27;\]  #要输入的文件路径  </span><br><span class="line">    type =&gt; &#x27;nginx\_access\_log&#x27;  </span><br><span class="line">    start\_position =&gt; &quot;beginning&quot;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li><p>path 可以用&#x2F;var&#x2F;log&#x2F;<em>.log,&#x2F;var&#x2F;log&#x2F;**&#x2F;</em>.log，</p>
</li>
<li><p>type 通用选项. 用于激活过滤器</p>
</li>
<li><p>start_position 选择logstash开始读取文件的位置，begining或者end。</p>
</li>
</ul>
<p>还有一些常用的例如：discover_interval，exclude，sincedb_path,sincedb_write_interval等可以参考官网</p>
<h3 id="syslogs"><a href="#syslogs" class="headerlink" title="syslogs"></a>syslogs</h3><p>从syslogs读取数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">syslog&#123;  </span><br><span class="line">    port =&gt;&quot;514&quot;   </span><br><span class="line">    type =&gt; &quot;syslog&quot;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">\# port 指定监听端口(同时建立TCP/UDP的514端口的监听)  </span><br><span class="line">  </span><br><span class="line">#从syslogs读取需要实现配置rsyslog：  </span><br><span class="line">\# cat /etc/rsyslog.conf   加入一行  </span><br><span class="line">\*.\* @172.17.128.200:514　  #指定日志输入到这个端口，然后logstash监听这个端口，如果有新日志输入则读取  </span><br><span class="line">\# service rsyslog restart   #重启日志服务  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="beats"><a href="#beats" class="headerlink" title="beats"></a>beats</h3><p>从Elastic beats接收数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">beats &#123;  </span><br><span class="line">    port =&gt; 5044   #要监听的端口  </span><br><span class="line">&#125;  </span><br><span class="line">\# 还有host等选项  </span><br><span class="line">  </span><br><span class="line">\# 从beat读取需要先配置beat端，从beat输出到logstash。  </span><br><span class="line">\# vim /etc/filebeat/filebeat.yml   </span><br><span class="line">..........  </span><br><span class="line">output.logstash:  </span><br><span class="line">hosts: \[&quot;localhost:5044&quot;\]  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h3><p>从kafka topic中读取数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">kafka&#123;  </span><br><span class="line">    bootstrap\_servers=&gt; &quot;kafka01:9092,kafka02:9092,kafka03:9092&quot;  </span><br><span class="line">    topics =&gt; \[&quot;access\_log&quot;\]  </span><br><span class="line">    group\_id =&gt; &quot;logstash-file&quot;  </span><br><span class="line">    codec =&gt; &quot;json&quot;  </span><br><span class="line">&#125;  </span><br><span class="line">kafka&#123;  </span><br><span class="line">    bootstrap\_servers=&gt; &quot;kafka01:9092,kafka02:9092,kafka03:9092&quot;  </span><br><span class="line">    topics =&gt; \[&quot;weixin\_log&quot;,&quot;user\_log&quot;\]    </span><br><span class="line">    codec =&gt; &quot;json&quot;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">\# bootstrap\_servers 用于建立群集初始连接的Kafka实例的URL列表。  </span><br><span class="line">\# topics  要订阅的主题列表，kafka topics  </span><br><span class="line">\# group\_id 消费者所属组的标识符，默认为logstash。kafka中一个主题的消息将通过相同的方式分发到Logstash的group\_id  </span><br><span class="line">\# codec 通用选项，用于输入数据的编解码器。  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="数据处理环节"><a href="#数据处理环节" class="headerlink" title="数据处理环节"></a>数据处理环节</h2><p>filter plugin 过滤器插件,主要是对数据进行处理。</p>
<h3 id="grok解析文本并构造"><a href="#grok解析文本并构造" class="headerlink" title="grok解析文本并构造"></a>grok解析文本并构造</h3><p>Grok 是一个十分强大的 Logstash Filter 插件，它可以通过正则解析任意文本，将非结构化日志数据格式转换为结构化的、方便查询的结构。</p>
<p>它是目前 Logstash 中解析非结构化日志数据最好的方式。</p>
<p>Grok 的语法规则是： 这里的 “语法” 指的是匹配模式，例如，使用 NUMBER 模式可以匹配出数字，IP 模式则会匹配出 127.0.0.1 这样的 IP 地址。比如按以下格式输入内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">172.16.213.132 \[16/Jun/2020:16:24:19 +0800\] &quot;GET / HTTP/1.1&quot; 403 5039  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>那么， • %{IP:clientip} 匹配模式将获得的结果为：clientip: 172.16.213.132 • %{HTTPDATE:timestamp} 匹配模式将获得的结果为：timestamp: 16&#x2F;Jun&#x2F;2020:16:24:19 +0800 • %{QS:referrer} 匹配模式将获得的结果为：referrer: “GET &#x2F; HTTP&#x2F;1.1” 到这里为止，我们已经获取了上面输入中前三个部分的内容，分别是 clientip、timestamp 和 referrer 三个字段。</p>
<p>如果要获取剩余部分的信息，方法类似。</p>
<p><strong>要在线调试 Grok，可以点击****在线调试，可点击这里进行在线调试，非常方便。</strong></p>
<p>下面是一个组合匹配模式，它可以获取上面输入的所有内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">%&#123;IP:clientip&#125;\\ \\\[%&#123;HTTPDATE:timestamp&#125;\\\]\\ %&#123;QS:referrer&#125;\\ %&#123;NUMBER:response&#125;\\ %&#123;NUMBER:bytes&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>正则匹配是非常严格的匹配，在这个组合匹配模式中，使用了转义字符 \，这是因为输入的内容中有空格和中括号。</p>
<p>通过上面这个组合匹配模式，我们将输入的内容分成了 5 个部分，即 5 个字段。</p>
<p>将输入内容分割为不同的数据字段，这对于日后解析和查询日志数据非常有用，这正是我们使用 grok 的目的。</p>
<p>Logstash 默认提供了近 200 个匹配模式（其实就是定义好的正则表达式）让我们来使用，可以在 Logstash 安装目录下找到。</p>
<p>例如，我这里的路径为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">/usr/local/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-patterns-core\-4.1.2/patterns  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>此目录下有定义好的各种匹配模式，基本匹配定义在 grok-patterns 文件中。</p>
<p>从这些定义好的匹配模式中，可以查到上面使用的四个匹配模式对应的定义规则。</p>
<p>除此之外，还有很多默认定义好的匹配模式文件，比如 httpd、java、linux-syslog、redis、mongodb、nagios 等，这些已经定义好的匹配模式，可以直接在 Grok 过滤器中进行引用。</p>
<p>当然也可以定义自己需要的匹配模式。</p>
<p>在了解完 Grok 的匹配规则之后，下面通过一个配置实例深入介绍下 Logstash 是如何将非结构化日志数据转换成结构化数据的。</p>
<p>首先看下面的一个事件配置文件：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">input&#123;  </span><br><span class="line">  stdin&#123;&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">filter&#123;  </span><br><span class="line">   grok&#123;  </span><br><span class="line">     match =&gt; \[&quot;message&quot;, &quot;%&#123;IP:clientip&#125;\\ \\\[%&#123;HTTPDATE:timestamp&#125;\\\]\\ %&#123;QS:referrer&#125;\\ %         &#123;NUMBER:response&#125;\\ %&#123;NUMBER:bytes&#125;&quot;\]  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">output&#123;  </span><br><span class="line">   stdout&#123;  </span><br><span class="line">     codec =&gt; &quot;rubydebug&quot;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在这个配置文件中，输入配置成了 stdin，在 filter 中添加了 grok 过滤插件，并通过 match 来执行正则表达式解析，</p>
<p>grok 中括号中的正则表达式就是上面提到的组合匹配模式，然后通过 rubydebug 编码格式输出信息。</p>
<p>这样的组合有助于调试和分析输出结果。</p>
<p>通过此配置启动 Logstash 进程后，我们仍然输入之前给出的那段内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">172.16.213.132 \[16/Jun/2020:16:24:19 +0800\] &quot;GET / HTTP/1.1&quot; 403 5039  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>然后，查看 rubydebug 格式的日志输出，内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;  </span><br><span class="line">     &quot;timestamp&quot; =&gt; &quot;16/Jun/2020:16:24:19 +0800&quot;,  </span><br><span class="line">      &quot;response&quot; =&gt; &quot;403&quot;,  </span><br><span class="line">         &quot;bytes&quot; =&gt; &quot;5039&quot;,  </span><br><span class="line">      &quot;@version&quot; =&gt; &quot;1&quot;,  </span><br><span class="line">      &quot;clientip&quot; =&gt; &quot;172.16.213.132&quot;,  </span><br><span class="line">          &quot;host&quot; =&gt; &quot;nnmaster.cloud&quot;,  </span><br><span class="line">      &quot;referrer&quot; =&gt; &quot;\\&quot;GET / HTTP/1.1\\&quot;&quot;,  </span><br><span class="line">       &quot;message&quot; =&gt; &quot;172.16.213.132 \[16/Jun/2020:16:24:19 +0800\] \\&quot;GET / HTTP/1.1\\&quot; 403 5039&quot;,  </span><br><span class="line">    &quot;@timestamp&quot; =&gt; 2020-06-16T07:46:53.120Z  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>从这个输出可知，通过 Grok 定义好的 5 个字段都获取到了内容，并正常输出了。</p>
<h3 id="date日期解析"><a href="#date日期解析" class="headerlink" title="date日期解析"></a>date日期解析</h3><p>解析字段中的日期，然后转存到@timestamp</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">\[2018-07-04 17:43:35,503\]  </span><br><span class="line">  </span><br><span class="line">grok&#123;  </span><br><span class="line">      match =&gt; &#123;&quot;message&quot;=&gt;&quot;%&#123;DATA:raw\_datetime&#125;&quot;&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">date&#123;  </span><br><span class="line">      match =&gt; \[&quot;raw\_datetime&quot;,&quot;YYYY-MM-dd HH:mm:ss,SSS&quot;\]  </span><br><span class="line">      remove\_field =&gt;\[&quot;raw\_datetime&quot;\]  </span><br><span class="line">&#125;  </span><br><span class="line">#将raw\_datetime存到@timestamp 然后删除raw\_datetime  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">#24/Jul/2018:18:15:05 +0800  </span><br><span class="line">date &#123;  </span><br><span class="line">      match =&gt; \[&quot;timestamp&quot;,&quot;dd/MMM/YYYY:HH:mm:ss Z\]  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="mutate字段转换"><a href="#mutate字段转换" class="headerlink" title="mutate字段转换"></a>mutate字段转换</h3><p>mutate字段转换, 对字段做处理 重命名、删除、替换和修改字段。</p>
<p>Mutate过滤器的配置选项</p>
<table>
<thead>
<tr>
<th><strong>选项</strong></th>
<th><strong>类型</strong></th>
<th><strong>是否必须</strong></th>
<th><strong>简述</strong></th>
</tr>
</thead>
<tbody><tr>
<td>convert</td>
<td>hash</td>
<td>No</td>
<td>转化命令，是对字段类型做转化，例如:<code>String</code>转为<code>integer</code></td>
</tr>
<tr>
<td>copy</td>
<td>hash</td>
<td>No</td>
<td>将一个已经存在的字段复制给另一个字段。</td>
</tr>
<tr>
<td>gsub</td>
<td>array</td>
<td>No</td>
<td>通过正则表达式匹配字段的值，然后替换为指定的字符串。</td>
</tr>
<tr>
<td>join</td>
<td>hash</td>
<td>No</td>
<td>使用分隔符连接数组。</td>
</tr>
<tr>
<td>lowercase</td>
<td>array</td>
<td>No</td>
<td>将string类型的字段值转化为小写的形式。</td>
</tr>
<tr>
<td>merge</td>
<td>hash</td>
<td>No</td>
<td>合并两个数组或者Hash类型的字段。string类型的字段会自动的合并为一个数组。</td>
</tr>
<tr>
<td>coerce</td>
<td>hash</td>
<td>No</td>
<td>为存在但是不为空的字段设置默认值</td>
</tr>
<tr>
<td>rename</td>
<td>hash</td>
<td>No</td>
<td>字段重命名</td>
</tr>
<tr>
<td>replace</td>
<td>hash</td>
<td>No</td>
<td>将一个字段的值替换为一个新的值。</td>
</tr>
<tr>
<td>split</td>
<td>hash</td>
<td>No</td>
<td>将一个字段按照指定符号切割为数组。</td>
</tr>
<tr>
<td>strip</td>
<td>array</td>
<td>No</td>
<td>去除字段中的空格。</td>
</tr>
<tr>
<td>update</td>
<td>hash</td>
<td>No</td>
<td>更新字段为一个新值。</td>
</tr>
<tr>
<td>uppercase</td>
<td>array</td>
<td>No</td>
<td>将字符串字段转化为大写形式。</td>
</tr>
<tr>
<td>capitalize</td>
<td>array</td>
<td>No</td>
<td>将字符串字段转化为首字母大写的形式。</td>
</tr>
<tr>
<td>tag_on_failure</td>
<td>string</td>
<td>No</td>
<td>错误发生时的配置</td>
</tr>
</tbody></table>
<h2 id="covert类型转换"><a href="#covert类型转换" class="headerlink" title="covert类型转换"></a>covert类型转换</h2><p><strong>covert</strong>：类型转换。类型包括：integer，float，integer_eu，float_eu，string和boolean</p>
<ul>
<li><p>字段类型为 hash</p>
</li>
<li><p>没有默认值</p>
</li>
</ul>
<p>将字段转化为不同的类型，例如：string 转 integer。</p>
<p>如果被转化的字段类型是数组，数组的所有成员都将被转化。如果对象是hash 就不会进行转化。</p>
<p><strong>实例：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filter &#123;  </span><br><span class="line">  mutate &#123;  </span><br><span class="line">    convert =&gt; &#123;  </span><br><span class="line">      &quot;fieldname&quot; =&gt; &quot;integer&quot;  </span><br><span class="line">      &quot;booleanfield&quot; =&gt; &quot;boolean&quot;  </span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="split"><a href="#split" class="headerlink" title="split"></a>split</h3><p><strong>split</strong>：使用分隔符把字符串分割成数组</p>
<p>eg：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">mutate&#123;  </span><br><span class="line">    split =&gt; &#123;&quot;message&quot;=&gt;&quot;,&quot;&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>aaa,bbb</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">&#123;  </span><br><span class="line">    &quot;@timestamp&quot; =&gt; 2018-06-26T02:40:19.678Z,  </span><br><span class="line">      &quot;@version&quot; =&gt; &quot;1&quot;,  </span><br><span class="line">          &quot;host&quot; =&gt; &quot;localhost&quot;,  </span><br><span class="line">       &quot;message&quot; =&gt; \[  </span><br><span class="line">        \[0\] &quot;aaa&quot;,  </span><br><span class="line">        \[1\] &quot;bbb&quot;  </span><br><span class="line">    \]&#125;  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>192,128,1,100</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;  </span><br><span class="line">        &quot;host&quot; =&gt; &quot;localhost&quot;,  </span><br><span class="line">     &quot;message&quot; =&gt; \[  </span><br><span class="line">      \[0\] &quot;192&quot;,  </span><br><span class="line">      \[1\] &quot;128&quot;,  </span><br><span class="line">      \[2\] &quot;1&quot;,  </span><br><span class="line">      \[3\] &quot;100&quot;  </span><br><span class="line"> \],  </span><br><span class="line">  &quot;@timestamp&quot; =&gt; 2018-06-26T02:45:17.877Z,  </span><br><span class="line">    &quot;@version&quot; =&gt; &quot;1&quot;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">mutate&#123;  </span><br><span class="line">    split =&gt; &#123;&quot;message&quot;=&gt;&quot;,&quot;&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="merge"><a href="#merge" class="headerlink" title="merge"></a>merge</h3><p><strong>merge</strong>：合并字段 。数组和字符串 ，字符串和字符串</p>
<p>eg：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filter&#123;  </span><br><span class="line">    mutate&#123;  </span><br><span class="line">        add\_field =&gt; &#123;&quot;field1&quot;=&gt;&quot;value1&quot;&#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    mutate&#123;   </span><br><span class="line">          split =&gt; &#123;&quot;message&quot;=&gt;&quot;.&quot;&#125;   #把message字段按照.分割  </span><br><span class="line">    &#125;  </span><br><span class="line">    mutate&#123;  </span><br><span class="line">        merge =&gt; &#123;&quot;message&quot;=&gt;&quot;field1&quot;&#125;   #将filed1字段加入到message字段  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输入：abc</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">&#123;  </span><br><span class="line">       &quot;message&quot; =&gt; \[  </span><br><span class="line">        \[0\] &quot;abc,&quot;  </span><br><span class="line">        \[1\] &quot;value1&quot;  </span><br><span class="line">    \],  </span><br><span class="line">    &quot;@timestamp&quot; =&gt; 2018-06-26T03:38:57.114Z,  </span><br><span class="line">        &quot;field1&quot; =&gt; &quot;value1&quot;,  </span><br><span class="line">      &quot;@version&quot; =&gt; &quot;1&quot;,  </span><br><span class="line">          &quot;host&quot; =&gt; &quot;localhost&quot;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输入：abc,.123</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">&#123;  </span><br><span class="line">       &quot;message&quot; =&gt; \[  </span><br><span class="line">        \[0\] &quot;abc,&quot;,  </span><br><span class="line">        \[1\] &quot;123&quot;,  </span><br><span class="line">        \[2\] &quot;value1&quot;  </span><br><span class="line">    \],  </span><br><span class="line">    &quot;@timestamp&quot; =&gt; 2018-06-26T03:38:57.114Z,  </span><br><span class="line">        &quot;field1&quot; =&gt; &quot;value1&quot;,  </span><br><span class="line">      &quot;@version&quot; =&gt; &quot;1&quot;,  </span><br><span class="line">          &quot;host&quot; =&gt; &quot;localhost&quot;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="rename"><a href="#rename" class="headerlink" title="rename"></a>rename</h3><p><strong>rename</strong>：对字段重命名</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filter&#123;  </span><br><span class="line">    mutate&#123;  </span><br><span class="line">        rename =&gt; &#123;&quot;message&quot;=&gt;&quot;info&quot;&#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>123</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;  </span><br><span class="line">    &quot;@timestamp&quot; =&gt; 2018-06-26T02:56:00.189Z,  </span><br><span class="line">          &quot;info&quot; =&gt; &quot;123&quot;,  </span><br><span class="line">      &quot;@version&quot; =&gt; &quot;1&quot;,  </span><br><span class="line">          &quot;host&quot; =&gt; &quot;localhost&quot;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="remove-field：移除字段"><a href="#remove-field：移除字段" class="headerlink" title="remove_field：移除字段"></a>remove_field：移除字段</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">mutate &#123;  </span><br><span class="line">    remove\_field =&gt; \[&quot;message&quot;,&quot;datetime&quot;\]  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><p><strong>join</strong>：用分隔符连接数组，如果不是数组则不做处理</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">mutate&#123;  </span><br><span class="line">        split =&gt; &#123;&quot;message&quot;=&gt;&quot;:&quot;&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">mutate&#123;  </span><br><span class="line">        join =&gt; &#123;&quot;message&quot;=&gt;&quot;,&quot;&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">abc:123  </span><br><span class="line">&#123;  </span><br><span class="line">    &quot;@timestamp&quot; =&gt; 2018-06-26T03:55:41.426Z,  </span><br><span class="line">       &quot;message&quot; =&gt; &quot;abc,123&quot;,  </span><br><span class="line">          &quot;host&quot; =&gt; &quot;localhost&quot;,  </span><br><span class="line">      &quot;@version&quot; =&gt; &quot;1&quot;  </span><br><span class="line">&#125;  </span><br><span class="line">aa:cc  </span><br><span class="line">&#123;  </span><br><span class="line">    &quot;@timestamp&quot; =&gt; 2018-06-26T03:55:47.501Z,  </span><br><span class="line">       &quot;message&quot; =&gt; &quot;aa,cc&quot;,  </span><br><span class="line">          &quot;host&quot; =&gt; &quot;localhost&quot;,  </span><br><span class="line">      &quot;@version&quot; =&gt; &quot;1&quot;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>gsub</strong>：用正则或者字符串替换字段值。仅对字符串有效</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">mutate&#123;  </span><br><span class="line">        gsub =&gt; \[&quot;message&quot;,&quot;/&quot;,&quot;\_&quot;\]   #用\_替换/  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">\------&gt;  </span><br><span class="line">a/b/c/  </span><br><span class="line">&#123;  </span><br><span class="line">      &quot;@version&quot; =&gt; &quot;1&quot;,  </span><br><span class="line">       &quot;message&quot; =&gt; &quot;a\_b\_c\_&quot;,  </span><br><span class="line">          &quot;host&quot; =&gt; &quot;localhost&quot;,  </span><br><span class="line">    &quot;@timestamp&quot; =&gt; 2018-06-26T06:20:10.811Z  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>update</strong>：更新字段。如果字段不存在，则不做处理</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">mutate&#123;  </span><br><span class="line">        add\_field =&gt; &#123;&quot;field1&quot;=&gt;&quot;value1&quot;&#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    mutate&#123;  </span><br><span class="line">        update =&gt; &#123;&quot;field1&quot;=&gt;&quot;v1&quot;&#125;  </span><br><span class="line">        update =&gt; &#123;&quot;field2&quot;=&gt;&quot;v2&quot;&#125;    #field2不存在 不做处理  </span><br><span class="line">    &#125;  </span><br><span class="line">\----------------&gt;  </span><br><span class="line">&#123;  </span><br><span class="line">    &quot;@timestamp&quot; =&gt; 2018-06-26T06:26:28.870Z,  </span><br><span class="line">        &quot;field1&quot; =&gt; &quot;v1&quot;,  </span><br><span class="line">          &quot;host&quot; =&gt; &quot;localhost&quot;,  </span><br><span class="line">      &quot;@version&quot; =&gt; &quot;1&quot;,  </span><br><span class="line">       &quot;message&quot; =&gt; &quot;a&quot;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>replace</strong>：更新字段。如果字段不存在，则创建</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">mutate&#123;  </span><br><span class="line">        add\_field =&gt; &#123;&quot;field1&quot;=&gt;&quot;value1&quot;&#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    mutate&#123;  </span><br><span class="line">        replace =&gt; &#123;&quot;field1&quot;=&gt;&quot;v1&quot;&#125;  </span><br><span class="line">        replace =&gt; &#123;&quot;field2&quot;=&gt;&quot;v2&quot;&#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">\----------------------&gt;  </span><br><span class="line">&#123;  </span><br><span class="line">       &quot;message&quot; =&gt; &quot;1&quot;,  </span><br><span class="line">          &quot;host&quot; =&gt; &quot;localhost&quot;,  </span><br><span class="line">    &quot;@timestamp&quot; =&gt; 2018-06-26T06:28:09.915Z,  </span><br><span class="line">        &quot;field2&quot; =&gt; &quot;v2&quot;,        #field2不存在，则新建  </span><br><span class="line">      &quot;@version&quot; =&gt; &quot;1&quot;,  </span><br><span class="line">        &quot;field1&quot; =&gt; &quot;v1&quot;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="geoip"><a href="#geoip" class="headerlink" title="geoip"></a>geoip</h3><p>根据来自Maxmind GeoLite2数据库的数据添加有关IP地址的地理位置的信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">geoip &#123;  </span><br><span class="line">           source =&gt; &quot;clientip&quot;  </span><br><span class="line">           database =&gt;&quot;/tmp/GeoLiteCity.dat&quot;  </span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="ruby"><a href="#ruby" class="headerlink" title="ruby"></a>ruby</h3><p>ruby插件可以执行任意Ruby代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">filter&#123;  </span><br><span class="line">    urldecode&#123;  </span><br><span class="line">        field =&gt; &quot;message&quot;  </span><br><span class="line">    &#125;  </span><br><span class="line">    ruby &#123;  </span><br><span class="line">        init =&gt; &quot;@kname = \[&#x27;url\_path&#x27;,&#x27;url\_arg&#x27;\]&quot;  </span><br><span class="line">        code =&gt; &quot;   </span><br><span class="line">            new\_event = LogStash::Event.new(Hash\[@kname.zip(event.get(&#x27;message&#x27;).split(&#x27;?&#x27;))\])   </span><br><span class="line">            event.append(new\_event)&quot;  </span><br><span class="line">    &#125;  </span><br><span class="line">    if \[url\_arg\]&#123;  </span><br><span class="line">        kv&#123;  </span><br><span class="line">            source =&gt; &quot;url\_arg&quot;  </span><br><span class="line">            field\_split =&gt; &quot;&amp;&quot;  </span><br><span class="line">            target =&gt; &quot;url\_args&quot;  </span><br><span class="line">            remove\_field =&gt; \[&quot;url\_arg&quot;,&quot;message&quot;\]  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">\# ruby插件  </span><br><span class="line">\# 以？为分隔符，将request字段分成url\_path和url\_arg  </span><br><span class="line">\--------------------&gt;  </span><br><span class="line">www.test.com?test  </span><br><span class="line">&#123;  </span><br><span class="line">       &quot;url\_arg&quot; =&gt; &quot;test&quot;,  </span><br><span class="line">          &quot;host&quot; =&gt; &quot;localhost&quot;,  </span><br><span class="line">      &quot;url\_path&quot; =&gt; &quot;www.test.com&quot;,  </span><br><span class="line">       &quot;message&quot; =&gt; &quot;www.test.com?test&quot;,    </span><br><span class="line">      &quot;@version&quot; =&gt; &quot;1&quot;,  </span><br><span class="line">    &quot;@timestamp&quot; =&gt;  2018-06-26T07:31:04.887Z  </span><br><span class="line">&#125;  </span><br><span class="line">www.test.com?title=elk&amp;content=学习elk  </span><br><span class="line">&#123;  </span><br><span class="line">      &quot;url\_args&quot; =&gt; &#123;  </span><br><span class="line">          &quot;title&quot; =&gt; &quot;elk&quot;,  </span><br><span class="line">        &quot;content&quot; =&gt; &quot;学习elk&quot;  </span><br><span class="line">    &#125;,  </span><br><span class="line">          &quot;host&quot; =&gt; &quot;localhost&quot;,  </span><br><span class="line">      &quot;url\_path&quot; =&gt; &quot;www.test.com&quot;,  </span><br><span class="line">      &quot;@version&quot; =&gt; &quot;1&quot;,  </span><br><span class="line">    &quot;@timestamp&quot; =&gt;  2018-06-26T07:33:54.507Z  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="urldecode"><a href="#urldecode" class="headerlink" title="urldecode"></a>urldecode</h3><p>用于解码被编码的字段,可以解决URL中 中文乱码的问题</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">urldecode&#123;  </span><br><span class="line">        field =&gt; &quot;message&quot;  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">\# field :指定urldecode过滤器要转码的字段,默认值是&quot;message&quot;  </span><br><span class="line">\# charset(缺省): 指定过滤器使用的编码.默认UTF-8  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="kv"><a href="#kv" class="headerlink" title="kv"></a>kv</h3><p>通过指定分隔符将字符串分割成key&#x2F;value</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">kv&#123;  </span><br><span class="line">        prefix =&gt; &quot;url\_&quot;   #给分割后的key加前缀  </span><br><span class="line">        target =&gt; &quot;url\_ags&quot;    #将分割后的key-value放入指定字段  </span><br><span class="line">        source =&gt; &quot;message&quot;   #要分割的字段  </span><br><span class="line">        field\_split =&gt; &quot;&amp;&quot;    #指定分隔符  </span><br><span class="line">        remove\_field =&gt; &quot;message&quot;  </span><br><span class="line">    &#125;  </span><br><span class="line">\--------------------------&gt;  </span><br><span class="line">a=1&amp;b=2&amp;c=3  </span><br><span class="line">&#123;  </span><br><span class="line">            &quot;host&quot; =&gt; &quot;localhost&quot;,  </span><br><span class="line">       &quot;url\_ags&quot; =&gt; &#123;  </span><br><span class="line">          &quot;url\_c&quot; =&gt; &quot;3&quot;,  </span><br><span class="line">          &quot;url\_a&quot; =&gt; &quot;1&quot;,  </span><br><span class="line">          &quot;url\_b&quot; =&gt; &quot;2&quot;  </span><br><span class="line">    &#125;,  </span><br><span class="line">      &quot;@version&quot; =&gt; &quot;1&quot;,  </span><br><span class="line">    &quot;@timestamp&quot; =&gt; 2018-06-26T07:07:24.557Z  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="useragent"><a href="#useragent" class="headerlink" title="useragent"></a>useragent</h3><p>添加有关用户代理(如系列,操作系统,版本和设备)的信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">if \[agent\] != &quot;-&quot; &#123;  </span><br><span class="line">  useragent &#123;  </span><br><span class="line">    source =&gt; &quot;agent&quot;  </span><br><span class="line">    target =&gt; &quot;ua&quot;  </span><br><span class="line">    remove\_field =&gt; &quot;agent&quot;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">\# if语句，只有在agent字段不为空时才会使用该插件  </span><br><span class="line">#source 为必填设置,目标字段  </span><br><span class="line">#target 将useragent信息配置到ua字段中。如果不指定将存储在根目录中  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="数据输出"><a href="#数据输出" class="headerlink" title="数据输出"></a>数据输出</h2><blockquote>
<p>output配置定义了数据输出目标</p>
</blockquote>
<h3 id="stdout"><a href="#stdout" class="headerlink" title="stdout"></a>stdout</h3><p>将数据输出到屏幕上</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">input&#123;  </span><br><span class="line">      file&#123;  </span><br><span class="line">        path=&gt;&quot;/home/order.log&quot;  </span><br><span class="line"> 	    discover\_interval =&gt; 10   </span><br><span class="line"> 	    start\_position =&gt; &quot;beginning&quot;  </span><br><span class="line">      &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">output&#123;  </span><br><span class="line">    stdout &#123; codec =&gt; rubydebug &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_40.png"></p>
<h3 id="file-1"><a href="#file-1" class="headerlink" title="file"></a>file</h3><p>将数据写入文件</p>
<p>读取指定文件-输出到文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">input&#123;  </span><br><span class="line">      file&#123;  </span><br><span class="line">        path=&gt;&quot;/home/order.log&quot;  </span><br><span class="line"> 	    discover\_interval =&gt; 10   </span><br><span class="line"> 	    start\_position =&gt; &quot;beginning&quot;  </span><br><span class="line">      &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">output&#123;  </span><br><span class="line">     file&#123;  </span><br><span class="line">         path=&gt;&quot;/home/aaa.log&quot;  </span><br><span class="line">      &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>ps: 需要注意的是 这里的输出文件必须要求 w的权限 看看是否报错</p>
<p>如果报错需要进入容器赋权</p>
<h3 id="kafka-1"><a href="#kafka-1" class="headerlink" title="kafka"></a>kafka</h3><p>数据发送到kafka</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">kafka&#123;  </span><br><span class="line">       bootstrap\_servers =&gt; &quot;localhost:9092&quot;  </span><br><span class="line">       topic\_id =&gt; &quot;test\_topic&quot;  #必需的设置。生成消息的主题  </span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="elasticseach"><a href="#elasticseach" class="headerlink" title="elasticseach"></a>elasticseach</h3><p>数据存储到elasticseach中</p>
<p>读取指定文件-输出到es</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">input&#123;  </span><br><span class="line">      file&#123;  </span><br><span class="line">            path=&gt;&quot;/home/order.log&quot;  </span><br><span class="line"> 	    discover\_interval =&gt; 10   </span><br><span class="line"> 	    start\_position =&gt; &quot;beginning&quot;  </span><br><span class="line">      &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">output&#123;  </span><br><span class="line">      elasticsearch&#123;  </span><br><span class="line">            hosts=&gt;\[&quot;172.30.66.86:9200&quot;\]  </span><br><span class="line">            index =&gt; &quot;test-%&#123;+YYYY.MM.dd&#125;&quot;  </span><br><span class="line">      &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Kibana查看应用日志"><a href="#Kibana查看应用日志" class="headerlink" title="Kibana查看应用日志"></a>Kibana查看应用日志</h2><blockquote>
<p>实操过程，请参见23章视频：《100Wqps 超高并发日志平台》实操</p>
</blockquote>
<h3 id="1-查看应用日志"><a href="#1-查看应用日志" class="headerlink" title="1 查看应用日志"></a>1 查看应用日志</h3><p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_41.png"></p>
<h3 id="2-如何搜索日志"><a href="#2-如何搜索日志" class="headerlink" title="2 如何搜索日志"></a>2 如何搜索日志</h3><p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_42.png"></p>
<h3 id="3-如何查看指定时间的应用日志"><a href="#3-如何查看指定时间的应用日志" class="headerlink" title="3 如何查看指定时间的应用日志"></a>3 如何查看指定时间的应用日志</h3><ul>
<li>-&gt;右上角选择时间</li>
</ul>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_43.png"></p>
<h3 id="4-如何定位错误日志"><a href="#4-如何定位错误日志" class="headerlink" title="4 如何定位错误日志"></a>4 如何定位错误日志</h3><ul>
<li>Search框输入_error_ -&gt; Refresh (有自己的语法规则,要搜索一下)</li>
</ul>
<blockquote>
</blockquote>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_44.png"></p>
<h3 id="5-如何展开显示日志"><a href="#5-如何展开显示日志" class="headerlink" title="5 如何展开显示日志"></a>5 如何展开显示日志</h3><ul>
<li>连续点开两个箭头!</li>
<li><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_45.png"></li>
</ul>
<h2 id="es的安全认证"><a href="#es的安全认证" class="headerlink" title="es的安全认证"></a>es的安全认证</h2><p>通常搭建的elk默认是不需要身份认证,这样就会把数据暴露在外网,因此会显得非常危险。</p>
<p>下面我们介绍如何为es加入身份认证 es身份认证参考链接</p>
<blockquote>
<p>切记,这里 修改es 配置文件和 启动es的二进制文件的时候 一定要用es系统用户不要用ubuntu或root用户操作。不然会报错。</p>
</blockquote>
<p>配置了 安全认证后 logstash + filebeat +es +kibfana 都需要在配置文件中 加入 访问的账号密码来认证。 logstash 配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">elasticsearch &#123;  </span><br><span class="line">      hosts =&gt; \[&quot;ip:9200&quot;\]  </span><br><span class="line">      user =&gt; elastic  --加入es用户  </span><br><span class="line">      password =&gt; xxxx   --加入es密码  </span><br><span class="line">      index =&gt; &quot;test-%&#123;+YYYY-MM-dd&#125;&quot;  </span><br><span class="line">      timeout =&gt; 300  </span><br><span class="line">  &#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>kibfana 配置文件</p>
<p>配置 Kibana 以使用内置 kibana 用户和您创建的密码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">server.port: 5601  </span><br><span class="line">server.host: &quot;0.0.0.0&quot;  </span><br><span class="line">elasticsearch.hosts: \[&quot;http://localhost:9200&quot;\]  </span><br><span class="line">kibana.index: &quot;.kibana&quot;  </span><br><span class="line">i18n.locale: &quot;zh-CN&quot;  --配置 kibana 显示中文  </span><br><span class="line">elasticsearch.username: &quot;kibana&quot;   --加入kibana 账户  </span><br><span class="line">elasticsearch.password: &quot;123456&quot;   --加入kibana 账户的密码 </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="配置-elk的ElastAlert-预警插件"><a href="#配置-elk的ElastAlert-预警插件" class="headerlink" title="配置 elk的ElastAlert 预警插件"></a>配置 elk的ElastAlert 预警插件</h2><blockquote>
<p>我们都知道 elk架构 是收集与分析 集群的日志 方便开发排错</p>
<p>但是 预警功能是缺一不可的，如果开发人员不能及时查看线上错误日式,这个时候 就需要我们的预警插件来实现实时推送告警。 ElastAlert : 是python开发一个插件因此需要配合python运行环境和python-pip 包管理工具,以及相关依赖包</p>
</blockquote>
<h4 id="安装相关依赖包"><a href="#安装相关依赖包" class="headerlink" title="安装相关依赖包"></a>安装相关依赖包</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">yum -y install openssl openssl-devel gcc gcc-c++  --centos系统安装方式  </span><br><span class="line">\--ubuntu 安装方式  </span><br><span class="line">sudo apt-get install openssl  --openssl依赖包  </span><br><span class="line">sudo apt-get install libssl-dev  --openssl-devel 依赖包   </span><br><span class="line">sudo apt-get  install  build-essential   --gcc 依赖包 注意:gcc和g++版本必须一致  </span><br><span class="line">sudo apt-get install g++ 7.4  --g++ 依赖包   </span><br><span class="line">g++ --version --查看版本  </span><br><span class="line">gcc --version  </span><br><span class="line">wget https://www.python.org/ftp/python/3.6.9/Python-3.6.9.tgz --下载二进制python源码  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="安装python运行环境"><a href="#安装python运行环境" class="headerlink" title="安装python运行环境"></a>安装python运行环境</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">tar xf Python\-3.6.9.tgz  </span><br><span class="line">cd Python\-3.6.9./configure --prefix=/usr/local/python --with-openssl  </span><br><span class="line">make &amp;&amp; make install  --编译源码  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">mv /usr/bin/python /usr/bin/python\_old  //把ubuntu自带的python2.7环境移出到另外一个文件夹  </span><br><span class="line">ln -s /usr/local/python/bin/python3 /usr/bin/python  //建立python软链接  </span><br><span class="line">ln -s /usr/local/python/bin/pip3 /usr/bin/pip  //建立pip软链接  </span><br><span class="line">pip install --upgrade pip //此处没有安装pip的需要去安装pip  </span><br><span class="line">sudo apt install python3-pip //安装pip3.0版本 对应了python 3.6.9版本  </span><br><span class="line">//此处我没有动ubuntu自带的python2.7版本的 因此我们使用新的python使用3.6.9时,按以下方式使用:  </span><br><span class="line">python3.6 --version  </span><br><span class="line">python2.7 --version  </span><br><span class="line">pip3 --version  </span><br><span class="line">//使用python和pip命令时 都改为 python3.6与pip3  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>到此python环境配置完成</p>
<h4 id="安装elastalert"><a href="#安装elastalert" class="headerlink" title="安装elastalert"></a>安装elastalert</h4><p>下载源码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">git clone https://github.com/Yelp/elastalert.git //下载 源码  </span><br><span class="line">cd elastalert  </span><br><span class="line">pip3 install &quot;elasticsearch&lt;8,&gt;7&quot;      </span><br><span class="line">//因为我们的es是7.4.0，所以这里选用的版本是这个  </span><br><span class="line">pip3 install -r requirements.txt 用pip安装依赖  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>安装成功时候 &#x2F;usr&#x2F;local&#x2F;python&#x2F;bin&#x2F;目录下会有四个文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">ls /usr/local/python/bin/elastalert\* 或者这个目录下  </span><br><span class="line">ls /usr/local/bin/elastalert\*  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_46.png"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">ln -s /usr/local/python/bin/elastalert\* /usr/bin  //建立软链接把这四个命令链接到bin目录下  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="配置ElastAlert"><a href="#配置ElastAlert" class="headerlink" title="配置ElastAlert"></a>配置ElastAlert</h4><p>配置config.yaml 文件 (创建)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">cp config.yaml.example  config.yaml   </span><br><span class="line">sudo vi config.yaml  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_47.png"></p>
<blockquote>
<p>rules_folder：ElastAlert从中加载规则配置文件的位置。它将尝试加载文件夹中的每个.yaml文件。</p>
<p>没有任何有效规则，ElastAlert将无法启动。</p>
<p>run_every： ElastAlert多久查询一次Elasticsearch的时间。</p>
<p>buffer_time：查询窗口的大小，从运行每个查询的时间开始向后延伸。对于其中use_count_query或use_terms_query设置为true的规则，将忽略此值。</p>
<p>es_host：是Elasticsearch群集的地址，ElastAlert将在其中存储有关其状态，查询运行，警报和错误的数据。 es_port：es对应的端口。</p>
<p>es_username： 可选的; 用于连接的basic-auth用户名es_host。</p>
<p>es_password： 可选的; 用于连接的basic-auth密码es_host。</p>
<p>es_send_get_body_as： 可选的; 方法查询Elasticsearch - GET，POST或source。</p>
<p>默认是GET writeback_index：ElastAlert将在其中存储数据的索引的名称。我们稍后将创建此索引。</p>
<p>alert_time_limit： 失败警报的重试窗口。</p>
</blockquote>
<p>创建elastalert-create-index索引 告警索引</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">$ elastalert-create-index  </span><br><span class="line">New index name (Default elastalert\_status)  </span><br><span class="line">Name of existing index to copy (Default None)  </span><br><span class="line">New index elastalert\_status created  </span><br><span class="line">Done!  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="配置Rule-告警规则配置"><a href="#配置Rule-告警规则配置" class="headerlink" title="配置Rule 告警规则配置"></a>配置Rule 告警规则配置</h4><p>所有的告警规则，通过在example_rules目下创建配置文件进行定义，这里简单创建一个来作为演示</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">name: Nginx\_err  //规则名称  </span><br><span class="line">use\_strftine\_index: true   </span><br><span class="line">index: 10.0.0.153\-system\_cro\-2020.11.18  //监听查询es的索引  </span><br><span class="line">type: any    //告警规则类型 有很多种 这种是 只要匹配到就触发告警  </span><br><span class="line">aggregation:  </span><br><span class="line"> seconds: 1    //告警频率  </span><br><span class="line">filter:  </span><br><span class="line">\- query:  </span><br><span class="line">    query\_string:  </span><br><span class="line">         query: &quot;status:500 or status:404&quot; //触发报警的匹配条件 这里可以用kibana的语法去匹配  </span><br><span class="line">num\_events: 1  //事件触发次数 的贬值  </span><br><span class="line">timeframe:  </span><br><span class="line">  minutes: 1   //一分钟内超过 num\_envents触发的次数 就触发告警  </span><br><span class="line">alert:  </span><br><span class="line"> - &quot;email&quot;   //告警类型 此处是email 例如钉钉 企业微信  </span><br><span class="line">email\_format: html  //email 正文格式  </span><br><span class="line">alert\_subject: &quot;正式环境Error告警&quot;  //告警正文标题  </span><br><span class="line">alert\_text\_type: alert\_text\_only   //正文类型  </span><br><span class="line">alert\_text: &quot;&lt;br&gt;&lt;br&gt;&lt;h3&gt;告警详情&lt;/h3&gt;&lt;table&gt;&lt;tr&gt;&lt;td style=&#x27;padding:5px;text-align: right;font-weight: bold;border-radius: 5px;background-color: #eef;&#x27;&gt;@timestamp:&lt;/td&gt;&lt;td style=&#x27;padding:5px;border-radius: 5px;background-color: #eef;&#x27;&gt;&#123;&#125;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&#x27;padding:5px;text-align: right;font-weight: bold;border-radius: 5px;background-color: #eef;&#x27;&gt;@version:&lt;/td&gt;&lt;td style=&#x27;padding:5px;border-radius: 5px;background-color: #eef;&#x27;&gt;&#123;&#125;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&#x27;padding:5px;text-align: right;font-weight: bold;border-radius: 5px;background-color: #eef;&#x27;&gt;\_id:&lt;/td&gt;&lt;td style=&#x27;padding:5px;border-radius: 5px;background-color: #eef;&#x27;&gt;&#123;&#125;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&#x27;padding:5px;text-align: right;font-weight: bold;border-radius: 5px;background-color: #eef;&#x27;&gt;\_index:&lt;/td&gt;&lt;td style=&#x27;padding:5px;border-radius: 5px;background-color: #eef;&#x27;&gt;&#123;&#125;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&#x27;padding:5px;text-align: right;font-weight: bold;border-radius: 5px;background-color: #eef;&#x27;&gt;ip:&lt;/td&gt;&lt;td style=&#x27;padding:5px;border-radius: 5px;background-color: #eef;&#x27;&gt;&#123;&#125;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&#x27;padding:5px;text-align: right;font-weight: bold;border-radius: 5px;background-color: #eef;&#x27;&gt;request:&lt;/td&gt;&lt;td style=&#x27;padding:5px;border-radius: 5px;background-color: #eef;&#x27;&gt;&#123;&#125;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  style=&#x27;padding:5px;text-align: right;font-weight: bold;border-radius: 5px;background-color: #eef;&#x27;&gt;status:&lt;/td&gt;&lt;td style=&#x27;padding:5px;border-radius: 5px;background-color: #eef;&#x27;&gt;&#123;&#125;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&#x27;padding:5px;text-align: right;font-weight: bold;border-radius: 5px;background-color: #eef;&#x27;&gt;method:&lt;/td&gt;&lt;td style=&#x27;padding:5px;border-radius: 5px;background-color: #eef;&#x27;&gt;&#123;&#125;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&#x27;padding:5px;text-align: right;font-weight: bold;border-radius: 5px;background-color: #eef;&#x27;&gt;bytes:&lt;/td&gt;&lt;td style=&#x27;padding:5px;border-radius: 5px;background-color: #eef;&#x27;&gt;&#123;&#125;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&#x27;padding:5px;text-align: right;font-weight: bold;border-radius: 5px;background-color: #eef;&#x27;&gt;source:&lt;/td&gt;&lt;td style=&#x27;padding:5px;border-radius: 5px;background-color: #eef;&#x27;&gt;&#123;&#125;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&#x27;padding:5px;text-align: right;font-weight: bold;border-radius: 5px;background-color: #eef;&#x27;&gt;client\_ip:&lt;/td&gt;&lt;td style=&#x27;padding:5px;border-radius: 5px;background-color: #eef;&#x27;&gt;&#123;&#125;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&#x27;padding:5px;text-align: right;font-weight: bold;border-radius: 5px;background-color: #eef;&#x27;&gt;httpversion:&lt;/td&gt;&lt;td style=&#x27;padding:5px;border-radius: 5px;background-color: #eef;&#x27;&gt;&#123;&#125;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&quot;  //正文内容  </span><br><span class="line">alert\_text\_args:  </span><br><span class="line"> - &quot;@timestamp&quot;   //使用的是python的format格式动态填充数据  </span><br><span class="line"> - &quot;@version&quot;     //这些是属性值 按顺序对饮正文内容里面的 &#123;&#125;   </span><br><span class="line"> - \_id  </span><br><span class="line"> - \_index  </span><br><span class="line"> - host.name  </span><br><span class="line"> - request  </span><br><span class="line"> - status  </span><br><span class="line"> - method  </span><br><span class="line"> - bytes  </span><br><span class="line"> - message  </span><br><span class="line"> - remote\_ip  </span><br><span class="line"> - httpversion  </span><br><span class="line">email:  </span><br><span class="line"> - &quot;xxx@xx.com&quot;  //收件人 多个请依次往下填写  </span><br><span class="line"> - &quot;xxxx@qq.com&quot;  </span><br><span class="line"> - &quot;xxxx@xx.com&quot;  </span><br><span class="line">smtp\_host: smtp.mxhichina.com  //邮件服务器  </span><br><span class="line">smtp\_port: 25   //邮件端口  </span><br><span class="line">smtp\_auth\_file: /home/ubuntu/elk/alert/elastalert/smtp\_auth\_file.yaml //此处新建了一个文件是 发件人的认证文件 存放发件人账户和密码或授权码  </span><br><span class="line">from\_addr: haoyacong@gimmake.com  //发件人  </span><br><span class="line">email\_reply\_to: haoyacong@gimmake.com  //收件人标头  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>运行ElastAlert</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">cd ElastAlert  //ElastAlert 的安装目录  </span><br><span class="line">python3.6 -m elastalert.elastalert --verbose --config config.yaml --rule ./example\_rules/nginx\_404.yaml  //指定告警规则文件    </span><br><span class="line">nohup python3.6 -m elastalert.elastalert --verbose --config config.yaml --rule ./example\_rules/nginx\_404.yaml &amp; //在后台运行  </span><br><span class="line">//如果运行多个告警规则执行多个上面的命令  如果执行example\_rules下的全部规则文件 使用以下命令:  </span><br><span class="line">nohup python3.6 -m elastalert.elastalert --verbose --config config.yaml &amp;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="8-底座项目中ELK的应用"><a href="#8-底座项目中ELK的应用" class="headerlink" title="8 底座项目中ELK的应用"></a>8 底座项目中ELK的应用</h1><p>底座项目没有使用FELK架构，也就是没有部署filebeat，直接使用的java的loastash客户端，通过tcp协议直接采集logback日志到logstash</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;  </span><br><span class="line">    &lt;groupId&gt;net.logstash.logback&lt;/groupId&gt;  </span><br><span class="line">    &lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt;  </span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>后期可以自行根据上面的综合实操基于filebeat扩展</p>
<h2 id="8-1-logstash-logback-encoder简介"><a href="#8-1-logstash-logback-encoder简介" class="headerlink" title="8.1 logstash-logback-encoder简介"></a>8.1 logstash-logback-encoder简介</h2><p><code>logstash-logback-encoder</code> 是一个用于将 Java 应用程序的日志输出转换为 Logstash 可解析的 JSON 格式的库。它是基于 Logback（一个流行的 Java 日志框架）构建的，旨在使 Logback 输出与 Elasticsearch、Kibana 和 Logstash 等日志管理工具兼容，从而简化日志的收集和分析过程。</p>
<h3 id="主要功能和特点"><a href="#主要功能和特点" class="headerlink" title="主要功能和特点"></a>主要功能和特点</h3><ol>
<li><p><strong>JSON 格式输出</strong>： <code>logstash-logback-encoder</code> 允许你将日志输出格式化为 JSON，使得日志数据可以很容易地被 Logstash 处理和解析。Logstash 可以将这些 JSON 格式的日志数据传送到 Elasticsearch，供 Kibana 可视化展示。</p>
</li>
<li><p><strong>支持 Logback 配置</strong>： 它是基于 Logback 实现的，因此你可以使用 Logback 配置文件（如 <code>logback.xml</code>）来设置日志输出的格式。它支持将日志条目编码为标准的 JSON 格式，并可以灵活地自定义输出内容。</p>
</li>
<li><p><strong>灵活的字段映射</strong>： 你可以自定义 JSON 输出的字段。默认情况下，日志条目会包含如下字段：</p>
</li>
</ol>
<ul>
<li><p><code>timestamp</code>：日志条目的时间戳。</p>
</li>
<li><p><code>level</code>：日志级别（如 <code>INFO</code>、<code>ERROR</code>）。</p>
</li>
<li><p><code>thread</code>：线程名称。</p>
</li>
<li><p><code>logger</code>：日志记录器名称。</p>
</li>
<li><p><code>message</code>：日志消息本体。</p>
</li>
<li><p><code>exception</code>：异常信息（如果有的话）。</p>
</li>
</ul>
<ol start="5">
<li><p><strong>支持 MDC（Mapped Diagnostic Context）</strong>： <code>logstash-logback-encoder</code> 支持 Logback 的 MDC 功能，这允许你在日志中附加额外的上下文信息。例如，可以记录用户 ID、请求 ID、会话信息等，以便于后续分析。</p>
</li>
<li><p><strong>格式化选项</strong>： 你可以通过配置文件指定日志的格式，包括字段顺序、字段名称等。库提供了丰富的配置选项，帮助用户根据需求定制日志输出格式。</p>
</li>
<li><p><strong>与 ELK Stack 的集成</strong>： <code>logstash-logback-encoder</code> 是 ELK（Elasticsearch, Logstash, Kibana）栈中的重要组成部分。它将 Logback 日志输出转换为 JSON 格式，这使得日志可以很容易地被 Logstash 解析、索引到 Elasticsearch，并通过 Kibana 展示和分析。</p>
</li>
<li><p><strong>兼容性</strong>： 该库与 Logback 的其他日志输出配置兼容，可以与其他类型的日志记录器和框架（如 SLF4J）一起使用。</p>
</li>
</ol>
<h3 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h3><h4 id="1-添加-Maven-依赖"><a href="#1-添加-Maven-依赖" class="headerlink" title="1. 添加 Maven 依赖"></a>1. 添加 Maven 依赖</h4><p>首先，确保你在项目中添加了 <code>logstash-logback-encoder</code> 的 Maven 依赖。在 <code>pom.xml</code> 中添加以下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;dependency\&gt;  </span><br><span class="line">    &lt;groupId\&gt;net.logstash.logback&lt;/groupId\&gt;  </span><br><span class="line">    &lt;artifactId\&gt;logstash-logback-encoder&lt;/artifactId\&gt;  </span><br><span class="line">    &lt;version\&gt;7.0&lt;/version\&gt; &lt;!-- 请使用最新版本 --&gt;  </span><br><span class="line">&lt;/dependency\&gt;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="2-配置-Logback-输出为-JSON-格式"><a href="#2-配置-Logback-输出为-JSON-格式" class="headerlink" title="2. 配置 Logback 输出为 JSON 格式"></a>2. 配置 Logback 输出为 JSON 格式</h4><p>在 Logback 配置文件（通常是 <code>logback.xml</code>）中，你可以配置一个 <code>Encoder</code> 来输出 JSON 格式的日志。以下是一个简单的例子：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;configuration\&gt;  </span><br><span class="line">  </span><br><span class="line">    &lt;appender name\=&quot;LOGSTASH&quot; class\=&quot;ch.qos.logback.core.ConsoleAppender&quot;\&gt;  </span><br><span class="line">        &lt;encoder class\=&quot;net.logstash.logback.encoder.LogstashEncoder&quot;/&gt;  </span><br><span class="line">    &lt;/appender\&gt;  </span><br><span class="line">  </span><br><span class="line">    &lt;root level\=&quot;INFO&quot;\&gt;  </span><br><span class="line">        &lt;appender-ref ref\=&quot;LOGSTASH&quot;/&gt;  </span><br><span class="line">    &lt;/root\&gt;  </span><br><span class="line">  </span><br><span class="line">&lt;/configuration\&gt;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这个配置会将所有日志输出到控制台，并且使用 <code>LogstashEncoder</code> 将日志条目转换为 JSON 格式。</p>
<h4 id="3-使用自定义日志格式"><a href="#3-使用自定义日志格式" class="headerlink" title="3. 使用自定义日志格式"></a>3. 使用自定义日志格式</h4><p>你还可以通过配置更多的选项来自定义输出的 JSON 结构。例如，添加 MDC 数据、时间戳格式等：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;encoder class\=&quot;net.logstash.logback.encoder.LogstashEncoder&quot;\&gt;  </span><br><span class="line">    &lt;timestampPattern\&gt;yyyy-MM-dd&#x27;T&#x27;HH:mm:ss.SSSZ&lt;/timestampPattern\&gt;  </span><br><span class="line">    &lt;fieldNames\&gt;  </span><br><span class="line">        &lt;message\&gt;msg&lt;/message\&gt;  </span><br><span class="line">        &lt;timestamp\&gt;time&lt;/timestamp\&gt;  </span><br><span class="line">        &lt;level\&gt;loglevel&lt;/level\&gt;  </span><br><span class="line">        &lt;logger\&gt;logger\_name&lt;/logger\&gt;  </span><br><span class="line">        &lt;thread\&gt;thread\_name&lt;/thread\&gt;  </span><br><span class="line">    &lt;/fieldNames\&gt;  </span><br><span class="line">&lt;/encoder\&gt;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在此示例中，时间戳的格式被自定义为 ISO 8601 格式，字段名称也被自定义。</p>
<h3 id="常见配置项"><a href="#常见配置项" class="headerlink" title="常见配置项"></a>常见配置项</h3><ul>
<li><p><code>timestampPattern</code>：指定日志时间戳的格式（默认使用 ISO 8601 格式）。</p>
</li>
<li><p><code>fieldNames</code>：用于自定义输出的字段名称。</p>
</li>
<li><p><code>includeCallerData</code>：是否将调用者信息（如类名、方法名）包含在日志中。</p>
</li>
<li><p><code>customFields</code>：用于添加自定义的静态字段，适用于在每条日志中注入相同的字段（如应用程序名称、版本号等）。</p>
</li>
<li><p><code>stackTraceElementConverter</code>：用于控制如何格式化堆栈跟踪信息。</p>
</li>
</ul>
<h3 id="示例-JSON-输出"><a href="#示例-JSON-输出" class="headerlink" title="示例 JSON 输出"></a>示例 JSON 输出</h3><p>使用 <code>logstash-logback-encoder</code> 输出的 JSON 日志条目可能类似于：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">json&#123;  </span><br><span class="line">  &quot;timestamp&quot;: &quot;2024-11-17T10:55:27.123+0000&quot;,  </span><br><span class="line">  &quot;loglevel&quot;: &quot;INFO&quot;,  </span><br><span class="line">  &quot;logger\_name&quot;: &quot;com.example.MyClass&quot;,  </span><br><span class="line">  &quot;thread\_name&quot;: &quot;main&quot;,  </span><br><span class="line">  &quot;msg&quot;: &quot;Application started&quot;,  </span><br><span class="line">  &quot;exception&quot;: null  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><code>logstash-logback-encoder</code> 是一个专为将 Logback 日志输出为 Logstash 可处理的 JSON 格式设计的库，主要用于与 ELK Stack（Elasticsearch、Logstash、Kibana）结合使用。通过该库，Java 应用程序可以生成结构化的日志数据，方便后续的日志收集、存储和分析。其强大的自定义功能使其非常适合用于生产环境中对日志的高效管理。</p>
<h2 id="8-2-logstash的java客户端配置"><a href="#8-2-logstash的java客户端配置" class="headerlink" title="8.2 logstash的java客户端配置"></a>8.2 logstash的java客户端配置</h2><p>在需要elk采集日志的app引入<code>ruoyi-common-logstash</code>依赖，依赖<code>logstash-logback-encoder</code>采集日志</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;dependency\&gt;  </span><br><span class="line">            &lt;groupId\&gt;com.ruoyi&lt;/groupId\&gt;  </span><br><span class="line">            &lt;artifactId\&gt;ruoyi-common-logstash&lt;/artifactId\&gt;  </span><br><span class="line">        &lt;/dependency\&gt;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>logback-logstash.xml</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;  </span><br><span class="line">  </span><br><span class="line">&lt;included\&gt;  </span><br><span class="line">  </span><br><span class="line">    &lt;springProperty scope\=&quot;context&quot; name\=&quot;appName&quot; source\=&quot;spring.application.name&quot;/&gt;  </span><br><span class="line">  </span><br><span class="line">    &lt;!--输出到logstash的appender--&gt;  </span><br><span class="line">    &lt;appender name\=&quot;logstash&quot; class\=&quot;net.logstash.logback.appender.LogstashTcpSocketAppender&quot;\&gt;  </span><br><span class="line">        &lt;!--可以访问的logstash日志收集端口--&gt;  </span><br><span class="line">        &lt;destination\&gt;$&#123;logstash.address&#125;&lt;/destination\&gt;  </span><br><span class="line">        &lt;encoder charset\=&quot;UTF-8&quot; class\=&quot;net.logstash.logback.encoder.LogstashEncoder&quot;\&gt;  </span><br><span class="line">            &lt;customFields\&gt;&#123;&quot;spring.application.name&quot;:&quot;$&#123;appName&#125;&quot;&#125;&lt;/customFields\&gt;  </span><br><span class="line">        &lt;/encoder\&gt;  </span><br><span class="line">    &lt;/appender\&gt;  </span><br><span class="line">  </span><br><span class="line">    &lt;root level\=&quot;info&quot;\&gt;  </span><br><span class="line">        &lt;appender-ref ref\=&quot;logstash&quot;/&gt;  </span><br><span class="line">    &lt;/root\&gt;  </span><br><span class="line">&lt;/included\&gt;  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>父工程pom.xml配置<code>logstash.address</code></p>
<p><img src="/./2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/img_48.png"></p>
<h2 id="8-3-一键部署elk"><a href="#8-3-一键部署elk" class="headerlink" title="8.3 一键部署elk"></a>8.3 一键部署elk</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">elasticsearch:  </span><br><span class="line">    image: elasticsearch:7.17.6  </span><br><span class="line">    container\_name: elasticsearch  </span><br><span class="line">    ports:  </span><br><span class="line">      \- &quot;9200:9200&quot;  </span><br><span class="line">      \- &quot;9300:9300&quot;  </span><br><span class="line">    environment:  </span><br><span class="line">      \# 设置集群名称  </span><br><span class="line">      cluster.name: elasticsearch  </span><br><span class="line">      \# 以单一节点模式启动  </span><br><span class="line">      discovery.type: single-node  </span><br><span class="line">      ES\_JAVA\_OPTS: &quot;-Xms512m -Xmx512m&quot;  </span><br><span class="line">    volumes:  </span><br><span class="line">      \- /docker/elk/elasticsearch/plugins:/usr/share/elasticsearch/plugins  </span><br><span class="line">      \- /docker/elk/elasticsearch/data:/usr/share/elasticsearch/data  </span><br><span class="line">      \- /docker/elk/elasticsearch/logs:/usr/share/elasticsearch/logs  </span><br><span class="line">    network\_mode: &quot;host&quot;  </span><br><span class="line">  </span><br><span class="line">  kibana:  </span><br><span class="line">    image: kibana:7.17.6  </span><br><span class="line">    container\_name: kibana  </span><br><span class="line">    ports:  </span><br><span class="line">      \- &quot;5601:5601&quot;  </span><br><span class="line">    depends\_on:  </span><br><span class="line">      \# kibana在elasticsearch启动之后再启动  </span><br><span class="line">      \- elasticsearch  </span><br><span class="line">    environment:  </span><br><span class="line">      #设置系统语言文中文  </span><br><span class="line">      I18N\_LOCALE: zh-CN  </span><br><span class="line">      \# 访问域名  </span><br><span class="line">      \# SERVER\_PUBLICBASEURL: https://kibana.cloud.com  </span><br><span class="line">    volumes:  </span><br><span class="line">      \- /docker/elk/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml  </span><br><span class="line">    network\_mode: &quot;host&quot;  </span><br><span class="line">  </span><br><span class="line">  logstash:  </span><br><span class="line">    image: logstash:7.17.6  </span><br><span class="line">    container\_name: logstash  </span><br><span class="line">    ports:  </span><br><span class="line">      \- &quot;4560:4560&quot;  </span><br><span class="line">    volumes:  </span><br><span class="line">      \- /docker/elk/logstash/pipeline/logstash.conf:/usr/share/logstash/pipeline/logstash.conf  </span><br><span class="line">      \- /docker/elk/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml  </span><br><span class="line">    depends\_on:  </span><br><span class="line">      \- elasticsearch  </span><br><span class="line">    network\_mode: &quot;host&quot;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>logstash配置tcp类型输入，输出es</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">input &#123;  </span><br><span class="line">  tcp &#123;  </span><br><span class="line">    mode =&gt; &quot;server&quot;  </span><br><span class="line">    host =&gt; &quot;0.0.0.0&quot;  </span><br><span class="line">    port =&gt; 4560  </span><br><span class="line">    codec =&gt; json\_lines  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">output &#123;  </span><br><span class="line">  elasticsearch &#123;  </span><br><span class="line">    hosts =&gt; &quot;127.0.0.1:9200&quot;  </span><br><span class="line">    index =&gt; &quot;%&#123;\[spring.application.name\]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://flowtime.asia">周五打工人</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://flowtime.asia/2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/">http://flowtime.asia/2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://flowtime.asia" target="_blank">繁华流年间</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ELK/">ELK</a></div><div class="post_share"><div class="social-share" data-image="/img/liushui.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/03/11/CAP%E5%AE%9A%E7%90%86-%E5%93%AA%E4%BA%9B%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%98%AFAP-%E5%93%AA%E4%BA%9B%E6%98%AFCP-%E4%B8%BA%E4%BB%80%E4%B9%88/" title="CAP定理,哪些中间件是AP,哪些是CP,为什么"><img class="cover" src="/img/haibian.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">CAP定理,哪些中间件是AP,哪些是CP,为什么</div></div></a></div><div class="next-post pull-right"><a href="/2025/03/06/%E4%BA%BF%E7%BA%A7redis%E6%8E%92%E8%A1%8C%E6%A6%9C%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1/" title="亿级redis排行榜如何设计"><img class="cover" src="/img/hubian.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">亿级redis排行榜如何设计</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/nav.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">周五打工人</div><div class="author-info__description">残阳掠过飞鸟的尾羽，暮色便染红了天际线。我常倚在雕花木窗前，看晨曦的银线如何被晚风纺成昏黄的绢帛。檐角铜铃数着更漏，风起时，满庭银杏便簌簌落下金箔般的时光碎片——昨日尚在枝头招摇的新绿，今晨已零落成泥。河水载着落英昼夜奔涌，恍惚间，岸边垂钓的少年已化作拄杖的老叟。马厩里嘶鸣的乌骓踏碎月影，待晨光刺破雾霭，马蹄印里竟开出点点白梅。原来岁月是永不回头的骑手，我们皆是祂扬鞭时抖落的尘，在时光的褶皱里开出刹那芳华，又被新的晨昏覆盖成苔痕斑驳的诗行。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">51</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">42</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">无论你是追求知识的急先锋，还是对生活感悟充满好奇，亦或是渴望找到属于自己情感共鸣的灵魂，本博客都将竭诚欢迎你的加入。共同书写属于自己的精彩篇章。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#elk%EF%BC%88Elasticsearch%E3%80%81Logstash%E3%80%81Kibana%EF%BC%89%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A%E7%9A%84-%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF"><span class="toc-number">1.</span> <span class="toc-text">elk（Elasticsearch、Logstash、Kibana）从入门到精通的 学习路线</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-ELK%E6%97%A5%E5%BF%97%E5%B9%B3%E5%8F%B0%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.</span> <span class="toc-text">1 ELK日志平台介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E5%9F%BA%E7%A1%80%E6%97%A5%E5%BF%97%E5%B9%B3%E5%8F%B0%E7%9A%84%E8%83%8C%E6%99%AF"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 基础日志平台的背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-ELK%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">2.2.</span> <span class="toc-text">1.2 ELK的关系</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-ELK%E7%9A%84%E6%9E%B6%E6%9E%84"><span class="toc-number">2.3.</span> <span class="toc-text">1.3 ELK的架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-1-%E7%AE%80%E5%8D%95%E7%9A%84ELK%E6%97%A5%E5%BF%97%E5%B9%B3%E5%8F%B0"><span class="toc-number">2.3.1.</span> <span class="toc-text">1.3.1 简单的ELK日志平台</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-2-ELK%E6%94%B9%E8%BF%9B%E4%B9%8B%E5%BC%95%E5%85%A5Filebeat"><span class="toc-number">2.3.2.</span> <span class="toc-text">1.3.2 ELK改进之引入Filebeat</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-3-ELK%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">2.3.3.</span> <span class="toc-text">1.3.3 ELK的应用场景</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-ELK%E7%9A%84%E4%B8%8D%E8%B6%B3"><span class="toc-number">2.4.</span> <span class="toc-text">1.4 ELK的不足</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#es%E7%9A%84%E8%B5%84%E6%BA%90%E5%8D%A0%E7%94%A8"><span class="toc-number">2.4.1.</span> <span class="toc-text">es的资源占用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%98%E5%82%A8%E5%AE%B9%E9%87%8F%EF%BC%9A"><span class="toc-number">2.4.2.</span> <span class="toc-text">存储容量：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90%EF%BC%9A"><span class="toc-number">2.4.3.</span> <span class="toc-text">计算资源：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%86%E7%89%87%E6%95%B0%E9%87%8F%E8%AF%84%E4%BC%B0%EF%BC%9A"><span class="toc-number">2.4.4.</span> <span class="toc-text">索引和分片数量评估：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%BB%A1%E8%B6%B310w%E7%BA%A7%E3%80%81100Wqps%E5%90%9E%E5%90%90%E9%87%8Fqps%E3%80%81EB%E7%BA%A7%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8%E5%91%A2"><span class="toc-number">2.4.5.</span> <span class="toc-text">如何满足10w级、100Wqps吞吐量qps、EB级日志存储呢</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85-es-logstash-kibana"><span class="toc-number">3.</span> <span class="toc-text">2 一键安装 es+logstash+ kibana</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E5%BA%94%E7%9A%84%E9%95%9C%E5%83%8F%E7%89%88%E6%9C%AC"><span class="toc-number">3.0.1.</span> <span class="toc-text">对应的镜像版本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#docker%E7%BC%96%E7%A0%81%E6%96%87%E4%BB%B6"><span class="toc-number">3.0.2.</span> <span class="toc-text">docker编码文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BF%E9%97%AEkibana"><span class="toc-number">3.0.3.</span> <span class="toc-text">访问kibana</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8kibana%E6%98%BE%E7%A4%BA%E7%9A%84%E6%95%88%E6%9E%9C"><span class="toc-number">3.0.4.</span> <span class="toc-text">在kibana显示的效果</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Elasticsearch%E5%9F%BA%E7%A1%80%E5%92%8C%E5%AE%9E%E6%93%8D"><span class="toc-number">4.</span> <span class="toc-text">3 Elasticsearch基础和实操</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-filebeat%E5%9F%BA%E7%A1%80%E5%92%8C%E5%AE%9E%E6%93%8D"><span class="toc-number">5.</span> <span class="toc-text">4 filebeat基础和实操</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-filebeat%E5%92%8Cbeats%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">5.1.</span> <span class="toc-text">4.1 filebeat和beats的关系</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-Filebeat%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-number">5.2.</span> <span class="toc-text">4.2 Filebeat工作原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-Filebeat%E5%90%AF%E5%8A%A8%E5%91%BD%E4%BB%A4"><span class="toc-number">5.3.</span> <span class="toc-text">4.3 Filebeat启动命令</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-Filebeat%E6%96%87%E4%BB%B6%E5%A4%B9%E7%BB%93%E6%9E%84"><span class="toc-number">5.4.</span> <span class="toc-text">4.4 Filebeat文件夹结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5-Filebeat%E9%85%8D%E7%BD%AE%E5%8F%82%E8%80%83"><span class="toc-number">5.5.</span> <span class="toc-text">4.5 Filebeat配置参考</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E9%85%8D%E7%BD%AE"><span class="toc-number">5.5.1.</span> <span class="toc-text">输入配置</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#log-input"><span class="toc-number">5.5.1.1.</span> <span class="toc-text">log input</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E9%A1%B9"><span class="toc-number">5.5.1.1.1.</span> <span class="toc-text">配置项</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%89%B9%E5%88%AB%E8%AF%B4%E6%98%8E%EF%BC%9Amultiline%E7%AE%A1%E7%90%86%E5%A4%9A%E8%A1%8C%E6%B6%88%E6%81%AF"><span class="toc-number">5.5.1.1.2.</span> <span class="toc-text">特别说明：multiline管理多行消息</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E9%85%8D%E7%BD%AE"><span class="toc-number">5.5.2.</span> <span class="toc-text">输出配置</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Logstash-output"><span class="toc-number">5.5.2.1.</span> <span class="toc-text">Logstash output</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-6-%E5%AE%9E%E6%93%8D%E6%A1%88%E4%BE%8B"><span class="toc-number">5.6.</span> <span class="toc-text">4.6 实操案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-1-%E6%A0%87%E5%87%86%E8%BE%93%E5%85%A5%EF%BC%8C%E6%8E%A7%E5%88%B6%E5%8F%B0%E8%BE%93%E5%87%BA"><span class="toc-number">5.6.1.</span> <span class="toc-text">4.6.1 标准输入，控制台输出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-2-%E6%97%A5%E5%BF%97%E8%BE%93%E5%85%A5%EF%BC%8C%E6%8E%A7%E5%88%B6%E5%8F%B0%E8%BE%93%E5%87%BA"><span class="toc-number">5.6.2.</span> <span class="toc-text">4.6.2 日志输入，控制台输出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-3-%E6%97%A5%E5%BF%97%E8%BE%93%E5%85%A5%EF%BC%8CES%E8%BE%93%E5%87%BA"><span class="toc-number">5.6.3.</span> <span class="toc-text">4.6.3 日志输入，ES输出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-4-%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%EF%BC%9ATomcat%E6%97%A5%E5%BF%97"><span class="toc-number">5.6.4.</span> <span class="toc-text">4.6.4 企业级实战案例：Tomcat日志</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-6-4-1-%E5%8E%9F%E7%94%9Ftomcat%E6%97%A5%E5%BF%97"><span class="toc-number">5.6.4.1.</span> <span class="toc-text">4.6.4.1 原生tomcat日志</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-6-4-2-json%E6%A0%BC%E5%BC%8Ftomcat%E6%97%A5%E5%BF%97"><span class="toc-number">5.6.4.2.</span> <span class="toc-text">4.6.4.2 json格式tomcat日志</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-6-4-3-%E5%A4%9A%E8%A1%8C%E5%8C%B9%E9%85%8D%EF%BC%8C%E6%94%B6%E9%9B%86tomcat%E9%94%99%E8%AF%AF%E6%97%A5%E5%BF%97"><span class="toc-number">5.6.4.3.</span> <span class="toc-text">4.6.4.3 多行匹配，收集tomcat错误日志</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-logstash%E5%9F%BA%E7%A1%80%E5%92%8C%E5%AE%9E%E6%93%8D"><span class="toc-number">6.</span> <span class="toc-text">5 logstash基础和实操</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-logstash%E4%BD%9C%E7%94%A8%EF%BC%9A"><span class="toc-number">6.1.</span> <span class="toc-text">5.1 logstash作用：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-logstash%E7%9A%84%E6%9E%B6%E6%9E%84%EF%BC%9A"><span class="toc-number">6.2.</span> <span class="toc-text">5.2 logstash的架构：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Input-%E8%BE%93%E5%85%A5%EF%BC%89%EF%BC%9A"><span class="toc-number">6.2.1.</span> <span class="toc-text">Input(输入）：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Filter-%E8%BF%87%E6%BB%A4%E5%99%A8%EF%BC%89"><span class="toc-number">6.2.2.</span> <span class="toc-text">Filter(过滤器）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Output-%E8%BE%93%E5%87%BA%EF%BC%89%EF%BC%9A"><span class="toc-number">6.2.3.</span> <span class="toc-text">Output(输出）：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-Logstash%E7%9A%84%E4%B8%8D%E8%B6%B3"><span class="toc-number">6.3.</span> <span class="toc-text">5.3 Logstash的不足</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-logstash%E8%AF%BB%E5%8F%96filebeat-%E8%BE%93%E5%87%BA%E5%88%B0es%E9%9B%86%E7%BE%A4"><span class="toc-number">6.4.</span> <span class="toc-text">5.4 logstash读取filebeat-输出到es集群</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-5-logstash%E5%AE%9E%E6%93%8D%E6%A1%88%E4%BE%8B"><span class="toc-number">6.5.</span> <span class="toc-text">5.5 logstash实操案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-1-%E6%A0%87%E5%87%86%E8%BE%93%E5%85%A5%EF%BC%8C%E6%A0%87%E5%87%86%E8%BE%93%E5%87%BA"><span class="toc-number">6.5.1.</span> <span class="toc-text">5.5.1 标准输入，标准输出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-2-tcp%E8%BE%93%E5%85%A5%EF%BC%8C%E6%A0%87%E5%87%86%E8%BE%93%E5%87%BA"><span class="toc-number">6.5.2.</span> <span class="toc-text">5.5.2 tcp输入，标准输出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-3-http%E8%BE%93%E5%85%A5%EF%BC%8C%E6%A0%87%E5%87%86%E8%BE%93%E5%87%BA"><span class="toc-number">6.5.3.</span> <span class="toc-text">5.5.3 http输入，标准输出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-4-gork%E6%8F%92%E4%BB%B6"><span class="toc-number">6.5.4.</span> <span class="toc-text">5.5.4 gork插件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-5-filebeat%E8%BE%93%E5%85%A5%EF%BC%8C%E8%BE%93%E5%87%BAes-tomcat%E6%97%A5%E5%BF%97"><span class="toc-number">6.5.5.</span> <span class="toc-text">5.5.5 filebeat输入，输出es(tomcat日志)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Kibana%E5%9F%BA%E7%A1%80%E5%92%8C%E5%AE%9E%E6%93%8D"><span class="toc-number">7.</span> <span class="toc-text">6 Kibana基础和实操</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-ELK%E7%BB%BC%E5%90%88%E5%AE%9E%E6%93%8D"><span class="toc-number">8.</span> <span class="toc-text">7 ELK综合实操</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8filebeat%E5%8F%91%E9%80%81%E6%97%A5%E5%BF%97"><span class="toc-number">8.1.</span> <span class="toc-text">使用filebeat发送日志</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%B6%E4%BD%9Cfilebeat%E9%95%9C%E5%83%8F"><span class="toc-number">8.1.1.</span> <span class="toc-text">制作filebeat镜像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%B6%E4%BD%9C%E5%9F%BA%E7%A1%80%E7%9A%84unbantu%E9%95%9C%E5%83%8F"><span class="toc-number">8.1.2.</span> <span class="toc-text">制作基础的unbantu镜像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E9%80%81%E9%95%9C%E5%83%8F%E5%88%B0dockerhub"><span class="toc-number">8.1.3.</span> <span class="toc-text">推送镜像到dockerhub</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%B6%E4%BD%9Cfilebeat%E9%95%9C%E5%83%8F-1"><span class="toc-number">8.1.4.</span> <span class="toc-text">制作filebeat镜像</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#dockerfile"><span class="toc-number">8.1.4.1.</span> <span class="toc-text">dockerfile</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A8%E9%80%81%E9%95%9C%E5%83%8F%E5%88%B0dockerhub-1"><span class="toc-number">8.1.4.2.</span> <span class="toc-text">推送镜像到dockerhub</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#example-application%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9A%84filebeat%E9%85%8D%E7%BD%AE%EF%BC%9A"><span class="toc-number">8.2.</span> <span class="toc-text">example-application微服务的filebeat配置：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#filebeat-yml%E7%9A%84%E5%8F%82%E8%80%83%E9%85%8D%E7%BD%AE%EF%BC%9A"><span class="toc-number">8.2.1.</span> <span class="toc-text">filebeat.yml的参考配置：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#input-yml%E9%85%8D%E7%BD%AE%EF%BC%9A"><span class="toc-number">8.2.2.</span> <span class="toc-text">input.yml配置：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9dockerfile"><span class="toc-number">8.2.3.</span> <span class="toc-text">修改dockerfile</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E9%94%AE%E5%8F%91%E5%B8%83"><span class="toc-number">8.2.4.</span> <span class="toc-text">一键发布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E4%B9%8B%E5%90%8E"><span class="toc-number">8.2.5.</span> <span class="toc-text">启动之后</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#message-dispatcher%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E6%97%A5%E5%BF%97"><span class="toc-number">8.2.6.</span> <span class="toc-text">message-dispatcher微服务的日志</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E6%97%A5%E5%BF%97%E7%B4%A2%E5%BC%95"><span class="toc-number">8.2.7.</span> <span class="toc-text">查看日志索引</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#logstash-%E8%AF%A6%E8%A7%A3"><span class="toc-number">8.3.</span> <span class="toc-text">logstash 详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#stash%E7%AC%AC%E4%B8%80%E4%B8%AA%E4%BA%8B%E4%BB%B6"><span class="toc-number">8.3.1.</span> <span class="toc-text">stash第一个事件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Logstash%E7%9A%84%E6%A0%B8%E5%BF%83%E6%B5%81%E7%A8%8B%E7%9A%84%E4%B8%89%E4%B8%AA%E7%8E%AF%E8%8A%82"><span class="toc-number">8.3.1.1.</span> <span class="toc-text">Logstash的核心流程的三个环节</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#logstash%E6%95%B0%E5%80%BC%E7%B1%BB%E5%9E%8B"><span class="toc-number">8.3.2.</span> <span class="toc-text">logstash数值类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#logstash-%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD"><span class="toc-number">8.3.3.</span> <span class="toc-text">logstash 条件判断</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#logstash-%E6%AF%94%E8%BE%83%E8%BF%90%E7%AE%97%E7%AC%A6"><span class="toc-number">8.3.4.</span> <span class="toc-text">logstash 比较运算符</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5%E7%8E%AF%E8%8A%82"><span class="toc-number">8.4.</span> <span class="toc-text">数据输入环节</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#stdin"><span class="toc-number">8.4.1.</span> <span class="toc-text">stdin</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#file"><span class="toc-number">8.4.2.</span> <span class="toc-text">file</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#syslogs"><span class="toc-number">8.4.3.</span> <span class="toc-text">syslogs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#beats"><span class="toc-number">8.4.4.</span> <span class="toc-text">beats</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka"><span class="toc-number">8.4.5.</span> <span class="toc-text">kafka</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%8E%AF%E8%8A%82"><span class="toc-number">8.5.</span> <span class="toc-text">数据处理环节</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#grok%E8%A7%A3%E6%9E%90%E6%96%87%E6%9C%AC%E5%B9%B6%E6%9E%84%E9%80%A0"><span class="toc-number">8.5.1.</span> <span class="toc-text">grok解析文本并构造</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#date%E6%97%A5%E6%9C%9F%E8%A7%A3%E6%9E%90"><span class="toc-number">8.5.2.</span> <span class="toc-text">date日期解析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mutate%E5%AD%97%E6%AE%B5%E8%BD%AC%E6%8D%A2"><span class="toc-number">8.5.3.</span> <span class="toc-text">mutate字段转换</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#covert%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="toc-number">8.6.</span> <span class="toc-text">covert类型转换</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#split"><span class="toc-number">8.6.1.</span> <span class="toc-text">split</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#merge"><span class="toc-number">8.6.2.</span> <span class="toc-text">merge</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rename"><span class="toc-number">8.6.3.</span> <span class="toc-text">rename</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#remove-field%EF%BC%9A%E7%A7%BB%E9%99%A4%E5%AD%97%E6%AE%B5"><span class="toc-number">8.6.4.</span> <span class="toc-text">remove_field：移除字段</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#join"><span class="toc-number">8.6.5.</span> <span class="toc-text">join</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#geoip"><span class="toc-number">8.6.6.</span> <span class="toc-text">geoip</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ruby"><span class="toc-number">8.6.7.</span> <span class="toc-text">ruby</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#urldecode"><span class="toc-number">8.6.8.</span> <span class="toc-text">urldecode</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kv"><span class="toc-number">8.6.9.</span> <span class="toc-text">kv</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#useragent"><span class="toc-number">8.6.10.</span> <span class="toc-text">useragent</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA"><span class="toc-number">8.7.</span> <span class="toc-text">数据输出</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#stdout"><span class="toc-number">8.7.1.</span> <span class="toc-text">stdout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#file-1"><span class="toc-number">8.7.2.</span> <span class="toc-text">file</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka-1"><span class="toc-number">8.7.3.</span> <span class="toc-text">kafka</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#elasticseach"><span class="toc-number">8.7.4.</span> <span class="toc-text">elasticseach</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kibana%E6%9F%A5%E7%9C%8B%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97"><span class="toc-number">8.8.</span> <span class="toc-text">Kibana查看应用日志</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%9F%A5%E7%9C%8B%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97"><span class="toc-number">8.8.1.</span> <span class="toc-text">1 查看应用日志</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%A6%82%E4%BD%95%E6%90%9C%E7%B4%A2%E6%97%A5%E5%BF%97"><span class="toc-number">8.8.2.</span> <span class="toc-text">2 如何搜索日志</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E6%8C%87%E5%AE%9A%E6%97%B6%E9%97%B4%E7%9A%84%E5%BA%94%E7%94%A8%E6%97%A5%E5%BF%97"><span class="toc-number">8.8.3.</span> <span class="toc-text">3 如何查看指定时间的应用日志</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%A6%82%E4%BD%95%E5%AE%9A%E4%BD%8D%E9%94%99%E8%AF%AF%E6%97%A5%E5%BF%97"><span class="toc-number">8.8.4.</span> <span class="toc-text">4 如何定位错误日志</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%A6%82%E4%BD%95%E5%B1%95%E5%BC%80%E6%98%BE%E7%A4%BA%E6%97%A5%E5%BF%97"><span class="toc-number">8.8.5.</span> <span class="toc-text">5 如何展开显示日志</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#es%E7%9A%84%E5%AE%89%E5%85%A8%E8%AE%A4%E8%AF%81"><span class="toc-number">8.9.</span> <span class="toc-text">es的安全认证</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE-elk%E7%9A%84ElastAlert-%E9%A2%84%E8%AD%A6%E6%8F%92%E4%BB%B6"><span class="toc-number">8.10.</span> <span class="toc-text">配置 elk的ElastAlert 预警插件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E7%9B%B8%E5%85%B3%E4%BE%9D%E8%B5%96%E5%8C%85"><span class="toc-number">8.10.0.1.</span> <span class="toc-text">安装相关依赖包</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85python%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83"><span class="toc-number">8.10.0.2.</span> <span class="toc-text">安装python运行环境</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85elastalert"><span class="toc-number">8.10.0.3.</span> <span class="toc-text">安装elastalert</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEElastAlert"><span class="toc-number">8.10.0.4.</span> <span class="toc-text">配置ElastAlert</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AERule-%E5%91%8A%E8%AD%A6%E8%A7%84%E5%88%99%E9%85%8D%E7%BD%AE"><span class="toc-number">8.10.0.5.</span> <span class="toc-text">配置Rule 告警规则配置</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-%E5%BA%95%E5%BA%A7%E9%A1%B9%E7%9B%AE%E4%B8%ADELK%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">9.</span> <span class="toc-text">8 底座项目中ELK的应用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-logstash-logback-encoder%E7%AE%80%E4%BB%8B"><span class="toc-number">9.1.</span> <span class="toc-text">8.1 logstash-logback-encoder简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%8A%9F%E8%83%BD%E5%92%8C%E7%89%B9%E7%82%B9"><span class="toc-number">9.1.1.</span> <span class="toc-text">主要功能和特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">9.1.2.</span> <span class="toc-text">使用方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%B7%BB%E5%8A%A0-Maven-%E4%BE%9D%E8%B5%96"><span class="toc-number">9.1.2.1.</span> <span class="toc-text">1. 添加 Maven 依赖</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E9%85%8D%E7%BD%AE-Logback-%E8%BE%93%E5%87%BA%E4%B8%BA-JSON-%E6%A0%BC%E5%BC%8F"><span class="toc-number">9.1.2.2.</span> <span class="toc-text">2. 配置 Logback 输出为 JSON 格式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E4%BD%BF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E6%97%A5%E5%BF%97%E6%A0%BC%E5%BC%8F"><span class="toc-number">9.1.2.3.</span> <span class="toc-text">3. 使用自定义日志格式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E9%85%8D%E7%BD%AE%E9%A1%B9"><span class="toc-number">9.1.3.</span> <span class="toc-text">常见配置项</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-JSON-%E8%BE%93%E5%87%BA"><span class="toc-number">9.1.4.</span> <span class="toc-text">示例 JSON 输出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">9.1.5.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-logstash%E7%9A%84java%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%85%8D%E7%BD%AE"><span class="toc-number">9.2.</span> <span class="toc-text">8.2 logstash的java客户端配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2elk"><span class="toc-number">9.3.</span> <span class="toc-text">8.3 一键部署elk</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/03/11/%E6%9E%81%E8%87%B4ElasticSearch%E8%B0%83%E4%BC%98%EF%BC%8C%E8%AE%A9%E4%BD%A0%E7%9A%84ES%E7%8B%82%E9%A3%99100%E5%80%8D%EF%BC%81/" title="极致ElasticSearch调优，让你的ES狂飙100倍！"><img src="/img/liushui.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="极致ElasticSearch调优，让你的ES狂飙100倍！"/></a><div class="content"><a class="title" href="/2025/03/11/%E6%9E%81%E8%87%B4ElasticSearch%E8%B0%83%E4%BC%98%EF%BC%8C%E8%AE%A9%E4%BD%A0%E7%9A%84ES%E7%8B%82%E9%A3%99100%E5%80%8D%EF%BC%81/" title="极致ElasticSearch调优，让你的ES狂飙100倍！">极致ElasticSearch调优，让你的ES狂飙100倍！</a><time datetime="2025-03-11T04:40:01.000Z" title="发表于 2025-03-11 12:40:01">2025-03-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/11/%E6%9C%AC%E5%9C%B0%E6%B6%88%E6%81%AF%E8%A1%A8%E4%BA%8B%E5%8A%A1%EF%BC%9A10Wqps%E9%AB%98%E5%B9%B6%E5%8F%91%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E7%9A%84%E7%BB%88%E6%9E%81%E6%96%B9%E6%A1%88/" title="本地消息表事务：10Wqps高并发分布式事务的终极方案"><img src="/img/hubian.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="本地消息表事务：10Wqps高并发分布式事务的终极方案"/></a><div class="content"><a class="title" href="/2025/03/11/%E6%9C%AC%E5%9C%B0%E6%B6%88%E6%81%AF%E8%A1%A8%E4%BA%8B%E5%8A%A1%EF%BC%9A10Wqps%E9%AB%98%E5%B9%B6%E5%8F%91%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E7%9A%84%E7%BB%88%E6%9E%81%E6%96%B9%E6%A1%88/" title="本地消息表事务：10Wqps高并发分布式事务的终极方案">本地消息表事务：10Wqps高并发分布式事务的终极方案</a><time datetime="2025-03-11T04:21:56.000Z" title="发表于 2025-03-11 12:21:56">2025-03-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/11/Spring%E4%BA%8B%E5%8A%A1-Spring%E4%BA%8B%E5%8A%A1%E7%9A%8410%E7%A7%8D%E5%A4%B1%E6%95%88%E5%9C%BA%E6%99%AF-%E5%8A%A0%E5%85%A5%E5%9E%8B%E4%BC%A0%E6%92%AD%E5%92%8C%E5%B5%8C%E5%A5%97%E5%9E%8B%E4%BC%A0%E6%92%AD%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/" title="Spring事务,Spring事务的10种失效场景.加入型传播和嵌套型传播有什么区别"><img src="/img/hubian.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spring事务,Spring事务的10种失效场景.加入型传播和嵌套型传播有什么区别"/></a><div class="content"><a class="title" href="/2025/03/11/Spring%E4%BA%8B%E5%8A%A1-Spring%E4%BA%8B%E5%8A%A1%E7%9A%8410%E7%A7%8D%E5%A4%B1%E6%95%88%E5%9C%BA%E6%99%AF-%E5%8A%A0%E5%85%A5%E5%9E%8B%E4%BC%A0%E6%92%AD%E5%92%8C%E5%B5%8C%E5%A5%97%E5%9E%8B%E4%BC%A0%E6%92%AD%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/" title="Spring事务,Spring事务的10种失效场景.加入型传播和嵌套型传播有什么区别">Spring事务,Spring事务的10种失效场景.加入型传播和嵌套型传播有什么区别</a><time datetime="2025-03-11T02:02:50.000Z" title="发表于 2025-03-11 10:02:50">2025-03-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/11/CAP%E5%AE%9A%E7%90%86-%E5%93%AA%E4%BA%9B%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%98%AFAP-%E5%93%AA%E4%BA%9B%E6%98%AFCP-%E4%B8%BA%E4%BB%80%E4%B9%88/" title="CAP定理,哪些中间件是AP,哪些是CP,为什么"><img src="/img/haibian.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CAP定理,哪些中间件是AP,哪些是CP,为什么"/></a><div class="content"><a class="title" href="/2025/03/11/CAP%E5%AE%9A%E7%90%86-%E5%93%AA%E4%BA%9B%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%98%AFAP-%E5%93%AA%E4%BA%9B%E6%98%AFCP-%E4%B8%BA%E4%BB%80%E4%B9%88/" title="CAP定理,哪些中间件是AP,哪些是CP,为什么">CAP定理,哪些中间件是AP,哪些是CP,为什么</a><time datetime="2025-03-11T01:05:25.000Z" title="发表于 2025-03-11 09:05:25">2025-03-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/" title="elk圣经：Elasticsearch、Logstash、Kibana从入门到精通"><img src="/img/liushui.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="elk圣经：Elasticsearch、Logstash、Kibana从入门到精通"/></a><div class="content"><a class="title" href="/2025/03/08/elk%E5%9C%A3%E7%BB%8F%EF%BC%9AElasticsearch%E3%80%81Logstash%E3%80%81Kibana%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/" title="elk圣经：Elasticsearch、Logstash、Kibana从入门到精通">elk圣经：Elasticsearch、Logstash、Kibana从入门到精通</a><time datetime="2025-03-08T07:49:00.000Z" title="发表于 2025-03-08 15:49:00">2025-03-08</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By 周五打工人</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://mywaline-7unwzwfdi-motors-projects-a4d09e71.vercel.app/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://mywaline-7unwzwfdi-motors-projects-a4d09e71.vercel.app/',
      region: '',
      onCommentLoaded: () => {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))

    
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(init,0)
    else getScript('https://cdn.jsdelivr.net/npm/twikoo@1.6.31/dist/twikoo.all.min.js').then(init)
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>